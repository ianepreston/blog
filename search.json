[
  {
    "objectID": "posts/2020-05-13-conda_envs.html",
    "href": "posts/2020-05-13-conda_envs.html",
    "title": "How to work with conda environments in shell scripts and Makefiles",
    "section": "",
    "text": "I’ve struggled with automating working with the conda python environment manager for a while. It’s a relatively small part of my work flow so I haven’t made figuring it out a top priority, but it’s really bugging me. In this post I’m going to document the problem and all the troubleshooting steps I went through to resolve it. I’m writing this post in parallel with actually resolving the issue, so it’s going to be a bit stream of consciousness."
  },
  {
    "objectID": "posts/2020-05-13-conda_envs.html#makefile",
    "href": "posts/2020-05-13-conda_envs.html#makefile",
    "title": "How to work with conda environments in shell scripts and Makefiles",
    "section": "Makefile",
    "text": "Makefile\n# Oneshell means I can run multiple lines in a recipe in the same shell, so I don't have to\n# chain commands together with semicolon\n.ONESHELL:\n# Need to specify bash in order for conda activate to work.\nSHELL=/bin/bash\n# Note that the extra activate is needed to ensure that the activate floats env to the front of PATH\nCONDA_ACTIVATE=source $$(conda info --base)/etc/profile.d/conda.sh ; conda activate ; conda activate\n\ntest:\n    $(CONDA_ACTIVATE) eg_env\n    echo $$(which python)\n\n# format the file with black\nlint:\n    $(CONDA_ACTIVATE) eg_env\n    black eg.py\n\n# Run the file directly\nrun_py:\n    $(CONDA_ACTIVATE) eg_env\n    python eg.py\n\n# Run the file from a shell script\nrun_sh:\n    $(CONDA_ACTIVATE) eg_env\n    bash eg.sh"
  },
  {
    "objectID": "posts/2020-05-13-conda_envs.html#env.yml",
    "href": "posts/2020-05-13-conda_envs.html#env.yml",
    "title": "How to work with conda environments in shell scripts and Makefiles",
    "section": "env.yml",
    "text": "env.yml\nname: eg_env\ndependencies:\n    - numpy\n    - black"
  },
  {
    "objectID": "posts/2020-05-13-conda_envs.html#eg.py",
    "href": "posts/2020-05-13-conda_envs.html#eg.py",
    "title": "How to work with conda environments in shell scripts and Makefiles",
    "section": "eg.py",
    "text": "eg.py\nimport numpy as np\n\nprint(np.__version__)"
  },
  {
    "objectID": "posts/2020-05-13-conda_envs.html#eg.sh-for-makefile",
    "href": "posts/2020-05-13-conda_envs.html#eg.sh-for-makefile",
    "title": "How to work with conda environments in shell scripts and Makefiles",
    "section": "eg.sh (for makefile)",
    "text": "eg.sh (for makefile)\n#!/bin/bash\n\necho \"This is my test bash script\"\necho \"Running python script\"\npython eg.py"
  },
  {
    "objectID": "posts/2020-05-13-conda_envs.html#eg.sh-standalone",
    "href": "posts/2020-05-13-conda_envs.html#eg.sh-standalone",
    "title": "How to work with conda environments in shell scripts and Makefiles",
    "section": "eg.sh (standalone)",
    "text": "eg.sh (standalone)\n#!/bin/bash\n\necho \"This is my test bash script\"\necho \"Activating conda environment\"\neval \"$($(which conda) 'shell.bash' 'hook')\"\nconda activate eg_env\necho \"Running python script\"\npython eg.py"
  },
  {
    "objectID": "posts/2023-02-03-migration.html",
    "href": "posts/2023-02-03-migration.html",
    "title": "Migrating to quarto",
    "section": "",
    "text": "Update\nThis blog is now rendered with quarto rather than fastpages. Honestly fastpages was working fine for me, but they’ve deprecated it and I like being on the new thing. There are some nice navigation and filtering features added in this new tool, and the live preview on my dev machine works quite nicely. Mostly I’m making this post to ensure that my github action was set up correctly."
  },
  {
    "objectID": "posts/2020-05-03-ssh.html",
    "href": "posts/2020-05-03-ssh.html",
    "title": "Managing SSH keys for a home network",
    "section": "",
    "text": "This guide covers how I set up and manage ssh keys on my home network. It’s not meant as an explainer on what ssh is or why you should use it, more a recipe for how I do things.\n\nEdit October 16 2020 - Added scripts to automate this setup"
  },
  {
    "objectID": "posts/2020-05-03-ssh.html#get-the-drive-ready",
    "href": "posts/2020-05-03-ssh.html#get-the-drive-ready",
    "title": "Managing SSH keys for a home network",
    "section": "Get the drive ready",
    "text": "Get the drive ready\nI have a 64GB USB key that I’m going to store the CAs on. That’s way too big for what I need, and I wouldn’t mind having it handy to shuttle other files around (I’m not going to be taking it anywhere, on account of it has CAs for my network on it, but still). So I created a big exfat partition that took up most of the disk for files and such, along with a 200MB ntfs partition right at the end for storing keys. I went with exfat for the storage partition because it’s readable in both Windows and Linux (once you install exfat-utils and exfat-fuse), is designed for flash media, and can handle large files. I went with NTFS for the secrets partition because I have to have file permissions on the private keys or it won’t work, and windows machines won’t read EXT4. It does mean I’ll have to install NTFS support on any Linux boxes I want to run it from, which is a hassle, but I can live with that."
  },
  {
    "objectID": "posts/2020-05-03-ssh.html#generate-ca-keys",
    "href": "posts/2020-05-03-ssh.html#generate-ca-keys",
    "title": "Managing SSH keys for a home network",
    "section": "Generate CA keys",
    "text": "Generate CA keys\nFrom the usb drive:\nssh-keygen -t ed25519 -f user_ca -C user_ca\nssh-keygen -t ed25519 -f host_ca -C host_ca\nGenerate a host and user signing key using ed25519 encryption. Generates public private key pairs named host/user_ca and host/user_ca.pub for the public keys. RSA is the default, but all of my systems support ed25519 and I understand it’s better in terms of security and performance so I might as well take this opportunity to update.\n\nGenerate known_hosts file\nThis will go in the ~/.ssh folder of clients in order to validate access.\necho -n \"@cert-authority * \" &gt; known_hosts\ncat host_ca.pub &gt;&gt; known_hosts"
  },
  {
    "objectID": "posts/2020-05-03-ssh.html#generate-the-host-key-and-sign-it",
    "href": "posts/2020-05-03-ssh.html#generate-the-host-key-and-sign-it",
    "title": "Managing SSH keys for a home network",
    "section": "Generate the host key and sign it",
    "text": "Generate the host key and sign it\nFor any machines that I want to be able to ssh into I need to generate and sign a host key. From the same folder as the CA keys were generated:\nssh-keygen -t ed25519 -f mars_host_ed25519_key\nssh-keygen -s host_ca -I mars -h mars_host_ed25519_key.pub\n\nWARNING: DO NOT PUT A PASSPHRASE ON HOST KEYS\nThe first line operates the same as before. The second line signs the public key. -I mars is the certificate’s identity, apparently it can be used to revoke a certificate in the future although I’m not totally clear on how at this point. You can use the -n flag to specify which hostname in particular this key is valid for but I don’t really see the need in as small a setup as I’m doing. -h identifies this as a host key.\nAt the end of this, three new files are generated: mars_host_ed25519_key, mars_host_ed25519_key-cert.pub, and mars_host_ed25519_key.pub."
  },
  {
    "objectID": "posts/2020-05-03-ssh.html#configure-ssh-to-use-host-certificates",
    "href": "posts/2020-05-03-ssh.html#configure-ssh-to-use-host-certificates",
    "title": "Managing SSH keys for a home network",
    "section": "Configure SSH to use host certificates",
    "text": "Configure SSH to use host certificates\nMove the three generated files from the last section and copy user_ca.pub to /etc/ssh on your host and set the permissions to match the other files there. In this example I’ve physically moved the key over to the host machine and mounted it. If your host currently has ssh enabled with password based authentication you could scp it over instead:\nsudo mv mars_host_ed25519_key* /etc/ssh/\nsudo cp user_ca.pub /etc/ssh/\ncd /etc/ssh/\nsudo chown root:root mars_host*\nsudo chown root:root user_ca.pub\nsudo chmod 644 mars_host*\nsudo chmod 600 mars_host_ed25519_key # stricter private key permissions\nsudo chmod 644 user_ca.pub\nNext, edit /etc/ssh/sshd_config to have the lines\nHostKey /etc/ssh/mars_host_ed25519_key\nHostCertificate /etc/ssh/mars_host_ed25519_key-cert.pub\nTrustedUserCAKeys /etc/ssh/user_ca.pub\nThere was a commented out line in the file for HostKey so I put it below there. Placement shouldn’t really matter but it will hopefully be easier to find if I have to track this file down later.\nsudo systemctl restart sshd\nOnce you’re sure everything is working you’ll want to disable password authentication. Be aware that if you screw up keys and have the second line set you’ll have to physically connect to the machine to resolve it. For my home network this is no big deal, but just be aware. Go into /etc/ssh/sshd_config and set the following lines and restart ssh again:\nPubkeyAuthentication yes\nPasswordAuthentication no"
  },
  {
    "objectID": "posts/2020-05-03-ssh.html#create-user-key",
    "href": "posts/2020-05-03-ssh.html#create-user-key",
    "title": "Managing SSH keys for a home network",
    "section": "Create user key",
    "text": "Create user key\nStill in the folder with the ca key:\nssh-keygen -f luna_ed25519 -t ed25519\nssh-keygen -s user_ca -I ian@luna -n admin,ansible,ipreston,pi luna_ed25519.pub\nmv luna* ~/.ssh/\ncp known_hosts ~/.ssh/\nThe only new flag in this is -n ipreston,admin,pi which is the comma separated list of users I want this to be valid for. In addition to the username I set up on my server I want this key to be able to connect to my pfsense admin user and my raspberry pi, which I generally just leave with the default pi user."
  },
  {
    "objectID": "posts/2020-05-03-ssh.html#give-it-a-test-drive",
    "href": "posts/2020-05-03-ssh.html#give-it-a-test-drive",
    "title": "Managing SSH keys for a home network",
    "section": "Give it a test drive",
    "text": "Give it a test drive\nAt this point if everything is configured correctly you should be able to ssh from your client to your host and only be prompted for the passkey you set (or without any prompting if you didn’t set a passkey)\nssh -i ~/.ssh/luna_ed25519 ipreston@mars"
  },
  {
    "objectID": "posts/2020-05-03-ssh.html#set-up-an-ssh-config-file",
    "href": "posts/2020-05-03-ssh.html#set-up-an-ssh-config-file",
    "title": "Managing SSH keys for a home network",
    "section": "Set up an ssh config file",
    "text": "Set up an ssh config file\nIf that all worked the next step will be to set a nice alias to save some typing in the future.\ntouch ~/.ssh/config\nchmod 600 ~/.ssh/config\nIn config:\nHost mars\n    User ipreston\n    IdentityFile ~/.ssh/luna_ed25519\nThere are lots of other options you can set in that config but that’s all I need for my setup. After that all you should need to type is ssh mars to be connected to your host with the right user and using the correct authentication."
  },
  {
    "objectID": "posts/2020-05-03-ssh.html#set-up-host-authentication",
    "href": "posts/2020-05-03-ssh.html#set-up-host-authentication",
    "title": "Managing SSH keys for a home network",
    "section": "Set up host authentication",
    "text": "Set up host authentication\nThis script when run as root will generate a signed host key, move it to the correct directory, modify SSH configuration to authenticate using it and then restart ssh.\n#!/usr/bin/env bash\n\n# Bash \"strict\" mode, -o pipefail removed\nSOURCED=false && [ \"${0}\" = \"${BASH_SOURCE[0]}\" ] || SOURCED=true\nif ! $SOURCED; then\n  set -eEu\n  shopt -s extdebug\n  trap 's=$?; echo \"$0: Error on line \"$LINENO\": $BASH_COMMAND\"; exit $s' ERR\n  IFS=$'\\n\\t'\nfi\n\n# Text modifiers\nBold=\"\\033[1m\"\nReset=\"\\033[0m\"\n\n# Colors\nRed=\"\\033[31m\"\nGreen=\"\\033[32m\"\nYellow=\"\\033[33m\"\n\nerror_msg() {\n  T_COLS=$(tput cols)\n  echo -e \"${Red}$1${Reset}\\n\" | fold -sw $((T_COLS - 1))\n  exit 1\n}\n\ncheck_root() {\n  echo \"Checking root permissions...\"\n\n  if [[ \"$(id -u)\" != \"0\" ]]; then\n    error_msg \"ERROR! You must execute the script as the 'root' user.\"\n  fi\n}\n\n\ngenerate_and_sign_host_key() {\n  private_suffix=\"_host_ed25519_key\"\n  private_key=\"$HOSTNAME$private_suffix\"\n  public_key=\"$private_key.pub\"\n  ssh-keygen -t ed25519 -f $private_key -N \"\"\n  chown root:root $private_key*\n  # Have to lock it down before signing\n  chmod 600 $private_key\n  ssh-keygen -s CA/host_ca -I $HOSTNAME -h $public_key\n  cp CA/user_ca.pub /etc/ssh/\n  chown root:root /etc/ssh/user_ca.pub\n  chmod 644 /etc/ssh/user_ca.pub\n  chmod 644 $private_key*\n  # Set back to stricter private key access\n  chmod 600 $private_key\n  mv $private_key* /etc/ssh/\n}\n\nupdate_sshd_config() {\n  private_suffix=\"_host_ed25519_key\"\n  private_key=\"$HOSTNAME$private_suffix\"\n  hostkey=\"HostKey /etc/ssh/$private_key\"\n  cert=\"HostCertificate /etc/ssh/$private_key-cert.pub\"\n  ca=\"TrustedUserCAKeys /etc/ssh/user_ca.pub\"\n\n  sshd=\"/etc/ssh/sshd_config\"\n  sed -i \"/^#HostKey\\s\\/etc\\/ssh\\/ssh_host_ed25519_key/a $ca\" $sshd\n  sed -i \"/^#HostKey\\s\\/etc\\/ssh\\/ssh_host_ed25519_key/a $cert\" $sshd\n  sed -i \"/^#HostKey\\s\\/etc\\/ssh\\/ssh_host_ed25519_key/a $hostkey\" $sshd\n  sed -i 's/^#PasswordAuthentication\\syes/PasswordAuthentication no/g' $sshd\n}\n\ncheck_root\ngenerate_and_sign_host_key\nupdate_sshd_config\nsystemctl restart sshd"
  },
  {
    "objectID": "posts/2020-05-03-ssh.html#setup-user-authentication",
    "href": "posts/2020-05-03-ssh.html#setup-user-authentication",
    "title": "Managing SSH keys for a home network",
    "section": "Setup user authentication",
    "text": "Setup user authentication\nWhatever user your run this as should end up with a configured SSH setup that will allow access into any similarly configured hosts.\n#!/usr/bin/env bash\n\n# Bash \"strict\" mode, -o pipefail removed\nSOURCED=false && [ \"${0}\" = \"${BASH_SOURCE[0]}\" ] || SOURCED=true\nif ! $SOURCED; then\n  set -eEu\n  shopt -s extdebug\n  trap 's=$?; echo \"$0: Error on line \"$LINENO\": $BASH_COMMAND\"; exit $s' ERR\n  IFS=$'\\n\\t'\nfi\n\n# Text modifiers\nBold=\"\\033[1m\"\nReset=\"\\033[0m\"\n\n# Colors\nRed=\"\\033[31m\"\nGreen=\"\\033[32m\"\nYellow=\"\\033[33m\"\n\nerror_msg() {\n  T_COLS=$(tput cols)\n  echo -e \"${Red}$1${Reset}\\n\" | fold -sw $((T_COLS - 1))\n  exit 1\n}\n\ncheck_root() {\n  echo \"Checking root permissions...\"\n\n  if [[ \"$(id -u)\" == \"0\" ]]; then\n    error_msg \"ERROR! Don't run this as root.\"\n  fi\n}\n\nupdate_known_hosts() {\n  cp CA/known_hosts $HOME/.ssh/known_hosts\n}\n\ngen_keys() {\n  cp CA/user_ca .\n  chown $USER user_ca\n  chmod 600 user_ca\n  private_suffix=\"_ed25519\"\n  private_key=$HOME/.ssh/$HOSTNAME$private_suffix\n  public_key=\"$private_key.pub\"\n  ssh-keygen -f $private_key -t ed25519 -N \"\"\n  ssh-keygen -s user_ca -I $USER@$HOSTNAME -n admin,ipreston,cornucrapia,pi,ansible $public_key\n  rm user_ca\n}\n\nupdate_config() {\n  user_config=\"$HOME/.ssh/config\"\n  cp CA/config $user_config\n  chown $USER $user_config\n  chmod 600 $user_config\n  sed -i -e \"s/HOST/$HOSTNAME/g\" $user_config\n}\n\n\ncheck_root\nmkdir -p $HOME/.ssh\nupdate_known_hosts\ngen_keys\nupdate_config"
  },
  {
    "objectID": "posts/2020-05-17-conda.html",
    "href": "posts/2020-05-17-conda.html",
    "title": "Trouble with reproducible conda environments",
    "section": "",
    "text": "Introduction\nI’m having trouble making reproducible conda environments. I’ve posted the question below on Stack Overflow and Reddit but I’ve got nothing. I’m leaving the question here for easy future reference. If I come up with a solution I’ll update this post.\n\n\nThe question\nHi everyone.\nI’m really struggling to create a reproducible conda environment. I’ll outline the approach I’ve taken so far and the issue I’ve encountered. I’d appreciate any tips for what I can do to troubleshoot next or resources I could check.\nAs background, I work on a small team, and want to be able to share a copy of an environment I’ve been using with other members of my team so I can be sure we have identical versions of all the libraries required for our work.\nMy current workflow is as follows:\n\nWrite out an environment file with unpinned dependencies and let conda build the environment\n\nname: example_env_build\nchannels:\n    - conda-forge\n    - defaults\ndependencies:\n    - pandas\n    - requests\nThe actual environment has a lot more stuff in it, but that’s the idea\n\nI then create the environment with conda env create -f example_env_build.yml\nI export the environment so that all versions and their dependencies will be pinned with conda env export -n example_env_build --no-builds --file test_export.yml. I added --no-builds because I was finding the certain builds were getting marked as broken and causing issues and getting the version right seemed close enough for my purposes.\nI edit the test_export.yml file and change the name to example_env and remove the prefix line from the bottom.\nI build a new environment with this pinned file just to make sure it goes ok, and then share the file with the rest of my team.\n\nThis has generally worked well if everyone tries to build the environment relatively quickly after the file is created. However, the whole point of being able to specify a reproducible environment is that I should be able to recreate that environment at any time. Someone on my team recently got a new computer so I was trying to help her set up her environment and ran into a series of conflicts. To troubleshoot I tried to rebuild the environment on my machine and ran into the same situation.\nFor troubleshooting I did the following: * Clone my environment so I have a backup while I mess around conda create --name example_env_clone --clone example_env * Export the environment conda env export -n example_env --no-builds --file example_env_rebuild.yml * Delete the example environment so I can rebuild it conda env remove --name example_env * Try and recreate the environment I just exported conda env create -f example_env_rebuild.yml\nFrom there I ran into all sorts of version conflicts. I don’t understand this because a) These are all versions being used in a working environment and b) a lot of the “conflicts” don’t seem to be conflicts to me. As an example, here’s one from my current attempt:\nPackage phik conflicts for:\nphik=0.9.10\npandas-profiling=2.4.0 -&gt; phik[version='&gt;=0.9.8']\nI picked that one basically at random but there are tons like that. As I read it I’m trying to install phik 0.9.10, and pandas-profiling requires &gt;=0.9.8, which 0.9.10 satisfies.\nI’m at my wits end here. I’ve read a million “how to manage conda environments” guides (For example this, this, and this) along with the conda environment management docs. All of them seem to indicate what I’m doing should work perfectly fine, but my team and I constantly run into issues.\nHas anyone had a similar experience? Is there something I’m missing, or a resource I could consult? I’d greatly appreciate any pointers.\nThanks"
  },
  {
    "objectID": "posts/2022-03-17-mortgage.html",
    "href": "posts/2022-03-17-mortgage.html",
    "title": "Modeling out fixed closed vs capped variable mortgages",
    "section": "",
    "text": "I’m looking to buy a house in the near future. I’m pretty risk averse, so even though I know that in theory a variable rate mortgage should be better for me financially, I was willing to take a fixed rate mortgage in order to have more certainty. In discussion with a mortgage broker it was suggested that I could get the best of both worlds by taking out a capped variable mortgage and increasing my payment amount to what I would have paid under a fixed rate. In theory this gives me a hedge in that my payments are capped, and as long as my variable rate is below the fixed rate I’m making extra payments against my principle compared to what I’d be doing with a fixed rate. Let’s see what that looks like under some different scenarios using python. Some of these calculations I already implemented in my rent or own calculator but the scenarios I’m working on are different enough here that I’m only going to use that as a reference rather than importing any of the code."
  },
  {
    "objectID": "posts/2022-03-17-mortgage.html#introduction",
    "href": "posts/2022-03-17-mortgage.html#introduction",
    "title": "Modeling out fixed closed vs capped variable mortgages",
    "section": "",
    "text": "I’m looking to buy a house in the near future. I’m pretty risk averse, so even though I know that in theory a variable rate mortgage should be better for me financially, I was willing to take a fixed rate mortgage in order to have more certainty. In discussion with a mortgage broker it was suggested that I could get the best of both worlds by taking out a capped variable mortgage and increasing my payment amount to what I would have paid under a fixed rate. In theory this gives me a hedge in that my payments are capped, and as long as my variable rate is below the fixed rate I’m making extra payments against my principle compared to what I’d be doing with a fixed rate. Let’s see what that looks like under some different scenarios using python. Some of these calculations I already implemented in my rent or own calculator but the scenarios I’m working on are different enough here that I’m only going to use that as a reference rather than importing any of the code."
  },
  {
    "objectID": "posts/2022-03-17-mortgage.html#get-some-data",
    "href": "posts/2022-03-17-mortgage.html#get-some-data",
    "title": "Modeling out fixed closed vs capped variable mortgages",
    "section": "Get some data",
    "text": "Get some data\nTo do any scenario planning it will be helpful to have some historical data on the relevant interest rates. Specifically I want to know 5 year fixed mortgage rates, and the big bank prime rate, as that forms the basis of variable rate mortgages. At the time of this writing RBC is offering variable mortgages at “RBC Prime Rate - 0.750%” but they’re also offering a “special rate” of 3.34% compared to their posted rate of 4.79% for 5 year fixed. The delta there is 1.45%. Their posted variable rate is just their posted prime. I’m less interested in the exact rates than I am the delta. It looks like right now the delta of their offered rates between fixed and closed (3.34 - 1.95 = 1.39) is less than their posted fixed and closed rate (4.79 - 2.70 = 2.09). Because the data set I can work with only has posted rates and I have no way of estimating the “true” delta in historical periods I’ll use it as the basis for this analysis. However, it will be important to remember that at least in this current low interest rate environment there’s less of a gap between fixed and variable rates than my data will suggest. Depending on how close these scenarios play out I can re-run them by adding 2.09 - 1.39 = 0.7 onto my prime rate to simulate that narrower gap.\n\nfrom collections import defaultdict\nimport pandas as pd\nimport requests\n\nFirst we grab some historical rate data in JSON from the Bank of Canada website. I tried to grab it as a CSV but it’s terribly formatted, so we’ll go with this and clean it up.\n\nrates_json = requests.get(\"https://www.bankofcanada.ca/valet/observations/group/chartered_bank_interest/json\").json()\n\nNext I have to clean it up into a dataframe for analysis.\n\nrenamer = {k: f\"{v['description']} {v['label']}\".strip() for k, v in rates_json[\"seriesDetail\"].items()}\n\n\nbase_df = (\n    pd.DataFrame(rates_json[\"observations\"])\n    # convert date series from string to datetime\n    .assign(d=lambda df: pd.to_datetime(df[\"d\"]))\n    .set_index(\"d\")\n    # Get the actual rate value out of the dictionary it's in\n    .applymap(lambda x: float(x[\"v\"]), na_action='ignore')\n    .rename(columns=renamer)\n)\n\nLet’s take a quick look at it to make sure it’s sensible.\n\nbase_df.tail()\n\n\n\n\n\n\n\n\nNon-Chequable Savings Deposits\n5-year personal fixed term\nPrime rate\nConventional mortgage 5-year\nConventional mortgage 1-year\nConventional mortgage 3-year\nGuaranteed investment certificates 5-year\nGuaranteed investment certificates 1-year\nGuaranteed investment certificates 3-year\nDaily Interest Savings (balances over $100,000)\n\n\nd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022-02-16\n0.01\n1.15\n2.45\n4.79\n2.79\n3.49\n1.65\n0.60\n1.25\n0.01\n\n\n2022-02-23\n0.01\n1.45\n2.45\n4.79\n2.79\n3.49\n1.75\n0.60\n1.35\n0.01\n\n\n2022-03-02\n0.01\n1.45\n2.45\n4.79\n2.79\n3.49\n1.75\n0.60\n1.35\n0.01\n\n\n2022-03-09\n0.01\n1.55\n2.70\n4.79\n2.79\n3.49\n1.85\n0.65\n1.28\n0.01\n\n\n2022-03-16\n0.01\n1.55\n2.70\n4.79\n2.79\n3.49\n1.85\n0.78\n1.30\n0.01\n\n\n\n\n\n\n\nThat looks good to me. I’m just going to shrink it down to the just the columns I care about, resample it down to monthly frequency, and give them easier to work with names.\n\ndf = (\n    base_df\n    .rename(columns={\"Prime rate\": \"prime\", \"Conventional mortgage 5-year\": \"fiveyear\"})\n    .reindex(columns=[\"prime\", \"fiveyear\"])\n    # Most early dates don't have values for these series\n    .dropna(how=\"all\")\n    .reset_index()\n    # Group it to the first entry for each month\n    .groupby(pd.Grouper(key=\"d\", freq=\"M\", convention=\"start\"))\n    .first()\n    # Make it the start rather than end of the month to make date deltas easier\n    .reset_index()\n    .assign(d=lambda df: df[\"d\"].dt.to_period('M').dt.to_timestamp())\n    .set_index(\"d\")\n)\ndf.head()\n\n\n\n\n\n\n\n\nprime\nfiveyear\n\n\nd\n\n\n\n\n\n\n1975-01-01\n11.0\n12.00\n\n\n1975-02-01\n9.5\n11.50\n\n\n1975-03-01\n9.0\n10.75\n\n\n1975-04-01\n9.0\n10.50\n\n\n1975-05-01\n9.0\n10.75"
  },
  {
    "objectID": "posts/2022-03-17-mortgage.html#some-quick-exploration",
    "href": "posts/2022-03-17-mortgage.html#some-quick-exploration",
    "title": "Modeling out fixed closed vs capped variable mortgages",
    "section": "Some quick exploration",
    "text": "Some quick exploration\nLet’s do some basic charting on this data just to get a sense of what we’re working with.\n\n# Would rather use altair but I have lots of data points and I'm not sure how using render server will play with my static site generator\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Make my charts a decent size\nsns.set(rc = {'figure.figsize':(15,8)})\n\n\n# Seaborn likes data in tidy format\ncdf = df.melt(ignore_index=False).reset_index()\n\n\nsns.lineplot(data=cdf, x=\"d\", y=\"value\", hue=\"variable\");\n\n\n\n\nHere’s what prime vs 5 year mortgage rates have looked like for the last 50ish years. The variable rate tends to be below the fixed at any given point in time, but of course the variable rate can go up over your mortgage term while a fixed rate will stay fixed. Let’s start the actual analysis."
  },
  {
    "objectID": "posts/2022-03-17-mortgage.html#check-variable-vs-5-year-term",
    "href": "posts/2022-03-17-mortgage.html#check-variable-vs-5-year-term",
    "title": "Modeling out fixed closed vs capped variable mortgages",
    "section": "Check variable vs 5 year term",
    "text": "Check variable vs 5 year term\nFor any starting period I can compare a 5 year fixed rate to what the variable rate would have been over that same period and plot the delta between them. Let’s see what that looks like:\n\n# Get all the dates that have at least 5 years of history ahead of them\nstart_dates = list(df.index[:-60])\n\n\ndelta_dict = defaultdict(dict)\nfor start_date in start_dates:\n    five_year_rate = df.loc[start_date, \"fiveyear\"]\n    for delta in range(60):\n        variable_rate = df.loc[start_date + pd.DateOffset(months=delta), \"prime\"]\n        variable_delta = variable_rate - five_year_rate\n        delta_dict[delta][start_date] = variable_delta\ndelta_df = pd.DataFrame(delta_dict).T\ndelta_df.head()\n\n\n\n\n\n\n\n\n1975-01-01\n1975-02-01\n1975-03-01\n1975-04-01\n1975-05-01\n1975-06-01\n1975-07-01\n1975-08-01\n1975-09-01\n1975-10-01\n...\n2016-06-01\n2016-07-01\n2016-08-01\n2016-09-01\n2016-10-01\n2016-11-01\n2016-12-01\n2017-01-01\n2017-02-01\n2017-03-01\n\n\n\n\n0\n-1.0\n-2.0\n-1.75\n-1.5\n-1.75\n-2.00\n-2.00\n-2.25\n-2.50\n-2.25\n...\n-1.94\n-2.04\n-2.04\n-1.94\n-1.94\n-1.94\n-1.94\n-1.94\n-1.94\n-1.94\n\n\n1\n-2.5\n-2.5\n-1.75\n-1.5\n-1.75\n-2.00\n-2.00\n-2.25\n-1.75\n-2.25\n...\n-1.94\n-2.04\n-2.04\n-1.94\n-1.94\n-1.94\n-1.94\n-1.94\n-1.94\n-1.94\n\n\n2\n-3.0\n-2.5\n-1.75\n-1.5\n-1.75\n-2.00\n-2.00\n-1.50\n-1.75\n-2.25\n...\n-1.94\n-2.04\n-2.04\n-1.94\n-1.94\n-1.94\n-1.94\n-1.94\n-1.94\n-1.94\n\n\n3\n-3.0\n-2.5\n-1.75\n-1.5\n-1.75\n-2.00\n-1.25\n-1.50\n-1.75\n-2.25\n...\n-1.94\n-2.04\n-2.04\n-1.94\n-1.94\n-1.94\n-1.94\n-1.94\n-1.94\n-1.94\n\n\n4\n-3.0\n-2.5\n-1.75\n-1.5\n-1.75\n-1.25\n-1.25\n-1.50\n-1.75\n-2.25\n...\n-1.94\n-2.04\n-2.04\n-1.94\n-1.94\n-1.94\n-1.94\n-1.94\n-1.94\n-1.94\n\n\n\n\n5 rows × 507 columns\n\n\n\n\ndelta_cdf = (\n    delta_df\n    .melt(ignore_index=False)\n    .reset_index()\n    # change from a date to a count of months since the first period\n    # makes it easier to add in a colour scale and get a sense of time periods.\n    .assign(variable=lambda df: (df[\"variable\"] - df[\"variable\"].min()).dt.days)\n    .rename(columns={\"index\": \"period\", \"variable\": \"start date delta\", \"value\": \"variable_delta\"})\n)\n\n\nsns.lineplot(data=delta_cdf, x=\"period\", y=\"variable_delta\", hue=\"start date delta\");\n\n\n\n\nThis chart shows the change in the delta between a fixed rate mortgage and the variable rate over a 5 year period. Each line represents a potential start month, with the delta between fixed and variable on the y axis and how many months into the 5 year term that mortgage is on the x axis. The colour corresponds to the number of days after Jan 1 1970 the scenario starts (I just needed a nice linear series to get the colour coding).\nThat’s way too noisy to make a ton of sense of, but it shows that variable rates can be quite a lot higher or lower than the 5 year fixed. Most of the crazy outliers are lighter in colour, which corresponds to earlier points in history and given the wildly volatile interest rates we saw in the previous chart that seems reasonable. Eyeballing it it definitely looks like the majority of series are below the 0 line throughout the 5 year period.\nOne could make the argument that including the 1980s is unfair given the crazy stagflation and associated high interest rates used to combat them. If you think more recent history is a better guide we can look at just the scenarios starting since the year 2000:\n\nsubset_cdf = delta_cdf.loc[lambda df: df[\"start date delta\"] &gt;= 25 * 365]\nsns.lineplot(data=subset_cdf, x=\"period\", y=\"variable_delta\", hue=\"start date delta\");\n\n\n\n\nSo since 2000 it looks like your variable rate has almost never gone above the five year fixed rate. Again, recall that the actual fixed or variable rate you’d get at any of these points is not perfectly correlated with the posted rates I’m using, but it’s still a good indicator. Personally since I’m risk averse and we are currently looking at higher inflation than there’s been for 30 years I’m inclined to use the longer term scenarios, but it was an interesting exercise to see what more recent history looks like.\nNow, even though these charts are kind of pretty, they’re not very informative, there’s too much going on. What I can do instead is summarize the distribution of those lines. So for any point in the mortgage term I can show the mean delta, or the median, or any other quantile. Let’s take a look at that:\n\nfull_series = [delta_df.mean(axis=\"columns\").rename(\"mean\")]\nfor q in [0.05, 0.10, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    full_series.append(delta_df.quantile(q, axis=\"columns\").rename(f\"q{q}\"))\nfull_distributions = pd.concat(full_series, axis=\"columns\")\n\n\nfdist_cdf = (\n    full_distributions\n    .melt(ignore_index=False)\n    .reset_index()\n    .rename(columns={\"index\": \"period\"})\n)\nsns.lineplot(data=fdist_cdf, x=\"period\", y=\"value\", hue=\"variable\");\n\n\n\n\nSo over the whole observation period the mean gap between fixed and variable rate tends to trend downward, which makes sense given interest rates were declining over most of the observed period. At least 3/4 of the time they stayed below the fixed rate for the entire period, but when we look at the 90th percentile and above they were over the fixed rate throughout the period (this is not saying any one series was consistently that high above throughout the 5 year period, just that at least 10% of them were that much above it at that point.) Let’s try it again, just focusing on since 2000.\n\nrdelta_df = delta_df.T.loc[\"2000\":].T\nfull_series = [rdelta_df.mean(axis=\"columns\").rename(\"mean\")]\nfor q in [0.01, 0.05, 0.10, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]:\n    full_series.append(rdelta_df.quantile(q, axis=\"columns\").rename(f\"q{q}\"))\nfull_distributions = pd.concat(full_series, axis=\"columns\")\nfdist_cdf = (\n    full_distributions\n    .melt(ignore_index=False)\n    .reset_index()\n    .rename(columns={\"index\": \"period\"})\n)\nsns.lineplot(data=fdist_cdf, x=\"period\", y=\"value\", hue=\"variable\");\n\n\n\n\nSo in the more recent period I checked out some of the wider end of the tails. At the 99th percentile we do see a couple spots where it goes above 0, but not by much. Variable rates over the last 20 years have been a pretty sweet deal."
  },
  {
    "objectID": "posts/2022-03-17-mortgage.html#model-out-capped-variable-strategy",
    "href": "posts/2022-03-17-mortgage.html#model-out-capped-variable-strategy",
    "title": "Modeling out fixed closed vs capped variable mortgages",
    "section": "Model out capped variable strategy",
    "text": "Model out capped variable strategy\nLooking at these trends, it definitely seems like going with the capped variable rate would be the way to go, but let’s model it out a bit and see. The first thing I need to do is write a class that can handle mortgage payments, including variable payments or a capped variable strategy. For that I can leverage some of the code I wrote for my rent or own calculator (with some tweaks).\n\nimport math\nfrom collections import OrderedDict\nimport numpy_financial as npf\nimport numpy as np\n\n\nclass Mortgage:\n    \"\"\"Base mortgage class.\n    Parameters\n    ----------\n    delta_series: pd.Series\n        The series of rate deltas over the 5 year term. Can pass in a series of 0s to simulate a fixed rate mortgage\n    capped_variable: bool\n        Whether to use the capped variable or regular variable payment strategy\n    principal: Int\n        Value of the mortgage, default $400K to simulate a $500K purchase with 20% down\n    amortize_years: int\n        Amortization period of the mortgage (not term of mortgage), default to standard 25 for a new purchase\n    fixed_rate: float\n        fixed 5 year APR rate as posted online, will use AER for actual calculations, default to 4.79 since that's the latest value\n        in the series at the time of this writing.\n    \"\"\"\n\n    def __init__(self, delta_series: pd.Series, scenario_name: str, capped_variable: bool = True, principal: int=400_000 , amortize_years: int = 25, fixed_rate: float = 4.79) -&gt; None:\n        self.delta_series = delta_series\n        # So I can label dataframes or other returned objects\n        self.scenario_name = scenario_name\n        self.capped_variable = capped_variable\n        self.principal = principal\n        self.amortize_years = amortize_years\n        self.fixed_rate = fixed_rate\n        # Have to do different compounding period for variable (monthly) vs fixed (semi annual)\n        if delta_series.sum() == 0:\n            self.fixed = True\n        else:\n            self.fixed = False\n        self.fixed_pmt = self._calc_fixed_pmt()\n    \n    def periodic_rate(self, period: int) -&gt; float:\n        \"\"\"Calculate the periodic interest rate from the posted APR.\n        \n        Parameters\n        ----------\n        period: int\n            Which period in the term we're in (used to identify the variable rate delta)\n        \n        Returns\n        -------\n        float: periodic interest rate\n        \"\"\"\n        apr = self.fixed_rate + self.delta_series[period]\n        apr_dec = apr / 100\n        if self.fixed:\n            compounds = 2\n        else:\n            compounds = 12\n        aer = (1 + (apr_dec / compounds)) ** compounds - 1\n        return (1 + aer) ** (1 / 12) - 1\n\n    def _calc_fixed_pmt(self) -&gt; float:\n        \"\"\"Calculate what a fixed payment would be to compute capped variable payments.\"\"\"\n        apr_dec = self.fixed_rate / 100\n        compounds = 2 \n        aer = (1 + (apr_dec / compounds)) ** compounds - 1\n        periodic_rate = (1 + aer) ** (1 / 12) - 1\n        periods = self.amortize_years * 12\n        return -round(npf.pmt(periodic_rate, periods, self.principal), 2)\n        \n    def _amortizegen(self):\n        \"\"\"Yield a dictionary of a payment period.\n        \n        Have to do it incrementally because additional payments can mean recomputing things after every period.\n        \n        Parameters\n        ----------\n        period: int\n            Which period of the term we're in\n        \n        Yields\n        ------\n        Dict\n            All the data for another period of mortgage paments\n        \"\"\"\n        period = 0\n        beg_balance = self.principal\n        end_balance = self.principal\n        # Careful here, have to control how often this is called outside the function\n        while period &lt; 60:\n            periodic_rate = self.periodic_rate(period)\n            amortization_left = (self.amortize_years * 12) - period\n            interest = -npf.ipmt(rate=periodic_rate, per=1, nper=amortization_left, pv=beg_balance)\n            if self.capped_variable:\n                if interest &gt;= self.fixed_pmt:\n                    principal = 0\n                else:\n                    principal = self.fixed_pmt - interest\n            else:\n                principal = -npf.ppmt(rate=periodic_rate, per=1, nper=amortization_left, pv=beg_balance)\n            total_pmt = interest + principal\n            end_balance = beg_balance - principal\n            yield OrderedDict(\n                [\n                    (\"Period\", period),\n                    (\"Begin_balance\", beg_balance),\n                    (\"Payment\", total_pmt),\n                    (\"Principal\", principal),\n                    (\"Interest\", interest),\n                    (\"End_balance\", end_balance),\n                ]\n            )\n            # increment the counter, balance and date\n            period += 1\n            beg_balance = end_balance\n\n    def amortize(self) -&gt; pd.DataFrame:\n        \"\"\"Show payments on the mortgage.\n        \n        Returns\n        -------\n        pd.DataFrame\n            Dataframe of mortgage payments showing principal and interest contributions and amount outstanding\n        \"\"\"\n        df = (\n            pd.DataFrame(self._amortizegen())\n            .set_index(\"Period\")\n        )\n        return df\n    \n    def summarize(self) -&gt; dict:\n        \"\"\"Summarize the 5 year term.\"\"\"\n        df = self.amortize()\n        return {\n            \"scenario_name\": self.scenario_name,\n            \"end_balance\": df.iloc[-1][\"End_balance\"],\n            \"total_payments\": df[\"Payment\"].sum(),\n            \"average_payment\": df[\"Payment\"].mean(),\n            \"min_payment\": df[\"Payment\"].min(),\n            \"max_payment\": df[\"Payment\"].max(),\n            \"total_interest\": df[\"Interest\"].sum(),\n            \"total_principal\": df[\"Principal\"].sum(),\n        }\n\n    def pretty_summary(self) -&gt; None:\n        \"\"\"Print out the summary in pretty format.\"\"\"\n        summary = self.summarize()\n        result = f\"\"\"\n            Scenario: {summary[\"scenario_name\"]}\n            Capped: {self.capped_variable}\n            Total Payments: ${summary[\"total_payments\"]:,.0f}\n            Ending mortgage balance: ${summary[\"end_balance\"]:,.0f}\n            Total interest payments: ${summary[\"total_interest\"]:,.0f}\n            Total principal payments: ${summary[\"total_principal\"]:,.0f}\n            Average payment: ${summary[\"average_payment\"]:,.0f}\n            Minimum payment: ${summary[\"min_payment\"]:,.0f}\n            Maximum payment: ${summary[\"max_payment\"]:,.0f}\n        \"\"\"\n        print(result)\n\nLet’s look at the summary of a few of these scenarios\n\nfixed_scenario = Mortgage(\n    delta_series=np.zeros(60),\n    scenario_name=\"fixed payments\",\n    capped_variable=False\n)\nfixed_summary = fixed_scenario.summarize()\nfixed_scenario.pretty_summary() \n\n\n            Scenario: fixed payments\n            Capped: False\n            Total Payments: $136,730\n            Ending mortgage balance: $352,851\n            Total interest payments: $89,581\n            Total principal payments: $47,149\n            Average payment: $2,279\n            Minimum payment: $2,279\n            Maximum payment: $2,279\n        \n\n\n\nMortgage(\n    delta_series=delta_df[\"1975-01-01\"],\n    scenario_name=\"1975-01-01\",\n    capped_variable=False\n).pretty_summary() \n\n\n            Scenario: 1975-01-01\n            Capped: False\n            Total Payments: $111,026\n            Ending mortgage balance: $340,142\n            Total interest payments: $51,168\n            Total principal payments: $59,858\n            Average payment: $1,850\n            Minimum payment: $1,536\n            Maximum payment: $2,801\n        \n\n\n\nMortgage(\n    delta_series=delta_df[\"1975-01-01\"],\n    scenario_name=\"1975-01-01 capped\",\n    capped_variable=True\n).pretty_summary() \n\n\n            Scenario: 1975-01-01 capped\n            Capped: True\n            Total Payments: $136,730\n            Ending mortgage balance: $311,809\n            Total interest payments: $48,539\n            Total principal payments: $88,191\n            Average payment: $2,279\n            Minimum payment: $2,279\n            Maximum payment: $2,279\n        \n\n\nBefore I run a bunch of scenarios from historical ones, let’s plug in something derived from a forecast. TD forecasts the overnight rate for the BoC will end 2022 at 1.25% (it’s 0.5% as I write this) and climb to 1.75% by the end of 2023 before leveling off for the remainder of the forecast period. Let’s make a scenario where variable rates follow that same hike schedule 1:1\n\n# Current delta based on \"special\" rates available on RBC site\nstart_delta = -1.39\ndelta_series = np.repeat(start_delta, 60)\n# let's do 3 0.25 rate hikes over the next 9 months\nfor cut in [3, 6, 9]:\n    delta_series[cut:] += 0.25\n#Then we need two hikes in 2023\nfor cut in [12, 18]:\n    delta_series[cut:] += 0.25\n\n\nMortgage(\n    delta_series=delta_series,\n    scenario_name=\"1.25% total hikes, capped\",\n    capped_variable=True\n).pretty_summary() \n\n\n            Scenario: 1.25% total hikes, capped\n            Capped: True\n            Total Payments: $136,730\n            Ending mortgage balance: $346,032\n            Total interest payments: $82,762\n            Total principal payments: $53,968\n            Average payment: $2,279\n            Minimum payment: $2,279\n            Maximum payment: $2,279\n        \n\n\n\nMortgage(\n    delta_series=delta_series,\n    scenario_name=\"1.25% total hikes, uncapped\",\n    capped_variable=False\n).pretty_summary() \n\n\n            Scenario: 1.25% total hikes, uncapped\n            Capped: False\n            Total Payments: $132,412\n            Ending mortgage balance: $351,139\n            Total interest payments: $83,552\n            Total principal payments: $48,861\n            Average payment: $2,207\n            Minimum payment: $1,981\n            Maximum payment: $2,250\n        \n\n\nSo in this scenario (which I’m definitely not saying will happen, but it’s plausible) either of the variable strategies beat a fixed strategy since we never get up to the fixed rate. Between capped and full variable you make bigger payments and also pay less interest, so as long as those payments are manageable that seems like a pretty solid deal. Let’s run this against a bunch of historical scenarios now.\nFirst we’ll look at the distribution of outcomes for the capped strategy based on historical rate delta development\n\ncapped_scenarios = pd.DataFrame([\n    Mortgage(\n        delta_series=delta_df[col],\n        scenario_name=\"1975-01-01 capped\",\n        capped_variable=True\n    ).summarize()\n    for col in delta_df.columns\n])\n\n/home/ipreston/miniconda3/envs/mortgage/lib/python3.10/site-packages/numpy_financial/_financial.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  (1 + rate*when)*(temp - 1)/rate)\n/home/ipreston/miniconda3/envs/mortgage/lib/python3.10/site-packages/numpy_financial/_financial.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  (1 + rate*when)*(temp - 1)/rate)\n\n\n\ndef plot_hist(data, col):\n    g = sns.histplot(data=data,x=col, bins=50, stat=\"percent\")\n    plt.axvline(x=fixed_summary[col], color=\"red\")\n    return g\n\nThis chart shows the distribution of end balances (the amount still owing on the mortgage at the end of the 5 year term) using the capped variable strategy across all the historical scenarios we outlined above. The red vertical line is the equivalent point for going with a fixed rate. The vast majority of points show a lower ending balance, which is definitely good, although there is some tail risk of ending with a balance of up to about $50k more.\n\nplot_hist(capped_scenarios, \"end_balance\");\n\n\n\n\nSimilarly, we can look at the distribution of maximum payments made over the whole 5 year term. As long as the cap at least covers your interest payments you never go over it, so unsurprisingly over 70% of the scenarios have a max payment equal to the fixed amount. Again though, there’s a long tail (presumably corresponding to the massive rate hikes we saw in the late 70s and early 80s where you would end up making at least one payment that’s more than double the fixed amount, ouch.\n\nplot_hist(capped_scenarios, \"max_payment\");\n\n\n\n\nFinally, let’s look at the same scenarios, but just paying the variable rate the whole way through. The graphs represent the same things as above, just with the uncapped variable rate mortgage.\n\nuncapped_scenarios = pd.DataFrame([\n    Mortgage(\n        delta_series=delta_df[col],\n        scenario_name=\"1975-01-01 capped\",\n        capped_variable=False\n    ).summarize()\n    for col in delta_df.columns\n])\n\n/home/ipreston/miniconda3/envs/mortgage/lib/python3.10/site-packages/numpy_financial/_financial.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  (1 + rate*when)*(temp - 1)/rate)\n/home/ipreston/miniconda3/envs/mortgage/lib/python3.10/site-packages/numpy_financial/_financial.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  (1 + rate*when)*(temp - 1)/rate)\n\n\n\nplot_hist(uncapped_scenarios, \"end_balance\");\n\n\n\n\n\nplot_hist(uncapped_scenarios, \"max_payment\");\n\n\n\n\nThe distribution of end balances looks reasonably similar, although the capped variable definitely has more weight on the lower end of the distribution, which is quite good. Unsurprisingly, the max payment is a much wider distribution than under the capped scenario, with a decent chunk of payments above what you’d do in the fixed rate scenario."
  },
  {
    "objectID": "posts/2022-03-17-mortgage.html#conclusion",
    "href": "posts/2022-03-17-mortgage.html#conclusion",
    "title": "Modeling out fixed closed vs capped variable mortgages",
    "section": "Conclusion",
    "text": "Conclusion\nI’m not sure I want to put a detailed conclusion here, that could sound like giving investment advice. Please keep in mind, that I’m not a mortgage/real estate/investment professional. I’m some guy on the internet who likes making charts and wants to buy a house soon. I found this process informative for my decision making, I think I’ll leave it at that."
  },
  {
    "objectID": "posts/2023-06-04-ansible-autofs.html",
    "href": "posts/2023-06-04-ansible-autofs.html",
    "title": "Configuring autofs for CIFS mounts with ansible",
    "section": "",
    "text": "Introduction\nThis guide shows how to set up user isolated mounts of CIFS (samba) network shares on a shared linux VM. Each user will get folders under /mnt/&lt;user&gt; that they authenticate to using kerberos. In this document I’m assuming that users are members of a group and that each group all should have access to the same shares. We’ll further assume you’ve got a fact set in your playbook that maps each user to their corresponding group. If you need guidance on that you can see this post.\nThis is a nice way of attaching file shares because it ensures users don’t need elevated privileges to access file shares (although an administrator has to configure it for them) and that creating a share for one user doesn’t inadvertently expose it to others. For a fully user level way of attaching file shares you can use gio but I found it extremely flaky and annoying to use, so if you can handle having an administrator configure the share mount points for users I would recommend this approach.\n\n\nVariable format\nSomewhere in your playbook (in my case I set it in the variables folder of the role I was using, but putting in as a host variable or somewhere else may be more appropriate for your use case) we need a variable for each group that contains a list of associated shares. Something like this:\nexample_group1:\n  - local_share_name: share1\n    full_share_path: \"//fileshare.example.com/share1\"\n  - local_share_name: share2\n    full_share_path: \"//fileshare.example.com/share2\"\n\nexample_group2:\n  - local_share_name: share3\n    full_share_path: \"//fileshare.example.com/share3\"\n\n\nBasic setup\nIn this stage we will ensure pre-requisite software is installed on the host (assuming Ubuntu here, you will have to modify for other distros), and that the mount point folder for each user has been created:\n- name: Install autofs\n  ansible.builtin.apt:\n    pkg:\n      - autofs\n      - cifs-utils\n      - keyutils\n\n- name: Create base file share mount point\n  ansible.builtin.file:\n    path: \"/mnt/{{ item.user }}\"\n    state: directory\n    owner: \"{{ item.user }}\"\n    group: \"domain users@example.com\"\n    mode: \"0700\"\n  with_items: \"{{ users_dict }}\"\n\n- name: Start and enable the autofs service\n  ansible.builtin.systemd:\n    name: autofs\n    state: started\n    enabled: true\nWhere users_dict looks like what I created in this post. You will also have to modify the group variable to be appropriate for your environment.\nI actually have the autofs service start task at the bottom of this play in my case, but thematically it makes more sense here.\n\n\nPopulate auto.master\n- name: populate auto.master with entries for each users' configs\n  ansible.builtin.template:\n    src: auto.master.j2\n    dest: /etc/auto.master\n    owner: root\n    group: root\n    mode: '0644'\nEach user needs an entry in /etc/auto.master that points to a config file (which we’ll set in the next phase) with all their specific mount points. Using the template task and the template below we can accomplish this:\n# {{ ansible_managed }}\n{% for user in users_dict %}\n/mnt/{{ user.user }} /etc/auto.sambashares-{{ user.user }} --timeout=30 --ghost\n{% endfor %}\nEach user gets a point below the /mnt/&lt;user&gt; folder we created in the basic setup, we point to a config file for them, set a timeout so the fileshare will not stay connected if users aren’t using it and then we add the --ghost flag so that all mount points get a directory created, even if they’re not currently attached. See here for further docs.\n\n\nPopulate user level share specs\n- name: Populate user specific share mounts\n  ansible.builtin.template:\n    src: auto.sambashares.j2\n    dest: \"/etc/auto.sambashares-{{ item.user }}\"\n    owner: root\n    group: root\n    mode: '0644'\n  with_items: \"{{ users_dict }}\"\nAgain we’re populating a template, but this time we’re doing one for each user. As with above, most of the magic happens in the template itself:\n# {{ ansible_managed }}\n{% set shares = lookup('vars', item.group | replace('-', '_')) %}\n{% for share in shares %}\n{{ share.local_share_name }} -fstype=cifs,rw,sec=krb5,uid=${UID},cruid=${UID} :{{ share.full_share_path }}\n{% endfor %}\nThe replace step is because a lot of the groups I was using had a - in their name, which you can’t have in an ansible variable so I map the - to an _. We can then use that to refer to the variable described at the top of this post for whichever group the particular user happens to be in. Then we just iterate through all the shares defined and create a folder under /mnt/&lt;user&gt;/&lt;local_share_name&gt; that maps to full_share_path and will be authorized with kerberos.\n\n\nConclusion\nAutofs and ansible are a pretty nice way to set up a bunch of users with consistent file shares securely on a shared host or multiple hosts."
  },
  {
    "objectID": "posts/2023-04-09-network-rework.html",
    "href": "posts/2023-04-09-network-rework.html",
    "title": "Redesigning my network",
    "section": "",
    "text": "Now that I have basic connectivity for my managed switch I need to figure out what I actually want my network to look like, and how I want to make it look that way."
  },
  {
    "objectID": "posts/2023-04-09-network-rework.html#list-all-the-types-of-devices-i-have-and-their-usage-considerations",
    "href": "posts/2023-04-09-network-rework.html#list-all-the-types-of-devices-i-have-and-their-usage-considerations",
    "title": "Redesigning my network",
    "section": "List all the types of devices I have and their usage considerations",
    "text": "List all the types of devices I have and their usage considerations\nTo think about what networks I need I have to consider what sorts of things I have on my network and how I’d like to organize them.\nTo start, I have the management interfaces for my networking devices. The router at least won’t be on its own network, I’ll just have to lock that down with firewall rules. I’m not entirely clear how locking down the management interface on the switch works, I could probably put that on its own VLAN, but maybe that’s overkill. Right now my unifi controller is running on the same server that’s running all my other services, so I can’t isolate it with VLANs, that will also be firewall rules I guess.\nNext there’s my servers and homelab. Currently there’s my three node proxmox cluster as well as a standalone box that’s running my services until I get things figured out on the proxmox cluster, but eventually that will be consolidated physically into one big cluster. Eventually within those physical servers there might be VMs representing different environments (dev/prod) that I might want to isolate. I’m pretty sure I can apply VLAN tags to VM interfaces, will have to test that to be sure.\nI’ve got my work laptop, which should really be isolated from everything else. It’s got a wired connection so at least I won’t have to make an SSID just for it.\nI’ve got IoT devices, although that’s a bit of a hand wavy category. The wifi leak sensor I previously mentioned doesn’t have to talk to anything else in my house so I can safely isolate it. My phone might be considered an IoT device, but I want it to at least be able to talk to my NAS so it can do photo backups. My Kobo probably counts as an IoT device, but there’s an ebook service that I run that I’d want it to connect to. This will require some thought and maybe some firewall rules on top of just network segmentation I think.\nI’ve got trusted devices for admin like my workstation and my laptop. Those can probably just go on the same network as my lab and servers.\nI’ve got my partner’s trusted devices like her laptop. At this point I’m not sure if she needs elevated privileges compared to house guests, but it also feels a bit weird putting her on a guest network in her own house.\nSpeaking of which, we’ve got the phones and laptops of any guests that visit us. Generally I think they just need internet access and can be isolated from IoT and server stuff.\nI’ve mentioned it in a few other places, but I also have my NAS. I’d like to block most networks from accessing its management interface, but several of them will have to access its file share, and it also has my photo service running on it.\nThe last piece is the virtual networks for VPNs. I have OpenVPN running to connect to an off site Synology NAS that’s my off site backup. For everything else I use wireguard, although right now there’s just one tunnel both for myself for administration and trusted friends that I want to access my services. I’ll have to split those out."
  },
  {
    "objectID": "posts/2023-04-09-network-rework.html#initial-network-idea",
    "href": "posts/2023-04-09-network-rework.html#initial-network-idea",
    "title": "Redesigning my network",
    "section": "Initial network idea",
    "text": "Initial network idea\nI’ll probably end up changing this, and I’ll definitely start with a subset of them while I’m testing, but let’s get the idea down.\n\nInfra LAN\nThis will have the management interface for my switch, my proxmox nodes, my NAS, and any VMs or physical servers running production services. I think I’ll also put my workstation on this LAN to make administering things easier. I’ll either have an SSID that’s attached to this network or have a wireguard tunnel that can connect to it. If I can make the wireguard tunnel work internally and externally I’ll go with that.\n\n\nSandbox VLAN\nThis will only be used by VMs in my proxmox cluster. This is less for security than for testing out services in an isolated environment that might conflict with production services. It might also be a good place to test out firewall rules or other capabilities without impacting services.\n\n\nTrusted devices VLAN\nThis will be for devices my partner or I own that we want to be able to access internal services. Laptops, phones, streaming boxes etc. If I want to limit the services some devices can access I’ll do it with firewall rules.\n\n\nGuest devices VLAN\nThis will be for IoT stuff in the house and guests. It should just be able to access the internet and I could even experiment with rules that don’t let devices on this network talk to each other, just for added security. I think for now at least I’ll put my work machine on this network as well, especially if I can get device isolation within this VLAN.\n\n\nInfra wireguard tunnel\nA wireguard tunnel for me to use to administer my network.\n\n\nTrusted guests wireguard tunnel\nA wireguard tunnel for guests I’ve granted access to specific services. Exact services can be set with firewall rules and similar to guest devices VLAN I can restrict within network communication (I think). Addendum: I ended up not going with this and just using firewall rules.\n\n\nOffsite OpenVPN\nOpenVPN connection to my offsite backup. Should only be able to communicate with my NAS in the infra VLAN.\n\n\nTrusted devices SSID\nWiFi connection for the trusted devices VLAN\n\n\nGuest devices SSID\nWiFi connection for the guest devices VLAN.\n\n\nSummary\nSo that’s four VLANs, two of which require SSIDs for WiFi, plus two wireguard tunnels and an OpenVPN tunnel. Compared to my current network of one LAN with one associated SSID, one wireguard tunnel, and one OpenVPN tunnel. Definitely more complex, but at least right at this moment this doesn’t feel like complete hubris."
  },
  {
    "objectID": "posts/2023-04-09-network-rework.html#figure-out-the-menu",
    "href": "posts/2023-04-09-network-rework.html#figure-out-the-menu",
    "title": "Redesigning my network",
    "section": "Figure out the menu",
    "text": "Figure out the menu\nGetting into the menu interface is easy from the cli, just enter menu from the command prompt. From the menu interface option 5 takes me back to the CLI. Let’s take a look around at the things I’ll want to use.\n\nStatus and Counters\nUnder Status and Counters and then General System information I get some traffic info, the MAC address, and the firmware version of the switch (which looks like it’s at the latest version). Under Switch Management Address Information I see the management IP and gateway info I set. I also notice that the time server address isn’t set. I should probably change that since right now the switch thinks it’s January 1990. Add that to the todo list. Under Port Status I can see that only the port I’ve connected to pfsense is up. All the ports look basically the same except they’re a mixture of MDI and MDIX for MDI Mode. I have no idea what that means. Based on this post it’s related to the type of cable wiring you’re using. There’s also apparently an option to have it automatically configured. I’ll keep this in mind but for now let’s assume my switch defaults to auto configure that and it’ll just work, but keep this in mind for troubleshooting later. VLAN Address table is empty but has columns for MAC address and Located on Port so that might be handy to come back to later. Port Address Table lets me pick a port and then pulls up a table listing MAC address, but it’s empty. Leave that for now.\n\n\nSet time server\nHopping over to switch setup I set the Time Sync method to TIMEP, the mode to Manual and set the server address as my gateway to use the pfsense time server. Setting that didn’t change the time in the menu. I think maybe I need SNTP, which according to this is interoperable with the NTP protocol pfsense uses (I don’t have an option for NTP on the switch). Setting that up still doesn’t seem to be updating the time. Maybe I have to wait for a refresh? You’d think it would trigger one automatically when you change the config. How can I test this? Searching around doesn’t find anything. Let’s not get too sidetracked. I’ll set a timer for 12 minutes (default polling interval is 720 seconds) and check back on this later.\nComing back after 12 minutes the time is showing up correctly on the switch. Glad I didn’t spend a bunch of time troubleshooting that, although it’s silly it doesn’t try and sync after a config change automatically.\n\n\nSwitch configuration\nThis is the menu where most of the action will be for me. The first entry System Information is just what you get when you run the setup command so I’ve already been there. It’s where I configure the switch IP, default gateway and time settings (or try to at least).\nUnder Port/Trunk Settings I see that all my ports are set to enabled and Mode is set to auto, so that probably means I don’t have to worry about that whole MDI/MDX thing. I can also group ports into trunks here, which is probably where I’ll need to be to enable LACP. The interface is annoying, I have to hit space to toggle through 24 trunk groups before I can get back to the default of being empty. I can see why I’d want to use the CLI if I was going to do this a lot.\nNetwork Monitoring Port is disabled and that’s probably fine for now. IP Configuration has similar settings to what I did under System Information so I don’t need to mess with it.\nSNMP Community Names is a new thing for me. Based on this post I’m just going to leave it alone for now.\n\nVLAN Menu\nVLAN Menu seems like it will be of interest to me, let’s see what I can do here.\nUnder VLAN Support I can set the maximum number of VLANs to support. The default of 8 is more than the 3 I currently think I need so let’s leave that. Primary VLAN is set to DEFAULT_VLAN which from my preliminary reading should be VLAN1. I think that’s all fine, just note it for now. The last piece under here is whether GVRP is enabled, and by default it isn’t. From reading this that seems to be what I want so let’s move on.\nUnder VLAN Names I’ve currently only got ID 1 associated with DEFAULT_VLAN so that confirms my suspicion there. I’ll come back to this and add trust, guest, and lab VLANs later. Under VLAN Port Assignment I can tag various ports with the VLAN tags I’ve created. Again, that will come in handy eventually."
  },
  {
    "objectID": "posts/2023-04-09-network-rework.html#figure-out-the-cli",
    "href": "posts/2023-04-09-network-rework.html#figure-out-the-cli",
    "title": "Redesigning my network",
    "section": "Figure out the CLI",
    "text": "Figure out the CLI\nWhile it seems like I could do everything I need to do from the menu, let’s give me another option with the CLI.\n\nPort status: show interfaces brief nicely pages through the port status I saw in the menu. I think checking that from the menu is nicer since I can scroll up and down easily but it’s nice that it’s there. I can also check specific ports by adding a number, a comma separated list of numbers, or a dashed number range to just see a few ports. That might be handier.\nPort/Trunk settings: I can use the show trunks command to show configured trunks, with the option to add a port range for just certain ports. I can also run show lacp just to see LACP configured ports. I should also be able to use trunk &lt;port-list&gt; &lt; trk1 ... trk24 &gt; &lt; trunk | lacp &gt; to configure a trunk. A nice word of caution at this point, the docs strongly recommend not having these ports connected when you’re configuring this. So I’ll have to do this over serial with the uplink ports disconnected until both pfsense and the switch are configured.\nVLAN stuff: VLAN stuff is apparently not included in my docs. Weird. After grabbing the “Advanced traffic management guide” for the switch I’m ready to go. show vlans lists all the VLANs I have configured, currently just the default one. show vlan &lt;vlan-id&gt; does what you’d expect. For instance if I do show vlan 1 all my ports show as untagged which means untagged packets that they receive will be treated as part of vlan1. vlan &lt;vlan-id&gt; [name &lt;name-str&gt;] will either create a vlan with the specified ID and name, or enter me into the context of that vlan if it already exists. I think that’s all I actually need to do at this point."
  },
  {
    "objectID": "posts/2023-04-09-network-rework.html#get-sidetracked-on-issue-with-wsl",
    "href": "posts/2023-04-09-network-rework.html#get-sidetracked-on-issue-with-wsl",
    "title": "Redesigning my network",
    "section": "Get sidetracked on issue with WSL",
    "text": "Get sidetracked on issue with WSL\nOn my laptop plugged into a port other than 14 I pull a LAN IP address. I’m able to acces the admin console of pfsense, and I can ssh in.\nUnplugging from that port and moving over to 14 I initially seem to have my old IP. After unplugging and plugging back in one more time now I’m pulling 169.254.49.68, so something appears to be broken. I bet I should have set port 14 to “Untagged” for Guest VLAN. My laptop obviously isn’t applying VLAN tags, that will make more sense for ports connected to proxmox where I will be adding tags on VMs. Back to port 14. I pull 192.168.30.100! Great start. Ok but I can’t connect to the internet or even ping my default gateway at 192.168.30.1. That’s less good. Let’s check my firewall rules. Back over to a regular port so I can actually do that. Looking at the rules it looks like all the traffic is blocked by my private networks rule. I don’t see offhand why that would be the case, but let’s disable it and confirm that’s the issue. Disabling it didn’t fix things. Now I’m noticing that my allow all rule was actually set to just allow TCP, so maybe that’s the problem? Ok, with that fixed I can access the internet. That’s a good start. I can’t connect to the admin interface for pfsense, which is also intended behavior. I can’t seem to get online from within WSL though. I wonder if that’s something about the connection not being established with that VLAN tag originally? It shouldn’t be a firewall rule right now since I haven’t turned the private networks rule back on. I do a reboot just to be safe. Nope, that wasn’t it. I can’t even ping 192.168.30.1 from WSL. That’s super strange.\nThis requires a better setup for testing. Something very weird is going on and I don’t want to try and solve it standing in my utility room. Fortunately I have a spare port in my office upstairs, so I patch that one to the .30 VLAN port and my workstation to a port on the switch that doesn’t have VLANs assigned. Back in the office I confirm that I still have the same behaviour from my laptop on the .30 and my workstation works correctly on .10.\nBack on the laptop, just for kicks mostly I try and connect to the network with docker (which is running on top of WSL2) and it works?! Now my mind is really blown.\nLet’s try another experiment then. I’ll swap my workstation over to the VLAN port and see if its WSL can connect. Is it just something weird I didn’t realize I did on my laptop? Nope, exact same behaviour. Windows works fine, docker works fine, Ubuntu WSL does not.\nTime for some searching. There’s this GitHub issue which describes similar behavior but it’s a couple years old with no resolution. They do have a request to collect and provide logs, depending on what I find I might come back and contribute to this. There’s this GitHub issue but in this case they’re trying to add VLAN tags on the network adapter for the Windows machine. The root cause might be similar, but the scenario isn’t quite the same, and there’s no resolution listed anyway. There’s this GitHub issue about applying a VLAN ID to the WSL network interface. That might work and might be worthwhile for testing but I’d like to see if there’s a cleaner fix. There’s this GitHub issue that says the 8021q module isn’t available in WSL. That appears to be true for me, but shouldn’t be relevant since I’m not trying to add my own VLAN tags, I’m just having the switch assign them.\nWell I’m running real low on ideas at this point so back at the first issue I run their recommended log gathering script and put it up on the issue in a gist. I also add the note about docker working ok in case that’s a relevant clue to anyone who knows more about WSL and networking than me. In the meantime let’s take a look through the testing output and see if anything jumps out.\nI notice that I can’t even ping the internal gateway of the WSL virtual network. I check that on my workstation and I can’t do it there either though, but it’s able to get online and talk to other devices in my network. I also can’t ping the Windows host IP, but I can’t seem to do that from anywhere, including pfsense itself so I’m not sure what to make of that.\nRunning traceroute on WSL without the VLAN I can see it hit the internal WSL gateway, then my 192.168.10.1 gateway, then the internet. Running it again on the WSL that’s on the VLAN it makes it to the WSL gateway (even though I can’t ping it) and then times out, it can’t make it any further. Let’s try that within docker on the machine with a VLAN to see if that shows anything. After loading the container with docker pull ubuntu && docker run -it ubuntu /bin/bash and installing the tools I need with apt update && apt upgrade && apt install inetutils-traceroute inetutils-ping I run traceroute on the machine behind a VLAN. It doesn’t work? I reach the default docker network gateway of 172.17.0.1 ok, head on to a gateway of 192.168.65.5 which is super weird because I don’t have that subnet configured anywhere and then time out. But I can still ping out to the same internet site I was trying. Same thing for an internal server. I can ping it and resolve the correct internal IP, but traceroute gets hung up at 192.168.65.5, which from some searching is the docker DNS server. Let’s try the same thing on the machine that’s not behind a VLAN. Same behavior. What. Let’s try traceroute from the WSL of the machine that’s not behind a VLAN. Works totally fine. What is happening with these networks?\nAt this point I have to step back from this issue. There are no resolutions on GitHub, and I’ve added my logs and comments to the issues in case anything comes up. I’ve posted on Reddit and serverfault with no helpful response.\nFortunately, as long as docker works the impact of this on me is actually fairly minimal. I’ll keep my workstation in the Infra LAN without a tag so it will be fine. My laptop won’t be able to connect with WSL but I do almost everything Linux related on it from devcontainers anyway. I might have to make an Infra SSID when I get to the wireless step, just to have somewhere to connect from my laptop, but I don’t expect to need it often. As inconvenient as this is I don’t think it’s a showstopper so I’m going to move on. Maybe I’ll learn some more in the meantime that will be helpful.\nNote: Below I find that this behaviour doesn’t reproduce over WiFi. I’m so confused."
  },
  {
    "objectID": "posts/2023-04-09-network-rework.html#carry-on-with-vlan-setup",
    "href": "posts/2023-04-09-network-rework.html#carry-on-with-vlan-setup",
    "title": "Redesigning my network",
    "section": "Carry on with VLAN setup",
    "text": "Carry on with VLAN setup\nAt this point I think it’s worth setting up the basics of what I need in terms of VLANs. I’ll save the firewall rules for later, but I’ll at least create the tags and interfaces. In the switch interface I’ve already got my Infra VLAN (1, default) and Guest (30) VLAN names created so I just have to add Trust (15) and Lab (40). Then it’s down to VLAN port assignment. I’ve got to update my trunk port to allow the new VLANs I’ve created, I’ll do that first since it’s at the bottom of the list and otherwise I might forget it. The next two ports I’ll use for my wireless access points. I want the APs themselves to be on my infra network so I’ll set the default VLAN to untagged. They’re also going to be creating guest and trust networks when I get to that point so I turn tags 15 and 30 on. I don’t see anything in my lab/dev environment being wireless so I’ll leave that off for now. The next four ports I’ll eventually use for my NAS. It’s got four connections on it so I can have one for each network. I originally thought about just putting them all into infra with link aggregation, but after watching some more Lawrence Systems videos I realized that it makes more sense to have them on each network directly so I’m not putting load on my router whenever I’m using the NAS, as I would be if the NAS was on Infra and most of the devices accessing it were on trust or guest. With that in mind for the next four ports I’ll set each one to untagged for a single VLAN (default/infra, trust, guest, lab in order) and No for the other VLANs. Next up we’ve got my three proxmox nodes. Those should be on infra by default, but I want to be able to add lab VMs, so I’ll turn VLAN 40 on. I don’t think anything in there makes sense for trust or guest, so I’ll leave that off. Just a handy reminder for myself here, the options auto and forbid are for if GVRP is enabled on my switch, which as discussed above, it is not. The next two ports are for my current standalone server and my workstation, both of which I’m putting on infra, so the default VLAN gets left as untagged and the other VLANs are set to No. The last two devices are my work computer and a Hue bridge for my lights, both of which belong on guest, so I set that VLAN to untagged and No for the other VLANs.\nNow over on pfsense I have to create the VLANs, add DHCP for them, and (for now) give them a nice open “allow all” type firewall rule. The process is the same as what I described in the guest VLAN above so I won’t write it out again."
  },
  {
    "objectID": "posts/2023-04-09-network-rework.html#proxmox",
    "href": "posts/2023-04-09-network-rework.html#proxmox",
    "title": "Redesigning my network",
    "section": "Proxmox",
    "text": "Proxmox\nChanging proxmox might be tricky since it uses static IPs. Presumably if I go in and change my network config I will lose connectivity until I move the host over to the new network. This will probably also do fun things to my cluster and ceph setup. That’s ok though, I’m not running anything production on there yet, that’s part of why I wanted to do this network rework now.\nThe first thing I do is remove the static leases I was using in pfsense to ensure name resolution of the proxmox hosts. This means I have to connect in from their IPs, but that’s ok. I’ll add in name resolution again later. Now on the proxmox hosts, I’ll do this one at a time. On the first one under system I go to DNS and update the DNS server to the new gateway. I add the old one in as a secondary one for now. Next I modify the hosts section to identify the new IP I’m going to give this host (192.168.10.11, I’ll leave 2-10 for more foundational infrastructure and start in the tens to match the PVE node number). Finally, and here’s where things will break until I switch ports, I go to the network section and update the vmbr0 interface to the new address range. I get prompted to either apply changes or reboot. A reboot seems safer so I go for that and while that’s happening I head downstairs and move it over to the correct port on the switch. Backupstairs I can access it again from the new IP! It can no longer see the other two nodes, so maybe there’s something about not clustering across broadcast ranges. That’s fine, I’ll update the other two and then work on getting ceph set back up. After connecting the second node back it gets the correct IP and can join, but it can’t see the first node. Going up to the datacenter page in proxmox it looks like all the nodenames are still pointing to the old IP addresses, maybe name resolution isn’t working in pfsense?\nI know I could do host overrides in the DNS server settings in pfsense, but I like assigning static leases to devices so I can see all the IP addresses I’m using from the DHCP page. There might be other things I can do with DNS to auto identify hostnames but I’m going to save that for later (either later in this post or another post). In pfsense I apply static leases for the two nodes I’ve moved over (glad I copied that when I deleted their leases on the old network). Doing this allows me to ping and correctly resolve the name for the other node but I still don’t see them in the cluster. In the proxmox docs it looks like I have to edit /etc/pve/corosync.conf. There are also some handy docs on editing corosync, which include incrementing the version. I can’t follow them though because even as root the file is read only. This post is from someone having the exact same issue as me, it’s because my nodes don’t have quorum because I took them down so cluster settings get locked. That’s totally sensible, probably should have thought of that before I tried migrating. Reading these docs it looks like I can remove nodes from the cluster, but then I’m going to have a bad time and have to reinstall them to get them back in. Let’s back up and try doing this a bit more gracefully. I’ll reset the first two nodes to their old address and put them back on the old switch just to make sure I can get back to a known good state and then more slowly move them over. Back in pfsense I remove the static mappings for the two nodes on the infra network and add then back in the legacy network. I change the host config and IP settings on the nodes and reboot them. Down to the utility room to plug them back into the old switch. Ok, we’re back up with quorum. Now to figure out the smart way to do this migration. Adding redundant links feels like it should work, but the different links can’t talk to each other, so I’m not sure if I’ll get in a weird in between state partway through. I guess it should recover once all three nodes are on the new network. Let’s give it a shot. Given that this is already risky let’s follow the recommendations in the docs for editing corosync. On my first proxmox node I copy the current corosync config into a .new and .bak copy:\ncp /etc/pve/corosync.conf /etc/pve/corosync.conf.new\ncp /etc/pve/corosync.conf /etc/pve/corosync.conf.bak\nThen I edit the new file. For each node I add an entry for ring1_addr: with the IP I’m migrating to. In the totem I increment the config_version: field. Finally I duplicate the interface section with link number 1. Let’s just check it on another node to make sure things are syncing properly. Yup, it’s over on my other node. Move the .new file to overwrite the original config and I should be good to go. Let’s try migrating again.\nI move the static leases over from legacy LAN to Infra. This time I’m going to change all three nodes over at once and power them off, move the network cables over, and then bring them all back up. Let’s see if it works.\nThey came back up, and I can ping all of them at their new address. I can even ssh into them, but the web interface isn’t loading. So that’s fun. Ok, even weirder. I actually just can’t access the web interface for the first node. All three show as joined to my datacenter though and I can access the first node from the web interfaces of the other two. That’s pretty close to functional, just have to figure out this first web interface. Maybe it just needs a reboot? Worth a shot at least. Ok, that did it. Not really sure why that did it, but who ever knows why rebooting fixes things?\nLast up, let’s get rid of the old addresses from the corosync config. I make another .new copy, edit it to remove the old ring0 address and change the updated ring1 address to be ring0, increment the config_version and get rid of the second interface. Checking the cluster info from the datacenter page I see all three nodes using the correct IP on the first interface. Success!\n\nCEPH\nI’m sure all these address changes have done interesting things to my CEPH cluster. Let’s try and get that back on track now. On one of my nodes I head over to the CEPH tab and check the configuration page. I can’t seem to update anything from the web portal so let’s try making changes in the terminal to /etc/pve/ceph.conf. I update cluster_network, mon_host, public_network and the mon settings for each node.\nStill getting timeouts from CEPH, probably because it has to reload something? Let’s just reboot all the nodes again to be safe and see what happens. Still nothing. The config seems to have been applied but I don’t see anything. Running pveceph status or ceph -s just times out. Looking back on my previous effort let’s see if I can find some good troubleshooting steps. The first thing I did was initialize ceph if pveceph status showed “not initialized” so I’m going to skip that. pgrep ceph-mon shows no monitors running but pgrep ceph-mgr and pgrep ceph-osd both show processes. Interestingly in the web interface I can see all three monitors, just with status “unknown”, but I can’t see any managers or OSDs.\nJust as an aside here. I recognize I’m almost certainly going to spend more time troubleshooting this than I would just rebuilding the cluster. Especially since I went to all that effor to configure things with ansible. I’m treating this as a learning opportunity, not a productivity hack.\nReviewing the docs I find this handy warning that existing monitors are not supposed to change their IP address. From reading this I’m basically hooped unless I move all three nodes back to the old IP address range and even then I’m not sure I could painstakingly migrate one node at a time without losing quorum on either my ceph cluster or my proxmox cluster. In summary, don’t expect to be able to migrate a proxmox cluster over to a new address range, it’s going to be a rebuild.\nI still want to try a bit more here before I give up on rebuilding ceph. Mostly because I’m stubborn, partially because it will help me understand how ceph works.\nChecking what I still have running for ceph related processes I find the following:\nroot@pve3:~# ps ax | grep ceph\n   1621 ?        Ss     0:00 /usr/bin/python3.9 /usr/bin/ceph-crash\n   2478 ?        Ssl    0:01 /usr/bin/ceph-osd -f --cluster ceph --id 0 --setuser ceph --setgroup ceph\n   6658 pts/0    S+     0:00 grep ceph\nThe result is the same on all three of my nodes. Taking a look at this forum gives me a tip to check the status of ceph-crash which in turn gives me this error:\nApr 15 13:17:33 pve1 ceph-crash[1421]: ERROR:ceph-crash:directory /var/lib/ceph/crash/posted does not exist; please create\nAfter creating that directory I don’t see any records getting created.\nLooks like I can stop the running ceph-osd with systemctl stop ceph-osd.target I still have the ceph-crash service running and even if I kill it the process comes back, so it’s being triggered by some service. Running systemctl --type=service --state=running I can see there’s the ceph-crash service so I stop that on each node. After running that it doesn’t seem like I have any more CEPH services running. Let’s try a purge again.\nroot@pve3:~# pveceph purge --crash --logs\nError gathering ceph info, already purged? Message: got timeout\nForeign MON address in ceph.conf. Keeping config & keyrings\nSo no luck there. Let’s try this post and run the following on each node:\nrm -rf /etc/systemd/system/ceph*\nkillall -9 ceph-mon ceph-mgr ceph-mds\nrm -rf /var/lib/ceph/mon/  /var/lib/ceph/mgr/  /var/lib/ceph/mds/\nrm /etc/pve/ceph.conf\nNow I need to wipe my disks to get ready to start again. Initially when I try and wipe it from the menu I get a warning about the device having a holder. From this forum post I learn to run lsblk | grep ceph to find the id and then dmsetup remove &lt;the id I found&gt;. After this I can wipe the disks.\nLet’s see if I can rebuild the cluster using my ansible playbook from last time. After modifying it with the new network addresses of course. I can initialize the ceph cluster but creating monitors fails because I don’t have /var/lib/ceph/mon to create monitors in. Fine, I create that folder on each node and then try adding monitors. Same thing fore creating managers, have to create /var/lib/ceph/mgr, might as well recreate /var/lib/ceph/mds while I’m at it. This whole experience is really making me rethink integrating distributed storage with my hypervisor. Let’s try creating an OSD. Here’s where things fall apart again: auth: unable to find a keyring on /etc/pve/priv/ceph.client.bootstrap-osd.keyring: (2) No such file or directory\nOk. That’s enough of this. This is way off topic for figuring out my network and it’s pretty clear that I’m going to have to come back and figure out my distributed storage solution again."
  },
  {
    "objectID": "posts/2023-04-09-network-rework.html#nfs",
    "href": "posts/2023-04-09-network-rework.html#nfs",
    "title": "Redesigning my network",
    "section": "NFS",
    "text": "NFS\nAfter all that I realized my NFS share wasn’t loading anymore. After thinking for a second I realized that made sense since my NFS rules only allowed connections from the 192.168.85.0/24 range. Adding a new rule for the infra range fixed that up. Addendum: I did this for my proxmox shared folder but not for the rest of them here, this bites me later in the NAS migration section."
  },
  {
    "objectID": "posts/2023-04-09-network-rework.html#vms-with-vlan-tags",
    "href": "posts/2023-04-09-network-rework.html#vms-with-vlan-tags",
    "title": "Redesigning my network",
    "section": "VMs with VLAN Tags",
    "text": "VMs with VLAN Tags\nFor now let’s make sure creating VMs with VLAN tags works the way I think it should and move on. I’ll have to come back to refiguring my storage in a future post. First I create a VM with no VLAN tag assigned to the network interface and DHCP settings. It comes up and gives me an IP in the correct range. I could just create a new one, but let’s see what happens if I just shut this down and change the VLAN tag on the interface? On the hardware tab I go to the network interface and give it a VLAN tag of 40. It pulls the new IP when it comes back up and works perfectly. Finally something goes well!"
  },
  {
    "objectID": "posts/2023-04-09-network-rework.html#migrate-synology-and-prod-server",
    "href": "posts/2023-04-09-network-rework.html#migrate-synology-and-prod-server",
    "title": "Redesigning my network",
    "section": "Migrate Synology and prod server",
    "text": "Migrate Synology and prod server\nNow that I have my VLANs setup, it’s time to migrate my NAS over. To start I’m going to shutdown my production server so it doesn’t get any weird behavior while things are shifting over. Once I’ve got the Synology back up I’ll migrate it to the new Infra LAN as well. I can head into the “Info Center” and then Network section of the Synology control panel to get MAC addresses for each interface. This will allow me to create static routes in pfsense to give correct IP addresses. On the infra LAN the next free IP address after the gateway and switch is 192.168.10.3 so I’ll assign that to the NAS. I’ll keep the same pattern of making it 192.168.x.3 on each network for consistency. For hostnames I’ll just use the name I’ve assigned the nas on infra, and all other networks I’ll prepend the network name to distinguish. After adding in all the static mappings and confirming the order I set the ports on the VLAN it’s time to head down to the utility room and move some cables around. When I come back upstairs I’m delighted to see that I can ping all the interfaces at the IP I expected. I have to flush my DNS cache before I can ping the original hostname on the infra IP, but otherwise this part is smooth sailing. Let’s bring up my prod server and see if all my services still work. It came back up ok and it picked up the new IP, but only some of my docker containers are coming back. Again, that’s out of scope for this so I’ll just re-run my ansible playbook for setting up this machine and see if that resolves it. Ok, the problem was that I had volumes specified with reference to the old IP address. Even though the associated containers weren’t running, they still had a claim on those volumes and so ansible couldn’t remove and re-add them. After stopping all containers and removing them I re-ran the playbook and realized I had to update the NFS share permissions on my other shares to allow access from the Infra network. This was actually one of the reasons I wanted to migrate over. NFS is not secure (at least without some fancy user auth tied in that I’ve never bothered to setup) so having it only available on my infra network and using SMB everywhere else will enhance my security. While I was at this I moved my media box over to my guest network and after a bit of fiddling with kodi settings to point to the new share that’s on the guest network things all came back up."
  },
  {
    "objectID": "posts/2023-04-09-network-rework.html#migrate-the-last-couple-wired-devices",
    "href": "posts/2023-04-09-network-rework.html#migrate-the-last-couple-wired-devices",
    "title": "Redesigning my network",
    "section": "Migrate the last couple wired devices",
    "text": "Migrate the last couple wired devices\nBeyond what I’ve moved over so far the only wired devices I have left are my work computer, a Hue bridge for lights, and my access points. I’ll deal with the access points when it’s time to migrate the SSIDs in the coming section. The last two devices went smoothly and joined the guest network as I’d expect them to. I still haven’t set any firewall rules so the guest network isn’t actually any more/less secure right now, but at least they’re joining the correct network and nothing is immediately breaking."
  },
  {
    "objectID": "posts/2023-04-09-network-rework.html#wireguard-rules",
    "href": "posts/2023-04-09-network-rework.html#wireguard-rules",
    "title": "Redesigning my network",
    "section": "Wireguard rules",
    "text": "Wireguard rules\nI’m not going through these in any particular order beyond the order they happen to be in for my firewall rules tabs. I currently just have an allow all type rule on this interface so I need to restrict things a bit. The first rule I’ll add is to allow my laptops to access anything. I’ll make that the top rule and then I can add other rules below. Anyone else using this service is accessing either my file share over samba or one of the production services I’m running on my standalone machine. I add allow rules for samba by opening port 445 to my NAS IP, and the “web” alias port to my server. I also better allow access to this firewall since it’s providing DNS if nothing else, but I’ll block the admin ports first and then create an allow rule for the firewall generally. Rules are evaluated top to bottom so the block on admin ports will apply and then the allow on all other ports will follow. There might be a way to combine those rules but I’m going to try this for now. I don’t want anything else to happen through VPN so I can finally disable the allow all rule I had to start (I could just delete it but whatever).\nTesting the rules from my phone on LTE work the way I’d expect. I can access my web portals but I can’t get into the firewall web interface. I tried testing my laptop tethered to my phone in hotspot mode but couldn’t get it working. I couldn’t even get regular web traffic happening while the VPN was on. Wireguard is set for split tunneling so I wonder if that’s something about the hotspot having protections in place rather than an actual restriction. I’m going to leave testing the rest of this for now. Next time I’m out with my laptop I’ll try some other tests. Coming back to this after some reboots this is working as expected so I’m going to leave it alone."
  },
  {
    "objectID": "posts/2023-04-09-network-rework.html#guest-lan-rules",
    "href": "posts/2023-04-09-network-rework.html#guest-lan-rules",
    "title": "Redesigning my network",
    "section": "Guest LAN rules",
    "text": "Guest LAN rules\nFirst rule I need is blocking admin access to the firewall, using the alias I already created. Next I’ll create an alias for all my private networks, which is everything except this one and the WAN and block access to that. After that I just need an allow all rule. This one’s actually pretty straightforward rules wise. One more thing I have to open is access from my Kodi box to the port on my prod server where I’m hosting its mariadb database. I also have to add a rule so my kobo can talk to my calibre server.\nTesting these rules I’m delighted to find that everything still works. My robot vacuum needed a reboot when I loaded it on the app, but maybe that was a coincidence?"
  },
  {
    "objectID": "posts/2023-04-09-network-rework.html#trust-lan-rules",
    "href": "posts/2023-04-09-network-rework.html#trust-lan-rules",
    "title": "Redesigning my network",
    "section": "Trust LAN rules",
    "text": "Trust LAN rules\nThis one should be easy as well. I block access to the admin portal on the router, create an alias for my switch and access points and block that as well.\nThis works as expected. As a bonus, on my laptop if I connect through my wireguard tunnel I can access the router admin page, but I can’t without the tunnel, so the extra permissions on Wireguard seem to work even if I’m still on my local network as well."
  },
  {
    "objectID": "posts/2023-06-04-rootless-docker.html",
    "href": "posts/2023-06-04-rootless-docker.html",
    "title": "Configuring Rootless docker with ansible",
    "section": "",
    "text": "This is a write up summarizing the process I went through at work to configure Linux hosts with rootless docker. After figuring out the manual way to do things I further automated it with an ansible playbook, as there’s a lot of per-user stuff you have to do that quickly becomes untenable to do manually, even if you’re only doing it on one host, and really out of hand if you have multiple hosts.\nThis document will outline the key steps for configuring rootless docker for users and the associated ansible tasks required for it. I’m not going to show every aspect of setting up ansible like creating the inventory of hosts, just the components that are specific to rootless docker.\nI previously documented the manual approach I took while figuring all this out in this post.\n\n\nBriefly, let me describe what motivated this approach. At work we have a number of teams that want to use docker, either for a development environment in devcontainers, building containers for deployment, or both. All of our laptops run Windows, so the immediate obvious solution would be to install docker desktop. Unfortunately, that installation required turning on some services that we had disabled for security reasons, so we were not able to proceed with that approach. The next option would be docker on a remote Linux host. The traditional way of installing docker means that anyone who has access to work with docker effectively has root access to the system they’re running it on. This obviously presents a security issue on a shared machine, and the cost and complexity of giving every user their own VM was not practical, particularly for users that required GPUs for some of their workloads. Given these constraints, I set out to configure rootless docker so that multiple users could securely share a remote Linux instance and work in docker without security concerns. This has the added benefit of allowing users to do things like stop all their running containers with docker container stop $(docker container ls -aq) without stopping everyone else’s."
  },
  {
    "objectID": "posts/2023-06-04-rootless-docker.html#why-rootless-docker",
    "href": "posts/2023-06-04-rootless-docker.html#why-rootless-docker",
    "title": "Configuring Rootless docker with ansible",
    "section": "",
    "text": "Briefly, let me describe what motivated this approach. At work we have a number of teams that want to use docker, either for a development environment in devcontainers, building containers for deployment, or both. All of our laptops run Windows, so the immediate obvious solution would be to install docker desktop. Unfortunately, that installation required turning on some services that we had disabled for security reasons, so we were not able to proceed with that approach. The next option would be docker on a remote Linux host. The traditional way of installing docker means that anyone who has access to work with docker effectively has root access to the system they’re running it on. This obviously presents a security issue on a shared machine, and the cost and complexity of giving every user their own VM was not practical, particularly for users that required GPUs for some of their workloads. Given these constraints, I set out to configure rootless docker so that multiple users could securely share a remote Linux instance and work in docker without security concerns. This has the added benefit of allowing users to do things like stop all their running containers with docker container stop $(docker container ls -aq) without stopping everyone else’s."
  },
  {
    "objectID": "posts/2023-06-04-rootless-docker.html#create-etcsubuid-and-etcsubgid",
    "href": "posts/2023-06-04-rootless-docker.html#create-etcsubuid-and-etcsubgid",
    "title": "Configuring Rootless docker with ansible",
    "section": "Create /etc/subuid and /etc/subgid",
    "text": "Create /etc/subuid and /etc/subgid\nThe next thing we do is configure which UID and GID ranges on the host machine should be uniquely mapped for each user into their docker daemon. We want to reserve a range of IDs for each user so that permissions for a user within a container do not provide privilege escalation outside the container. Just as an aside, in a rootless runtime, UID 0 or root inside the container maps to the user that is running docker and their UID outside the container, so be sure to run your containers as root if you have any volumes bind mounted and don’t want to have to deal with weird permission issues.\n- name: apply subuid and subgid settings for mapping\n  ansible.builtin.template:\n    src: subid.j2\n    dest: /etc/{{ item }}\n    owner: root\n    group: root\n    mode: '0644'\n  with_items:\n    - \"subuid\"\n    - \"subgid\"\nThe task itself is quite straightforward, where the magic happens is in the template:\n# {{ ansible_managed }}\n{% for user in users_dict %}\n{{ user.user }}:{{100000 + (loop.index0 * 65536)}}:65536\n{% endfor %}\nAs mentioned, for each user we want a non-overlapping range of UIDs. In the docker docs they give each user a range of 65536 UIDs to use and start at 100000, which we reproduce here. The format of each entry is username:start UID range:size of range. We ensure this is non overlapping by multiplying the index of the loop we’re on by the size of the UID range. /etc/subuid and /etc/subgid have the exact same format so in the playbook we just apply the same template to both files."
  },
  {
    "objectID": "posts/2023-06-04-rootless-docker.html#stop-the-root-level-docker-service",
    "href": "posts/2023-06-04-rootless-docker.html#stop-the-root-level-docker-service",
    "title": "Configuring Rootless docker with ansible",
    "section": "Stop the root level docker service",
    "text": "Stop the root level docker service\nThis will conflict with the user level docker service, so we have to ensure it’s stopped:\n- name: Make sure the root level docker service is stopped and disabled\n  ansible.builtin.systemd:\n    name: \"{{ item }}\"\n    state: stopped\n    enabled: false\n  with_items:\n    - \"docker.service\"\n    - \"docker.socket\""
  },
  {
    "objectID": "posts/2023-06-04-rootless-docker.html#create-home-directories-for-each-user",
    "href": "posts/2023-06-04-rootless-docker.html#create-home-directories-for-each-user",
    "title": "Configuring Rootless docker with ansible",
    "section": "Create home directories for each user",
    "text": "Create home directories for each user\nThis is potentially being run on a newly created machine with users from Active Directory. Because of this, the users may not have a home directory created for them before they log in, so we have to ensure it’s created in order to copy later user level config files into it. We also create an ansible temp directory at this stage to suppress a warning.\nI’m not totally sure the home directory creation needs to be done as a separate task, since the temp directory will create all parent folders necessary, but I wrote the first task before I realized I needed the second, and it’s nice to separate out the reasons for each step.\n- name: Make sure home directory exists\n  ansible.builtin.file:\n    path: \"/home/{{ item.user }}\"\n    owner: \"{{ item.user }}\"\n    group: \"domain users@example.com\"\n    state: directory\n    mode: '0755'\n  with_items: \"{{ users_dict }}\"\n\n- name: Make sure the ansible temp dir exists for each user\n  ansible.builtin.file:\n    path: \"/home/{{ item.user }}/.ansible/tmp\"\n    owner: \"{{ item.user }}\"\n    group: \"domain users@example.com\"\n    state: directory\n    mode: '0700'\n  with_items: \"{{ users_dict }}\"\nYour value for group will likely be different, but you get the idea."
  },
  {
    "objectID": "posts/2023-06-04-rootless-docker.html#create-an-entry-in-etcpasswd",
    "href": "posts/2023-06-04-rootless-docker.html#create-an-entry-in-etcpasswd",
    "title": "Configuring Rootless docker with ansible",
    "section": "Create an entry in /etc/passwd",
    "text": "Create an entry in /etc/passwd\nThis is another feature of using domain users. Domain users don’t appear to automatically get an entry in /etc/passwd that lists things like their default shell. Even though users may have their default shell set to bash by PAM or whatever else, VS code doesn’t seem to recognize this without an /etc/passwd record, which causes it to try and run devcontainers through /bin/sh, which means your ~/.bashrc doesn’t get loaded, which causes problems you’ll see in future steps. The TLDR is we want to manually create a record for each user in /etc/passwd. If you’re not dealing with users managed by AD then you can probably skip all this.\n- name: Get lines for /etc/passwd\n  ansible.builtin.shell: |\n    getent passwd {{ item.user }}\n  register: getentstask\n  with_items: \"{{ users_dict }}\"\n  changed_when: false\n\n- name: Filter results to just stdout\n  set_fact: \n    getents: \"{{getentstask.results | map(attribute='stdout')}}\"\n\n- name: Make sure there's a line in /etc/passwd\n  ansible.builtin.lineinfile:\n    path: /etc/passwd\n    line: \"{{ item }}\"\n  with_items: \"{{ getents }}\"\nThis feels a bit weird. In theory running getent passwd &lt;user&gt; should just be returning exactly what’s in /etc/passwd for that user to stdout so taking that result from stdout and putting it in /etc/passwd feels a bit circular, but it’s necessary for AD users."
  },
  {
    "objectID": "posts/2023-06-04-rootless-docker.html#turn-on-linger-for-users",
    "href": "posts/2023-06-04-rootless-docker.html#turn-on-linger-for-users",
    "title": "Configuring Rootless docker with ansible",
    "section": "Turn on linger for users",
    "text": "Turn on linger for users\nTurning this on allows user level services like rootless docker to persist when the user is not logged in. If we want users to be able to host small apps with docker from their user account for testing without being logged in all the time this is handy\n- name: Turn on linger for all users\n  become: true\n  ansible.builtin.command:\n  args:\n    cmd: \"loginctl enable-linger {{ item.user }}\"\n    creates: \"/var/lib/systemd/linger/{{ item.user }}\"\n  with_items: \"{{ users_dict }}\"\nDon’t ask me why I used command here and shell in the previous one. I should really just use shell all the time."
  },
  {
    "objectID": "posts/2023-06-04-rootless-docker.html#run-the-installer",
    "href": "posts/2023-06-04-rootless-docker.html#run-the-installer",
    "title": "Configuring Rootless docker with ansible",
    "section": "Run the installer",
    "text": "Run the installer\n- name: Run the rootless docker installer\n  become: true\n  become_user: \"{{ item.user }}\"\n  ansible.builtin.command: \n  args:\n    cmd: dockerd-rootless-setuptool.sh install\n    creates: \"/home/{{ item.user }}/.docker/config.json\"\n  environment:\n    XDG_RUNTIME_DIR: \"/run/user/{{ item.uid }}\"\n  with_items: \"{{ users_dict }}\"\nNote the call to become_user, that’s important. You can’t run the install script as root and tell it to do it for a specific user, at least I couldn’t figure out how, so we need to actually run this as the user we want. Also note that setting XDG_RUNTIME_DIR is necessary for successful completion of the install and requires you to know the UID of the user you’re configuring. Failing to set this variable will result in the script still running but the daemon and user service not actually being installed."
  },
  {
    "objectID": "posts/2023-06-04-rootless-docker.html#set-bashrc-to-export-the-docker-socket",
    "href": "posts/2023-06-04-rootless-docker.html#set-bashrc-to-export-the-docker-socket",
    "title": "Configuring Rootless docker with ansible",
    "section": "Set bashrc to export the docker socket",
    "text": "Set bashrc to export the docker socket\nAt this point users have docker installed and should be able to run docker run hello-world or some other similar test. We do have to take an extra step to get it to work with VS code though, and that’s setting an environment variable that points to the docker socket. This is the part I mentioned above that won’t work if you don’t have your default shell set to bash in /etc/passwd.\n- name: Make sure bashrc exports the docker socket\n  become: true\n  become_user: \"{{ item.user }}\"\n  ansible.builtin.lineinfile:\n    path: \"/home/{{ item.user }}/.bashrc\"\n    line: \"export DOCKER_HOST=unix:///run/user/$(id -u)/docker.sock\"\n    create: true\n  with_items: \"{{ users_dict }}\""
  },
  {
    "objectID": "posts/2020-07-09-pypack.html",
    "href": "posts/2020-07-09-pypack.html",
    "title": "Python packaging",
    "section": "",
    "text": "Probably the best way to introduce this post is to explain a bit of my background, and then describe the problem I’m trying to solve.\n\n\nI have been using python for data analysis work since about 2017, so around 3 years at the time of writing this post. I work on a small team, and so it’s necessary for us to be able to share code for things like implementing business logic, or connecting to internal data sources. I also maintain an open source package called stats_can that can be used to access Statistics Canada datasets in python.\n\n\n\nThe current way my team shares code is by having a repository with a lib folder in it, and adding that folder to the PYTHONPATH environment variable in Windows.\nThe current way I build new versions of stats_can is through a cargo cult sequence of steps that I kind of sort of understand.\n\n\n\nFor the shared team library all of our stuff is basically in one giant package, broken up into subpackages. This leads to all sorts of problems:\n\nIt’s very difficult to write tests for it.\nThere’s no version numbering so it’s impossible to pin code at a particular version.\nWe can’t share it easily with other teams, and we really can’t share just one particular subpackage of it with other teams.\nThe whole thing just feels very wrong to me. I knew it wasn’t the way to go when I set it up, but I was very new to python and just didn’t have the experience/capacity to find a better way and it worked for the time being.\n\nFor stats_can my current system more or less works, it just has two problems:\n\nI only build conda packages. I’d like to allow pip users to access it but…\nLike I said, the build process is a bit of a house of cards that I barely understand, so adding in another build steps scares me.\n\nBoth of the examples described above are for libraries. I’ve built a couple of small apps, but have even less of an idea the correct way to build/deploy them.\n\n\n\nBasically I want to figure out the current best practice way to do the following:\n\nbuild a library package with versions that can be installed with pip and conda\ndeploy those packages to both a privately hosted repository (for work specific stuff) as well as pypi and Anaconda Cloud or conda-forge for public open source stuff\nOriginally I was also going to include building user facing (web or CLI) apps but this got pretty long already so I think I’m going to leave that for another post\nDitto for CI/CD, linting, extensive testing, and all the other things that go into managing a project. Too big to include in this post.\n\nSo a library with conda and pip packages, hosted both publicly and privately means four total ways to manage the library.\n\n\n\nI find that most of the packaging guides I’ve read show either how to build a completely trivial project that demonstrates one narrow feature, or some giant project that’s a lot to take in all at once. My aim is to start from a single file script and gradually build it up to the final product that I laid out in the what I’m trying to accomplish section. I’ll host the repositories for the library/app on GitHub, and use tags in order to mark the progress of the project through various stages."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#background",
    "href": "posts/2020-07-09-pypack.html#background",
    "title": "Python packaging",
    "section": "",
    "text": "I have been using python for data analysis work since about 2017, so around 3 years at the time of writing this post. I work on a small team, and so it’s necessary for us to be able to share code for things like implementing business logic, or connecting to internal data sources. I also maintain an open source package called stats_can that can be used to access Statistics Canada datasets in python."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#my-current-packaging-approach",
    "href": "posts/2020-07-09-pypack.html#my-current-packaging-approach",
    "title": "Python packaging",
    "section": "",
    "text": "The current way my team shares code is by having a repository with a lib folder in it, and adding that folder to the PYTHONPATH environment variable in Windows.\nThe current way I build new versions of stats_can is through a cargo cult sequence of steps that I kind of sort of understand."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#the-problem",
    "href": "posts/2020-07-09-pypack.html#the-problem",
    "title": "Python packaging",
    "section": "",
    "text": "For the shared team library all of our stuff is basically in one giant package, broken up into subpackages. This leads to all sorts of problems:\n\nIt’s very difficult to write tests for it.\nThere’s no version numbering so it’s impossible to pin code at a particular version.\nWe can’t share it easily with other teams, and we really can’t share just one particular subpackage of it with other teams.\nThe whole thing just feels very wrong to me. I knew it wasn’t the way to go when I set it up, but I was very new to python and just didn’t have the experience/capacity to find a better way and it worked for the time being.\n\nFor stats_can my current system more or less works, it just has two problems:\n\nI only build conda packages. I’d like to allow pip users to access it but…\nLike I said, the build process is a bit of a house of cards that I barely understand, so adding in another build steps scares me.\n\nBoth of the examples described above are for libraries. I’ve built a couple of small apps, but have even less of an idea the correct way to build/deploy them."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#what-im-hoping-to-do-here",
    "href": "posts/2020-07-09-pypack.html#what-im-hoping-to-do-here",
    "title": "Python packaging",
    "section": "",
    "text": "Basically I want to figure out the current best practice way to do the following:\n\nbuild a library package with versions that can be installed with pip and conda\ndeploy those packages to both a privately hosted repository (for work specific stuff) as well as pypi and Anaconda Cloud or conda-forge for public open source stuff\nOriginally I was also going to include building user facing (web or CLI) apps but this got pretty long already so I think I’m going to leave that for another post\nDitto for CI/CD, linting, extensive testing, and all the other things that go into managing a project. Too big to include in this post.\n\nSo a library with conda and pip packages, hosted both publicly and privately means four total ways to manage the library."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#how-this-will-progress",
    "href": "posts/2020-07-09-pypack.html#how-this-will-progress",
    "title": "Python packaging",
    "section": "",
    "text": "I find that most of the packaging guides I’ve read show either how to build a completely trivial project that demonstrates one narrow feature, or some giant project that’s a lot to take in all at once. My aim is to start from a single file script and gradually build it up to the final product that I laid out in the what I’m trying to accomplish section. I’ll host the repositories for the library/app on GitHub, and use tags in order to mark the progress of the project through various stages."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#preliminary-setup",
    "href": "posts/2020-07-09-pypack.html#preliminary-setup",
    "title": "Python packaging",
    "section": "Preliminary setup",
    "text": "Preliminary setup\n\nCreate repository\nThe first step in any project is to make a repository. This one has the uncreative name of ianlibdemo. If you want to follow along at home you can clone it and check out the tag for the associated stage in the tutorial. The state of the repository right after being created in this case can be accessed with git checkout eg01\n\n\nSet up environment\nSo I have somewhere to work from, and also to make this process reproducible for others the next thing I have to do is create an isolated python environment to work in. I’m a conda user so I’ll create an environment.yml file:\nname: ianlibdemo_conda_env\ndependencies:\n  - python\nThen I’ll create the environment with conda env create -f environment.yml.\nThere’s absolutely nothing to this environment, which is kind of the point.\n\n\nMake my super sweet library\nEnough talk! Let’s write some code! Well, actually, I’m not going to write any code. The point of this tutorial is to build a package, not write a super awesome library, so I’m just going to copy the demo project used in SciPy 2018 - the sheer joy of packaging. The original code is here. Basically what the module does is take a text file and output a copy with all the words capitalized (except a specified subset).\nIn the root directory of the repository I’ll copy the capital_mod.py file and cap_data.txt. I’ll also create an example_in.txt file that I can use to manually test the capitalize function.\nNow I have the following files in my repository:\n$ ls\n__pycache__/  capital_mod.py   example_in.txt   LICENSE\ncap_data.txt  environment.yml  README.md\nI can test the “package” out from the interactive prompt:\n$ python -i\nPython 3.8.3 (default, May 19 2020, 06:50:17) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import capital_mod\n&gt;&gt;&gt; capital_mod.get_datafile_name()\nWindowsPath('C:/Users/ianep/Documents/ianlibdemo/cap_data.txt')\n&gt;&gt;&gt; capital_mod.capitalize(\"example_in.txt\", \"example_out.txt\")\n&gt;&gt;&gt; quit()\nEverything looks like it ran fine, and if I check in the directory I have file example_out.txt that is indeed a capitalized version of example_in.txt. If you want to get your repository to this point run git checkout eg02.\nSo everything works great and we can go home, right?"
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#run-into-problems",
    "href": "posts/2020-07-09-pypack.html#run-into-problems",
    "title": "Python packaging",
    "section": "Run into problems",
    "text": "Run into problems\nThis is all well and good, but I don’t just want to use this functionality in this folder. The idea is that this is a utility library. Presumably there are all sorts of scripts that I want to add this file capitalization capability to. Maybe I have coworkers I want to share this with, or use it in an app I’m building. As it stands how can I accomplish this?"
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#some-bad-ways-to-solve-the-problem",
    "href": "posts/2020-07-09-pypack.html#some-bad-ways-to-solve-the-problem",
    "title": "Python packaging",
    "section": "Some bad ways to solve the problem",
    "text": "Some bad ways to solve the problem\n\nJust copy the file everywhere\nFine. It only works from the local directory? I’ll just put a copy of it everywhere I want it. This is pretty clearly a bad idea. It will be annoying to copy the file into every location I might want to use it, if I ever have to update the functionality I will then have to track down every instance of that file and make the change repeatedly, and it violates DRY so any experienced developer that sees me do it will make fun of me. Better not do it this way.\n\n\nAdd it to the path\nThis is already going to be a really long guide so I don’t want to add too much about the python path directly. This guide by Chris Yeh is the best I’ve found on the python path and import statements, so if you’re curious by all means check that out. Briefly though, let’s demonstrate the two ways we could directly add this “package” to the path, and therefore run it without being in the same directory.\nTo set the stage I’ve created a new directory separate from the package, and created a text file that I will try and capitalize:\n(ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp\n$ ls\ndemo_in.txt\n(ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp\n$ cat demo_in.txt\ni want to capitalize this text file, but it's in the wrong folder. oh no!\nIf I just try and do the same steps I did from within the folder it will fail:\n&gt;&gt;&gt; import capital_mod\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'capital_mod'\nThat’s because the folder with capital_mod.py is not on my path.\nOne way I can solve this is by adding the path to capital_mod.py to my path. Like so:\n$ export PYTHONPATH=\"/c/Users/Ian/Documents/ianlibdemo\"\n(ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp\n$ python -i\nPython 3.8.2 | packaged by conda-forge | (default, Apr 24 2020, 07:34:03) [MSC v.1916 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import capital_mod\n&gt;&gt;&gt; capital_mod.get_datafile_name()\nWindowsPath('C:/Users/Ian/Documents/ianlibdemo/cap_data.txt')\n&gt;&gt;&gt; capital_mod.capitalize(\"demo_in.txt\", \"demo_out.txt\")\n&gt;&gt;&gt; quit()\n(ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp\n$ cat demo_out.txt\nI Want to Capitalize This Text File, But It's In the Wrong Folder. Oh No!\nThis worked, but I don’t want to have to run that export command every time before I run a script, and sharing this code with other people and telling them to do that every time seems like a hassle. There are ways to permanently add folders to your python path. This guide covers them nicely. But we’re not actually going to go this route so let’s move on.\nThe slightly less hacky way is to use sys.path from within a python script. Back in my demo directory I can write a python script that looks like this:\nimport sys\nsys.path.append(r\"C:\\Users\\Ian\\Documents\\ianlibdemo\")\nimport capital_mod\ncapital_mod.capitalize(\"demo_in.txt\", \"demo_out.txt\")\nWe can see that this works as well:\n(ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp\n$ ls\ndemo_in.txt  syspathdemo.py\n(ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp\n$ python syspathdemo.py\n(ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp\n$ ls\ndemo_in.txt  demo_out.txt  syspathdemo.py\n(ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp\n$ cat demo_out.txt\nI Want to Capitalize This Text File, But It's In the Wrong Folder. Oh No!\nThis also worked, but I had to import sys, and I had to know the exact path to the library. It’s going to be annoying to have to put that in every script, and if I try and share this code with anyone else they’re going to have to modify it to point to wherever they’ve saved my library code."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#get-hypermodern",
    "href": "posts/2020-07-09-pypack.html#get-hypermodern",
    "title": "Python packaging",
    "section": "Get hypermodern",
    "text": "Get hypermodern\nAs I was working on this guide I discovered a series of articles by Claudio Jolowicz called Hypermodern Python. The series is an opinionated (in a good way) look at how to configure a python project in 2020. It’s excellent and well worth a read, but I can’t completely adopt its recommendations for two related reasons. The first is that it assumes you’re either using a *NIX system or can load WSL2 on your Windows machine. For my work setup neither of those assumptions hold. It also assumes you’re working in the standard python ecosystem and therefore doesn’t reference conda either for environment management or packaging. For the remainder of this guide I’m going to try and follow Claudio’s suggestions where possible, but adapt them to incorporate conda."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#turn-our-code-into-a-poetry-package",
    "href": "posts/2020-07-09-pypack.html#turn-our-code-into-a-poetry-package",
    "title": "Python packaging",
    "section": "Turn our code into a poetry package",
    "text": "Turn our code into a poetry package\nPoetry seems to be the current best practice for building python packages. Let’s see if we can get it working with conda.\n\nPoetry init\nAfter adding poetry as a dependency to my conda environment and updating the environment I run poetry init:\n$ poetry init\n\nThis command will guide you through creating your pyproject.toml config.\n\nPackage name [ianlibdemo]:\nVersion [0.1.0]:\nDescription []:  Python packaging - how does it work?\nAuthor [[Ian Preston] &lt;17241371+ianepreston@users.noreply.github.com&gt;, n to skip]:  Ian Preston\nLicense []:  GPL-3.0-or-later\nCompatible Python versions [^3.7]:\n\nWould you like to define your main dependencies interactively? (yes/no) [yes] no\nWould you like to define your dev dependencies (require-dev) interactively (yes/no) [yes] no\nGenerated file\n\n[tool.poetry]\nname = \"ianlibdemo\"\nversion = \"0.1.0\"\ndescription = \"Python packaging - how does it work?\"\nauthors = [\"Ian Preston\"]\nlicense = \"GPL-3.0-or-later\"\n\n[tool.poetry.dependencies]\npython = \"^3.7\"\n\n[tool.poetry.dev-dependencies]\n\n[build-system]\nrequires = [\"poetry&gt;=0.12\"]\nbuild-backend = \"poetry.masonry.api\"\n\n\nDo you confirm generation? (yes/no) [yes] yes\nAt the end of this process I have a pyproject.toml file in the root of my repository with the text listed above inside.\n\n\nsrc layout\nThe root folder of this repository is getting crowded. I’ve got various files that either describe the project or the environment I’m supposed to work on it in intermingled with the actual source code for the package. To address this I’ll make a separate folder for the actual package files, and as recommended by hypermodern python I’ll use src layout\n\n\npoetry install\nThe last step for a basic install is to use poetry to install the package into the environment. Since poetry 1.0 it should be able to detect conda environments and do its installation directly into them based on this PR.\n$ poetry install\nUpdating dependencies\nResolving dependencies... (0.1s)\n\nWriting lock file\n\nNo dependencies to install or update\n\n  - Installing ianlibdemo (0.1.0)\nSeems to work, let’s try that old example that wouldn’t run before:\n(ianlibdemo_conda_env) e975360@N2012 /c/tfs/text_demo\n$ ls\nexample_in.txt\n(ianlibdemo_conda_env) e975360@N2012 /c/tfs/text_demo\n$ python -i\nPython 3.7.7 (default, May  6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; from ianlibdemo.capital_mod import capitalize\n&gt;&gt;&gt; capitalize(\"example_in.txt\", \"example_out.txt\")\n&gt;&gt;&gt; quit()\n(ianlibdemo_conda_env) e975360@N2012 /c/tfs/text_demo\n$ cat example_in.txt\nthese words will all get capitalized, except the ones in that super special text file, like is, or, and a.\n(ianlibdemo_conda_env) e975360@N2012 /c/tfs/text_demo\n$ cat example_out.txt\nThese Words Will All Get Capitalized, Except the Ones In That Super Special Text File, Like Is, Or, And A.\nMagic! Note that I have to do one more layer of importing from the ianlibdemo package whereas before I was directly importing the capital_mod module, but otherwise we’re gold.\nOf course this hasn’t really solved my problem yet, I still don’t have an actual package that other people can install. But still, progress!\n\n\npoetry build\nIt turns out that making it to the previous step was essentially all I needed to create a pip installable package. Just running poetry build from the root of the repository creates a dist folder containing a sdist and a wheel\n\ntest the build\nHaving built this package, how would I install it?\nTo start the test I’ll create a new empty conda environment and make sure I can’t import the ianlibdemo package.\n$ conda create -n pyonly python\n...\n$ conda activate pyonly\n$ python -i\n&gt;&gt;&gt; import ianlibdemo\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'ianlibdemo'\nThis verifies that I have a clean environment without that package installed. I can use pip to install it like so:\n$ pip install /c/tfs/ianlibdemo/dist/ianlibdemo-0.1.0-py3-none-any.whl\n$ python -i\nPython 3.8.3 (default, May 19 2020, 06:50:17) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\"\n&gt;&gt;&gt; import ianlibdemo\nThe import ran successfully. I haven’t done a lot of validation that the package works the way I’d expect, but I’ll get to that when we set up testing later. Note that I installed the package using the .whl file that the build process created, but I could have also used the .tar.gz file in the same folder just as easily.\nSince we’ve now built a working package this seems like another good place for a checkpoint. To see the state of the project at this point you can run git checkout eg03."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#automate-testing",
    "href": "posts/2020-07-09-pypack.html#automate-testing",
    "title": "Python packaging",
    "section": "Automate testing",
    "text": "Automate testing\nThis is already going to be a big post so I’m definitely not going to offer extensive notes on testing, but I’d like to include enough to at least ensure it integrates with the rest of the process, and to save manually testing after each step.\n\nAdd a pytest dependency\nWe want to use pytest for testing, so the first step is to add it as a development dependency. Normally this would be a simple one liner, poetry add --dev pytest, but because of this bug between conda and poetry, at least at the time of this writing I had to install an update of msgpack before I could get it to run. I’ve amended the environment.yml file to include this fix, so between that and hopefully this bug being resolved in time this shouldn’t be an issue for anyone else following this guide, I just wanted to flag what I encountered and how I resolved it.\n\n\nWrite tests\nNow in the base of the repository we add a tests folder and add an empty __init__.py file and a test_capitalize.py file. The test file looks like this:\nfrom ianlibdemo import capital_mod\n\n\ndef test_capitalize_file(tmp_path):\n    in_file = tmp_path / \"in_file.txt\"\n    in_content = \"this is the lowercase input sentence\"\n    in_file.write_text(in_content)\n    out_file = tmp_path / \"out_file.txt\"\n    out_content = \"This is the Lowercase Input Sentence\\n\"\n    # Output shouldn't exist before we call the function\n    assert not out_file.exists()\n    capital_mod.capitalize(in_file, out_file)\n    assert out_file.exists()\n    assert out_file.read_text() == out_content\nNow from the base directory of the repository I can run pytest with poetry run pytest.\nTo see the project at this stage run git checkout eg04."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#publish-to-pypi",
    "href": "posts/2020-07-09-pypack.html#publish-to-pypi",
    "title": "Python packaging",
    "section": "Publish to pypi",
    "text": "Publish to pypi\nI’ve built a package, I can test that it works, the next step is to publish it somewhere for others to access. The defacto source for python packages is PyPi. However, since this is just a demo package I don’t want to publish it there, since it will just add clutter. Fortunately, there is a similar location designed exactly for testing out publishing packages, appropriately named Test PyPi.\n\nSet up for publishing\nIn order to publish packages you need an account. The registration process is straightforward. Note that pypi and test pypi use completely separate databases, and you will need an account for each of them. For now we’re just publishing to test pypi so it’s not an issue, but just something to keep in mind.\nNext I want to create an API token. You can just use your username and password to authenticate and publish packages, but tokens are the preferred method. Once you’re logged in you can click on your account, go to account settings, and under API tokens click “add API token”. Give it a descriptive name and save it somewhere secure (I put mine in a LastPass note). As they warn on the page it will only be displayed once, and if you lose it you’ll have to delete it and create a new one.\nNow we need to set up the test pypi repository in poetry. From the poetry docs you can see that repositories are added to your poetry config:\npoetry config repositories.testpypi https://test.pypi.org/legacy/\npoetry config pypi-token.testpypi &lt;your api key&gt;\nNote that these configurations are global to poetry, so they’re not saved in your repository. If you switch machines, or (I think) change conda environments since we installed poetry with conda you’ll have to redo these configurations.\n\n\nPublish\nOnce this is set up publishing is quite straightforward. If you haven’t already built the package do so with poetry build and then run poetry publish --repository testpypi.\n\n\n\ntest pypi\n\n\nLook at that! There it is!\n\n\nPull it back down and test\nLet’s just make sure that all worked.\nFirst make a clean conda environment with just pytest for testing and activate it:\nconda create -n test_env pytest\nconda activate test_env\nNavigate to the root of your package folder and try running tests. They should fail, because we don’t have the package installed in this environment:\ncd ~/Documents/ianlibdemo\npytest\n.\n.\n.\ntests\\test_capitalize.py:1: in &lt;module&gt;\n    from ianlibdemo import capital_mod\nE   ModuleNotFoundError: No module named 'ianlibdemo'\nNow pip install that package and try running tests again:\npip install --index-url https://test.pypi.org/simple/ ianlibdemo\npytest\n.\n.\n.\n======================== 1 passed, 1 warning in 0.09s =========================\nLooks good!"
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#publish-to-a-private-repository",
    "href": "posts/2020-07-09-pypack.html#publish-to-a-private-repository",
    "title": "Python packaging",
    "section": "Publish to a private repository",
    "text": "Publish to a private repository\nNot all of the code we develop should be published on the public internet. Some of it you just want accessible to an internal team. I have a private package index running using this docker container - setting that up will be its own post. Once you have that all set up though the process is exactly the same as for the public pypi so I’ll leave it at that for this guide.\nNone of the steps used to publish this package required changes to the library repository, so you can still use git checkout eg04 to view the state of the repository at this point."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#adding-dependencies",
    "href": "posts/2020-07-09-pypack.html#adding-dependencies",
    "title": "Python packaging",
    "section": "Adding dependencies",
    "text": "Adding dependencies\nOne thing I realized I should ensure is that all of this works with libraries that depend on other libraries. Let’s add a dependency on pandas and give that a shot.\nFortunately adding a dependency is easy. Since I want to require pandas I just run poetry add pandas and it’s now a dependency. I added a module called fun_pandas and a test for it in my tests suite. After that I rebuilt the package and uploaded it to a repository as described above, pulled it back down and tested it like before and everything worked! It’s nice when that happens.\nTo see the project at this stage you can run git checkout eg05."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#now-do-conda",
    "href": "posts/2020-07-09-pypack.html#now-do-conda",
    "title": "Python packaging",
    "section": "Now do conda",
    "text": "Now do conda\nThe next thing I want to work out is how to build a conda package. The first step is to add conda-build to my environment. The next step is to define a meta.yaml file to specify how to do the build.\n\nSort of working build\nRather than just dump the final working file, I think it will be useful to step through from the first version I got working to the final one I’m happy with. A lot of the steps for setting this up are hacky, so seeing what doesn’t work is as important as seeing what does for people who are trying to figure out how to apply this to their own projects.\nHere’s the first version of my meta.yaml that actually built:\n{% raw %}\n{% set name =  \"ianlibdemo\" %}\n{% set version = \"0.2.0\" %}\n\npackage:\n    name: \"{{ name|lower }}\"\n    version: \"{{ version }}\"\n\nsource:\n    path: ./dist/{{ name }}-{{ version }}-py3-none-any.whl\n\nbuild:\n    script: \"{{ PYTHON }} -m pip install ./{{ name }}-{{ version }}-py3-none-any.whl --no-deps --ignore-installed -vv \"\n\nrequirements:\n    host:\n        - pip\n        - python\n    run:\n        - python\n        - pandas\n\ntest:\n    imports:\n        - ianlibdemo\n{% endraw %}\nFrom an environment with conda-build installed I can build a package by running conda-build . from the base of the repository. It creates a conda package as a tar.bz2 file in a deeply nested directory. From there I can install it into an environment with something like:\nconda install /c/Users/e975360/.conda/envs/conda_build_test/conda-bld/win-64/ianlibdemo-0.2.0-py38_0.tar.bz2\nRunning pytest in an environment with that package installed resulted in one passed test and one failure for the one requiring pandas. As we’ll see below, that issue will get solved if I can load it to a package repository so I’ll leave that alone at this point.\n\nIssues with this build\n\nFirst off, note that it’s called meta.yaml not meta.yml. Despite .yml being the common and preferred extension for this file type (see this SO thread) it has to end with .yaml or conda-build can’t find it.\nAlso note that I’m pointing it to the .whl file that I built with poetry, rather than the .tar.gz that’s in the same folder. In theory I should be able to do either, and most examples online point to .tar.gz files, but I got errors about not having poetry in my build environment, and when I tried to add poetry I got a version conflict because apparently the main conda repository only has the python2.7 version of poetry and… it just seemed easier to use the .whl.\nIt makes a build that claims to be specific to windows and python 3.8 when in fact this should run on any OS and any python 3.\nI have to repeat the file name in two places\nI’m specifying the version number in two places now since it’s already in the pyproject.toml file. There’s a risk of them getting out of sync\nSimilar to the version number I have to specify dependencies in this file as well as pyproject.toml (pandas in this case). Unfortunately, since conda packages can have slightly different names than their pypi counterparts, and I have to actually specify python as a dependency here I don’t think there’s an automated way to keep these in sync. Fortunately I don’t expect dependencies to change as often as the package version so this will be less of a burden to manage.\nTo do anything with the created package I have to scroll up through a big install log and find the path to the file\nI get a bunch of build environments and intermediate files created on my machine (maybe this is why the build guide suggests using docker).\n\n\n\nFixing the issues\nSetting the build to work for any OS and python is an easy fix. Under the build section you just add one line. The build section now looks like this:\n{% raw %}\nbuild:\n    noarch: python\n    script: \"{{ PYTHON }} -m pip install ./{{ name }}-{{ version }}-py3-none-any.whl --no-deps --ignore-installed -vv \"\n{% endraw %}\nDefining the package file in once place is similarly easy. Jinja lets you concatenate variables with the ~ symbol. The updated relevant section looks like this:\n{% raw %}\n{% set name =  \"ianlibdemo\" %}\n{% set version = \"0.2.0\" %}\n{% set wheel = name ~ \"-\" ~ version ~ \"-py3-none-any.whl\" %}\npackage:\n    name: \"{{ name|lower }}\"\n    version: \"{{ version }}\"\n\nsource:\n    path: ./dist/{{ wheel }}\n\nbuild:\n    noarch: python\n    script: \"{{ PYTHON }} -m pip install ./{{ wheel }} --no-deps --ignore-installed -vv \"\n{% endraw %}\n\nAdding a Makefile\nThe rest of the issues outlined above aren’t directly the result of the meta.yaml file. To resolve them I’ll need to write some scripts, and to tie that all together I’ll use my good friend Make.\nTo begin I add some boilerplate to the beginning of the file to handle conda environments\n# Oneshell means I can run multiple lines in a recipe in the same shell, so I don't have to\n# chain commands together with semicolon\n.ONESHELL:\n# Need to specify bash in order for conda activate to work.\nSHELL=/bin/bash\n# Note that the extra activate is needed to ensure that the activate floats env to the front of PATH\nCONDA_ACTIVATE=source $$(conda info --base)/etc/profile.d/conda.sh ; conda activate ; conda activate\nENV_NAME = ianlibdemo_conda_env\nNext I create a python script that will read the version number from pyproject.toml and update the version in meta.yaml with it. I won’t reproduce that script here but it’s in the scripts folder of the ianlibdemo repository.\nFinally I add a target to sync the versions. I can then make that a pre-requisite of building the conda package.\n.PHONY: versionsync\n\nversionsync:\n    $(CONDA_ACTIVATE) $(PROJECT_NAME)\n    python scripts/version_sync.py\n.PHONY: means that target should be run each time it’s called. By default Make won’t redo a target if an output file already exists.\nNow running make versionsync from the root of the repository will take the version from pyproject.toml and put it in meta.yaml. Eventually I’ll also want to ensure that the python package has been built by poetry before building the conda package.\nPS: I documented how you can activate conda environments from within makefiles and bash scripts here. Since I had to refer back to it when doing this I thought it would be helpful to include a pointer.\nThe next issue I described above is that running conda-build generates the package in some obscure subdirectory and you have to scroll back up through the log file to find it. If I want to upload the package to a repository or install it directly that’s going to be a hassle. Fortunately conda-build comes with a --output flag that you can run to return where your package file would be saved if you actually ran conda-build. Knowing this I can write a small bash script which first builds the package and then uses the --output flag to find the generated package and copy it into my dist directory.\nThe new part of the Makefile looks like this:\nconda:\n    $(CONDA_ACTIVATE) $(ENV_NAME)\n    bash scripts/conda_build.sh\nAnd the bash script looks like this:\n#!/bin/bash\nconda-build .\nCONDA_PACK=$(conda-build . --output)\ncp $CONDA_PACK dist/\nI’m going to make a cleanup function later to remove all build artifacts so we’ll leave that alone for now.\n\n\n\n\nPublish to a public channel\nTo publish to an external public conda channel I have to install the anaconda-client package in my environment. The first time I do an upload I will need to log in with anaconda login and provide my username and password.\nAfter that I can add a new recipe to my makefile to publish the package:\nconda_ext_pub: conda\n    $(CONDA_ACTIVATE) $(ENV_NAME)\n    anaconda upload $$(conda-build . --output)\nconda_ext_pub depends on the conda recipe so this will build the package first and then upload it to Anaconda.org. After running make conda_ext_pub I can see that the package was indeed published to Anaconda.org:\n\n\n\nAnaconda\n\n\nAs with the previous installations I can create a new blank environment with just pytest installed, install this package into it with conda install -c ian.e.preston ianlibdemo and now both my tests pass, as pandas is installed as well.\n\n\nPublish to a private channel\nAs with the other private repository, actually setting up the repository is outside the scope of this post. This will assume that you have one created and that packages are stored on some sort of file share that you can access from your build machine. There’s no fancy way to publish conda packages to a private repository. You just drop the package file in the appropriate architecture subfolder (noarch in this case since this is a pure python package) and then run conda index on the repository folder. My server has a file watcher that detects changes and auto runs that, so all we have to do to publish a package is to make sure it’s in the right place. In this example the file share from my local machine is at \\\\r4001\\finpublic\\FP&A\\channel_test\\noarch and the web server is available at http://dml01:8081/.\nTo set up publishing I add the following to my makefile:\nCONDA_LIB_DIR = //r4001/finpublic/FP\\&A/channel_test/noarch\n.\n.\n.\nconda_int_pub: conda\n    $(CONDA_ACTIVATE) $(ENV_NAME)\n    cp $$(conda-build . --output) $(CONDA_LIB_DIR)\nAfter that I can install the package into a library by running conda install -c http://dml01:8081 ianlibdemo.\nTo see the project at this stage you can run git checkout eg06."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#put-it-all-together",
    "href": "posts/2020-07-09-pypack.html#put-it-all-together",
    "title": "Python packaging",
    "section": "Put it all together",
    "text": "Put it all together\nAll of the pieces are here, so the final thing to do is to put them all together. I started that process in the last section by creating a makefile, now I just have to finish it up by tying the pip packaging and publishing in with the conda packaging and publishing.\n\nClean slate\nAfter a package file is built and published we don’t really have any further need for it locally, but it’s not automatically deleted. Let’s make a clean task in Make that will clear out any previous builds. That way any new process can start fresh.\nThe clean task looks like this:\nclean:\n    # remove pip packages\n    rm -rf ./dist/*\n    # remove conda packages and build artifacts\n    $(CONDA_ACTIVATE) $(ENV_NAME)\n    bash scripts/conda_clean.sh\nand conda_clean.sh looks like this:\n#!/bin/bash\nexport CONDA_BLD_PATH=${CONDA_PREFIX}/conda-bld\nrm -rf $CONDA_BLD_PATH\n\n\nFull build chain\nThe last step is to add make tasks to build and publish the pip packages and set them as appropriate dependencies for the conda steps.\nFirst, add a task to build the pip installable package:\npip: clean\n    $(CONDA_ACTIVATE) $(ENV_NAME)\n    poetry build\nNext add tasks to publish to external and internal pip sources:\npip_ext_pub: pip\n    $(CONDA_ACTIVATE) $(ENV_NAME)\n    poetry publish --repository testpypi\n\npip_int_pub: pip\n    $(CONDA_ACTIVATE) $(ENV_NAME)\n    poetry publish --repository localpypi\nFinally as an example we can make wrapper tasks that will publish pip and conda packages to external/internal sources:\nall_int_pub: pip_int_pub conda_int_pub\n    echo \"publishing to internal conda and pip repository\"\n\nall_ext_pub: pip_ext_pub conda_ext_pub\n    echo \"publishing to external conda and pip repository\"\nAt this point if you want to build and publish your package you can just run make all_int_pub and it will clear out old build artifacts, build a new pip installable package, upload it to the internal pip package repository, sync the version number with conda, build a conda package and publish that to the internal conda package repository. Not bad!\nThis is concludes the changes I’m planning to make in this repository. If you just clone the repository as is you should see it in this state, or you can run git checkout eg07."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html",
    "href": "posts/2020-02-15-windows-ds-software.html",
    "title": "Setting up for data science in python on Windows",
    "section": "",
    "text": "There are lots of great guides for setting up an environment to do data science. For my purposes though they generally lack two things:\nThis guide is intended to be useful for anyone trying to get set up for data science in python on windows. I think most of the steps are fairly generic, and I’ll make an effort to highlight the parts that are more opinionated. The sections below will go over the core software that will need to be installed, and some handy customizations.\nedit 2020-02-17 - I realized I wanted make as well, so I added a section on that. Also made note of an issue with launching interactive python from git bash, as well as solutions.\nedit 2020-08-04 - I started using pylance for a python language server, discovered SQL formatter, and switched to the regular Vim plugin.\nedit 2020-10-28 - Add instructions for clearing out an old install, do everything as a user level install. I cleaned up some other instructions as I went through as well.\nedit 2020-11-02 - Remove reference to pyenv-win. It’s too much of a hassle. Unfortunately my best advice there is to get access to a *NIX environment somehow."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#app-installation",
    "href": "posts/2020-02-15-windows-ds-software.html#app-installation",
    "title": "Setting up for data science in python on Windows",
    "section": "App installation",
    "text": "App installation\nHead to the VS code download page and pick the Win64 user installer.\nGo through the install process, I think all the defaults are fine, so just keep hitting next. You can optionally add in things like “Add open with code action to Windows Explorer”. It won’t directly impact the rest of what we’re doing, just integrates code with the rest of your desktop a little more fully.\nYou can sign in with either a GitHub or Microsoft account to enable setting sync. If you use VS code on multiple machines, or just want to easily restore all your settings if you get a new computer I recommend enabling it. See the person icon in the lower left corner.\n\n\n\nsettings sync\n\n\nOn the work machine I’m testing this guide on I’m getting a settings sync error, even though the same install is syncing on my home machine. I love computers. Oh well, it means I’ll get to really walk through a fresh install for this guide."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#extensions",
    "href": "posts/2020-02-15-windows-ds-software.html#extensions",
    "title": "Setting up for data science in python on Windows",
    "section": "Extensions",
    "text": "Extensions\nWhat makes VS code so great is all its extensions. Here are some that are great:\n\nBracket Pair Colorizer 2 colour codes your brackets. Super helpful for debugging.\nDracula Official is a nice dark theme. The Material themes are also nice.\nGitLens integrates a lot of git functionality into VS code, things like showing who made what changes inline in your code.\nindent-rainbow colour codes your indentation level, similar to the brackets\nmarkdownlint gives style suggestions when writing markdown files (like this guide!)\nMaterial icon theme makes the icons in the file explorer a little nicer\nPython is pretty core for this for obvious reasons\nPylance is a new language server for Visual Studio, it offers nicer autocomplete and seems worth using.\nBetter TOML handles the config format of python packages\nThe Remote Development extension pack lets you run VS code on a local machine while developing on a remote system, Docker container, or WSL install.\nSQL Formatter will clean up any sql queries you write\nSQL Server (mssql) is very handy if you interact with SQL server. The first time you open this it will run a bunch of installers in the background.\nVim - Only install this if you know what Vim is and you want to use its keybindings. If you do this extension will make you very happy, if you don’t it will make you very sad.\nEditorconfig - Lets VS Code parse editorconfig files."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#finish-up-later",
    "href": "posts/2020-02-15-windows-ds-software.html#finish-up-later",
    "title": "Setting up for data science in python on Windows",
    "section": "Finish up later",
    "text": "Finish up later\nThere’s more configuration to do in VS code, but prior to that let’s set up the actual applications we’ll use with it."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#customizing-git",
    "href": "posts/2020-02-15-windows-ds-software.html#customizing-git",
    "title": "Setting up for data science in python on Windows",
    "section": "Customizing git",
    "text": "Customizing git\nThere are a couple handy things that are useful to customize about git. For one, VS Code is going to make a .vscode folder anywhere we open a folder with it. We’re never going to want to commit that file to version control, so we’ll create a global gitignore file (as opposed to the more standard repository specific ones) and exclude that file. I got this idea from this blog, so credit there. In your %UserProfile%\\.config\\git folder, create a file named ignore. Mine is quite basic at this point:\n*.vscode\nIf you find your setup generates other files you’d like to ignore put them here, but don’t use this file for language specific stuff like .ipynb-checkpoints, leave that to project specific .gitignore files.\nThe other thing that’s nice to do is clean up your default prompt. Notably, git bash by default includes MINGW64 in the prompt. I guess this is the $MSYSTEM$ environment variable, but I can’t imagine why I’d care to see that in my prompt. The other stuff it includes by default are pretty handy, but if you don’t like them you can modify the same file I’m going to point to to update your setup.\nThe file that contains your prompt information should be in either %UserProfile%\\AppData\\Local\\Programs\\Git\\etc\\profile.d\\git-prompt.sh. This is a standard bash script, so if you’re familiar with bash scripting, or modifying your bashrc in Linux or Mac this will be familiar. If not, it’s generally pretty readable. Make a backup of it and fiddle. To get rid of the MINGW64 we just have to find the lines that say\nPS1=\"$PS1\"'\\[\\033[35m\\]'       # change to purple\nPS1=\"$PS1\"'$MSYSTEM '          # show MSYSTEM\nand delete them or comment them out. I also got rid of\nPS1=\"$PS1\"'\\n'                 # new line\nso that my conda environment would be on the same line as the rest of my setup info."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#set-up-ssh",
    "href": "posts/2020-02-15-windows-ds-software.html#set-up-ssh",
    "title": "Setting up for data science in python on Windows",
    "section": "Set up ssh",
    "text": "Set up ssh\nThis will actually work exactly the same as Linux, which is nice. GitHub has nice docs on how to generate keys and associate them with your GitHub account, so I won’t reiterate that here. Anything you can connect to via SSH rather than password you should though. It’s more secure, and more convenient.\n\nSSH note\nI had an old install of putty when I first set up git bash. Even though I told it to use OpenSSH I guess I still had putty set somewhere in my environment. I had to modify the GIT_SSH environment variable for my system to point to the git ssh utility, which in my case was at C:\\Program Files\\Git\\usr\\bin\\ssh. Most people shouldn’t have to do this."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#additional-utilities",
    "href": "posts/2020-02-15-windows-ds-software.html#additional-utilities",
    "title": "Setting up for data science in python on Windows",
    "section": "Additional utilities",
    "text": "Additional utilities\nGit bash has decent functionality out of the box, but there may be additional utilities you want to install. In my particular case, I’d like to be able to use make in my projects. Thanks to this gist I found that it’s pretty easy to do. I’ll reproduce the make install instructions here, but all credit for this part goes to the original author.\nWherever you installed git bash there should be a mingw64 folder. My home machine did a system install, so I found it in C:\\Program Files\\Git\\mingw64, but my work one was a user level install, so that one ended up in %UserProfile%\\AppData\\Local\\Programs\\Git\\mingw64. You can always find where it is by right clicking the shortcut to git bash in your start menu and hitting properties, that will show you the path.\n\nKeep in mind you can easy add make, but it doesn’t come packaged with all the standard UNIX build toolchain–so you will have to ensure those are installed and on your PATH, or you will encounter endless error messages.\n\n\nGo to ezwinports.\nDownload make-4.1-2-without-guile-w32-bin.zip (get the version without guile).\nExtract zip.\nCopy the contents to your Git\\mingw64\\ merging the folders, but do NOT overwrite/replace any existing files.\n\nThat was all I had to do to make the basic makefiles that I wanted to use. As noted above, if you want to actually build c packages or something your process will likely be more complex. For a great beginner friendly intro to makefiles in the context of python projects, check out calm code."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#further-reading",
    "href": "posts/2020-02-15-windows-ds-software.html#further-reading",
    "title": "Setting up for data science in python on Windows",
    "section": "Further reading",
    "text": "Further reading\nThe main git page has tons of resources. I’ve also collected a few that I found useful under my Tagpacker page."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#actually-building-environments",
    "href": "posts/2020-02-15-windows-ds-software.html#actually-building-environments",
    "title": "Setting up for data science in python on Windows",
    "section": "Actually building environments",
    "text": "Actually building environments\nConda environment management is a big separate topic. Their documentation is really good, and I refer to it regularly."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#user-settings",
    "href": "posts/2020-02-15-windows-ds-software.html#user-settings",
    "title": "Setting up for data science in python on Windows",
    "section": "User settings",
    "text": "User settings\nVS code comes mostly with sensible defaults, but there are a few things I like to change. In VS code hit F1 and type Open settings (JSON). If you don’t see that option (it didn’t pop up for me the first time I tried it) just open settings and look for a setting that tells you to update it in settings.json. Below are my settings, minus the stuff that just got added through configuring the extensions described above:\n{\n    \"diffEditor.renderSideBySide\": true,\n    \"workbench.editor.enablePreviewFromQuickOpen\": false,\n    \"workbench.editor.enablePreview\": false,\n    \"workbench.colorTheme\": \"Dracula\",\n    \"workbench.iconTheme\": \"material-icon-theme\",\n    \"editor.rulers\": [88],\n    \"editor.suggestSelection\": \"first\",\n    \"editor.acceptSuggestionOnEnter\": \"off\",\n    \"editor.minimap.enabled\": false,\n    \"editor.lineNumbers\": \"relative\",\n    \"explorer.confirmDelete\": false,\n    \"editor.wordWrap\": \"on\",\n    \"python.formatting.provider\": \"black\",\n    \"python.linting.enabled\": true,\n    \"python.linting.flake8Enabled\": true,\n    \"python.languageServer\": \"Pylance\",\n    \"git.confirmSync\": false,\n    \"git.autofetch\": true,\n    \"editor.codeActionsOnSave\": null,\n    \"search.exclude\": {\n        \"**/node_modules\": true,\n        \"**/bower_components\": true,\n        \"**/env\": true,\n        \"**/venv\": true\n    },\n    \"files.watcherExclude\": {\n        \"**/.ipynb_checkpoints/**\": true,\n        \"**/$tf/**\": true,\n        \"**/.git/objects/**\": true,\n        \"**/.git/subtree-cache/**\": true,\n        \"**/node_modules/**\": true,\n        \"**/env/**\": true,\n        \"**/venv/**\": true,\n        \"**/.hypothesis/**\": true,\n    },\n    \"files.exclude\": {\n        \"*.sublime-*\": true,\n        \"**/__pycache__\": true,\n        \"**/.DS_Store\": true,\n        \"**/.git\": true,\n        \"**/.hypothesis/**\": true,\n        \"**/.ipynb_checkpoints\": true,\n        \"**/.pytest_cache\": true,\n        \"**/.vscode\": true,\n        \"**/*.log\": true,\n        \"**/*.lst\": true,\n        \"**/$tf\": true,\n        \"**/node_modules\": true,\n        \"venv\": true\n    }\n}\nMost of the settings have fairly clear names, and if you put them in your settings.json you’ll get a little mouseover tip that will tell you what they do.\nWhile you can hand code in the option to have VS code use git bash it’s probably easier to hit F1 again and select “Terminal: Select Default Shell” and choose it there. That will add a line to your settings that tells VS code to use git bash."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#further-resources",
    "href": "posts/2020-02-15-windows-ds-software.html#further-resources",
    "title": "Setting up for data science in python on Windows",
    "section": "Further Resources",
    "text": "Further Resources\nThere’s tons of stuff to learn about VS code to make it super handy. At a minimum, check out their keyboard shortcuts. I’m collecting other useful resources (with a decent amount of overlap with vim stuff) on my tagpacker."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#uninstall-apps",
    "href": "posts/2020-02-15-windows-ds-software.html#uninstall-apps",
    "title": "Setting up for data science in python on Windows",
    "section": "Uninstall Apps",
    "text": "Uninstall Apps\nEven if you installed a program as a user level install, you’ll still be prompted for an Administrator password if you try and remove it from “Add/Remove Programs” in the control panel because… Windows. Running the actual uninstall exe file works fine though.\nFor git this should be located at %UserProfile%\\AppData\\Local\\Programs\\Git\\unins000.exe. The uninstaller won’t remove that folder itself so delete it after you’ve run it.\nVS code is in a similar location and follows a similar process %UserProfile%\\AppData\\Local\\Programs\\Microsoft VS Code\\unins000.exe.\nMiniconda as a user level install is in a slightly different place: %UserProfile%\\Miniconda3\\Uninstall-Miniconda3.exe You’ll be prompted for an admin password a couple times, but if you click “no” it still seems to work, which is cool I guess. It will prompt you to reboot and when it comes back up you should be clear."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#remove-miscellaneous-crud",
    "href": "posts/2020-02-15-windows-ds-software.html#remove-miscellaneous-crud",
    "title": "Setting up for data science in python on Windows",
    "section": "Remove miscellaneous crud",
    "text": "Remove miscellaneous crud\nDepending on how long you’ve been using your machine and how many different installs you’ve done you may or may not have these files. Here’s a list of the things I checked for and then removed from my machine:\n\nOrphan start menu entries in %UserProfile\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\nRenmants in C:\\ProgramData. In my case I had a folder for jupyter"
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#remove-config-files",
    "href": "posts/2020-02-15-windows-ds-software.html#remove-config-files",
    "title": "Setting up for data science in python on Windows",
    "section": "Remove config files",
    "text": "Remove config files\nIf you like how your configuration is set up there’s no major harm in keeping these things. I’m just removing them to make sure I have a fair comparison to how a fresh install would go. All paths in the list of stuff I’ve removed are relative to %UserProfile%.\n\n.conda\n.config/configstore\n.continuum\n.cookiecutter_replay\n.cookiecutters\n.ipynb_checkpoints\n.ipython\n.jupyter\n.matplotlib\n.poetry\n.software\n.vscode\nAppData\nAppData\nAppData\nAppData\nAppData-data\nAppData\nAppData\nAppData\nAppDataTools\nAppData\nAppData\nAppData\nAppData\nAppData\nAppData\nAppData\n.bash_history\n.bash_profile\n.flake8\n.gitconfig\n.git-credentials\n.gitignore_global\n.python_history\n\nAs you can see, managing a python development environment creates a lot of crud on your system."
  },
  {
    "objectID": "posts/2020-11-26-arch-tldr.html",
    "href": "posts/2020-11-26-arch-tldr.html",
    "title": "Automating provisioning Arch continued - TLDR",
    "section": "",
    "text": "This is part 4 of a 4 part series describing how I provision my systems. Links to each part are below:"
  },
  {
    "objectID": "posts/2020-11-26-arch-tldr.html#setup-wifi",
    "href": "posts/2020-11-26-arch-tldr.html#setup-wifi",
    "title": "Automating provisioning Arch continued - TLDR",
    "section": "Setup WiFi",
    "text": "Setup WiFi\nIn the case where I’m doing this on a laptop I’ll likely have to get on WiFi before I can continue.\niwctl\nstation wlan0 connect &lt;your SSID&gt;  # You can enclose it in quotes if it has spaces\n&lt;enter passphrase&gt;\nexit\ndhcpcd wlan0"
  },
  {
    "objectID": "posts/2020-11-26-arch-tldr.html#make-sure-partitions-are-set-up",
    "href": "posts/2020-11-26-arch-tldr.html#make-sure-partitions-are-set-up",
    "title": "Automating provisioning Arch continued - TLDR",
    "section": "Make sure partitions are set up",
    "text": "Make sure partitions are set up\nIf you’re not just going to wipe the whole disk you can run lsblk to determine what partitions you have. cfdisk has a nice interface for creating and modifying partitions if necessary. To format the boot partition run:\nmkfs.vfat -F32 /dev/&lt;partition&gt;\nmkfs.ext4 /dev/&lt;partition&gt; will work for the root partition."
  },
  {
    "objectID": "posts/2020-11-26-arch-tldr.html#run-the-script",
    "href": "posts/2020-11-26-arch-tldr.html#run-the-script",
    "title": "Automating provisioning Arch continued - TLDR",
    "section": "Run the script",
    "text": "Run the script\nbash &lt;(curl -fsSL http://bootstrap.ianpreston.ca)\nAfter that power off, remove the USB and power back on."
  },
  {
    "objectID": "posts/2021-02-26-ferede.html",
    "href": "posts/2021-02-26-ferede.html",
    "title": "Alberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue",
    "section": "",
    "text": "This is my replication of the empirical results, tables, and figures produced in a paper by Dr. Ergete Ferede, published by the University of Calgary school of public policy in Volume 11:24, September 2018.\nThe original paper is here: https://www.policyschool.ca/wp-content/uploads/2018/09/NRR-Ferede.pdf\nI chose this paper to reproduce for two reasons. The first is pragmatic; the data it uses is all publicly available, so I actually can. The second is that it describes a topic of importance in the province of Alberta, where I live.\nYou can read the details of what the paper sets out to show in the paper itself, but in brief the idea is to show that provincial government spending increases in the year following an increase in non-renewable resource revenue, but it does not decrease accordingly in the year following declines in the same revenue source. This has a ratcheting effect on public finance that is a contributor to the “royalty rollercoaster” that is Alberta’s public finance.\nIn the following sections I’ll go through the code necessary to extract and transform the data set used in the paper, as well as reproduce its key empirical results. Since most economists don’t use python, and they make up a key part of my intended audience for this, I’ll be adding comments to my code that explicitly describe what some of the functions and methods I’m calling do.\nI’m including all of the code necessary to produce this reproduction, since that’s a big part of why I’m doing this exercise, but if you’re just interested in seeing how my reproduced results compare to the original paper you can skip all the code blocks. You can find the code for this notebook on my github\nA surprising result of this reproduction is that I’ve identified a single data point error in the original paper that negates its results. Read on to find out what the error was and the impact it had on the results."
  },
  {
    "objectID": "posts/2021-02-26-ferede.html#introduction",
    "href": "posts/2021-02-26-ferede.html#introduction",
    "title": "Alberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue",
    "section": "",
    "text": "This is my replication of the empirical results, tables, and figures produced in a paper by Dr. Ergete Ferede, published by the University of Calgary school of public policy in Volume 11:24, September 2018.\nThe original paper is here: https://www.policyschool.ca/wp-content/uploads/2018/09/NRR-Ferede.pdf\nI chose this paper to reproduce for two reasons. The first is pragmatic; the data it uses is all publicly available, so I actually can. The second is that it describes a topic of importance in the province of Alberta, where I live.\nYou can read the details of what the paper sets out to show in the paper itself, but in brief the idea is to show that provincial government spending increases in the year following an increase in non-renewable resource revenue, but it does not decrease accordingly in the year following declines in the same revenue source. This has a ratcheting effect on public finance that is a contributor to the “royalty rollercoaster” that is Alberta’s public finance.\nIn the following sections I’ll go through the code necessary to extract and transform the data set used in the paper, as well as reproduce its key empirical results. Since most economists don’t use python, and they make up a key part of my intended audience for this, I’ll be adding comments to my code that explicitly describe what some of the functions and methods I’m calling do.\nI’m including all of the code necessary to produce this reproduction, since that’s a big part of why I’m doing this exercise, but if you’re just interested in seeing how my reproduced results compare to the original paper you can skip all the code blocks. You can find the code for this notebook on my github\nA surprising result of this reproduction is that I’ve identified a single data point error in the original paper that negates its results. Read on to find out what the error was and the impact it had on the results."
  },
  {
    "objectID": "posts/2021-02-26-ferede.html#setup-and-data-acquisition",
    "href": "posts/2021-02-26-ferede.html#setup-and-data-acquisition",
    "title": "Alberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue",
    "section": "Setup and data acquisition",
    "text": "Setup and data acquisition\nThis section of the code loads required modules, downloads the required data sets, and reads them into DataFrames.\n\nimport datetime as dt\nfrom itertools import chain\nfrom pathlib import Path\n\nimport altair as alt\nfrom arch.unitroot import DFGLS, ADF, PhillipsPerron\nfrom IPython.display import Image\nimport pandas as pd\nimport pandas_datareader as pdr\nimport requests\nimport seaborn as sns\nimport stats_can\nimport statsmodels\nfrom statsmodels.tsa.api import VAR\n\n\n%matplotlib inline\nalt.renderers.enable(\"jupyterlab\");\n\nWe start by loading the required libraries that will be used to support the analysis. For reference here are links to the libraries that are being used:\n\nPathlib\ndatetime\nrequests\npandas\npandas_datareader\nnumpy\nstats_can\naltair\nseaborn\narch\nstatsmodels\nmatplotlib\n\n\nHistorical budget data\nFunctions in this section are concerned with acquiring historical Alberta budget data and reading it into a DataFrame\n\n\nCode\ndef download_budget_data() -&gt; Path:\n    \"\"\"Download the excel file for the analysis from the policy school page.\n\n    Note the readme sheet on the first file. Credit to Kneebone and Wilkins for\n    assembling it, and policy school for hosting it.\n\n    Originally used this URL, but found it was missing some later heritage\n    contributions. After discussion with Dr. Kneebone an updated set has been provided\n    https://www.policyschool.ca/wp-content/uploads/2019/01/Provincial-Government-Budget-Data-January-2019FINAL-USE.xlsx\n\n    Returns\n    -------\n    pathlib.Path\n        A path object with the location and name of the data\n    \"\"\"\n    print('Downloading data set')\n\n    url = 'https://www.policyschool.ca/wp-content/uploads/2019/03/Provincial-Government-Budget-Data-March-2019.xlsx'\n    # send a request to the url for the file\n    response = requests.get(\n        url,\n        stream=True,\n        headers={'user-agent': None}\n    )\n    # create a path object for the file in the data folder above\n    # where this notebook is saved with the file named\n    # budget.xlsx for easy later access.\n    fname = Path('.').joinpath('data').joinpath('budgets.xlsx')\n    # write the response from the request to the file in the path specified above\n    with open (fname, 'wb') as outfile:\n        for chunk in response.iter_content(chunk_size=512):\n            if chunk: # filter out keep-alive new chunks\n                outfile.write(chunk)\n    # Return the location of the file so we can load it later easily\n    return fname\n\n\ndef get_budget_file(force_update: bool=False) -&gt; Path:\n    \"\"\"Get the budget file, downloading if required.\n\n    Parameters\n    ----------\n    force_update: bool\n        Download the data file even if you already have it\n\n    Returns\n    -------\n    pathlib.Path\n        A path object with the location and name of the data\n    \"\"\"\n    # This is where we're expecting the file to be saved if it exists\n    fname = Path('.').joinpath('data').joinpath('budgets.xlsx')\n    if not fname.exists() or force_update:\n        download_budget_data()\n    return fname\n\n\ndef get_date_index(df: pd.DataFrame) -&gt; pd.DatetimeIndex:\n    \"\"\"Helper function to turn budget year strings into datetimes.\n\n    The Fiscal year columns span across years, e.g. 1965-66. In order\n    to use all the date indexed functionality I want to convert them into\n    an actual datetime format. This function accomplishes that\n    \n    Parameters\n    ----------\n    df: pd.DataFrame\n        The budget dataframe with the fiscal year style columns\n    \n    Returns\n    -------\n    pd.DatetimeIndex\n        A datetime index showing January 1 of the beginning of each\n        fiscal year for each period.    \n    \"\"\"\n    date_index = pd.to_datetime(\n        df\n        .assign(year=lambda df: df['budget_yr'].str[0:4].astype(int))\n        .assign(month=1)\n        .assign(day=1)\n        [['year', 'month', 'day']]\n    )\n    return date_index\n\n\ndef read_ab_budget() -&gt; pd.DataFrame:\n    \"\"\"Read Alberta budget data.\n\n    Downloads the data if necessary, reads it in and gives\n    the variables easier to work with names\n\n    Returns\n    -------\n    pd.DataFrame\n        Alberta's revenue and expenditure tables\n    \"\"\"\n    # Get the budget file, download if necessary  using functions\n    # defined above\n    fname = get_budget_file()\n    df = (\n        pd.read_excel(\n            fname,\n            sheet_name='Alberta',\n            # column titles are spaced over 3 rows\n            header=3,\n            # first column of data is B\n            index_col=1,\n            # there's a big footnote at the bottom we want to skip\n            skipfooter=21\n        )\n        # Because of the merged cells we get an empty first row\n        .loc[lambda x: x.index.notnull()]\n        # Not sure where the empty first column comes from but drop it\n        .drop(columns='Unnamed: 0')\n        .reset_index()\n        .rename(columns={\n            'index': 'budget_yr',\n            'Personal Income Tax': 'personal_income_tax',\n            'Corporation Income Tax': 'corporate_income_tax',\n            'Retail Sales Tax': 'retail_sales_tax',\n            'Federal Cash Transfers': 'federal_cash_transfers',\n            'Natural Resource Revenue': 'natural_resource_revenue',\n            'Other Own-Source Revenue': 'other_own_source_revenue',\n            'Total Revenue': 'total_revenue',\n            'Health': 'health_exp',\n            'Social Services': 'social_services_exp',\n            'Education': 'education_exp',\n            'Other Program Expenditures': 'other_program_exp',\n            'Total Program Expenditures': 'total_prog_exp',\n            'Debt Service': 'debt_service',\n            'Total  Expenditures': 'total_exp',\n            'Unnamed: 16': 'annual_deficit'\n        })\n        # Turn the fiscal year string into a datetime object\n        .assign(budget_dt=lambda df: get_date_index(df))\n        .set_index('budget_dt')\n    )\n    return df\n\n\ndef read_heritage() -&gt; pd.DataFrame:\n    \"\"\"Read deposits to the heritage trust fund from a separate table.\n\n    The paper nets out contributions to the heritage trust fund when they are\n    made, so we have to read them in to be able to net them out of resource revenue.\n\n    They're stored in the same sheet of the workbook, just down below the big table we\n    read in with the function above.\n    \"\"\"\n    fname = get_budget_file()\n    df = (\n        pd.read_excel(\n            fname,\n            sheet_name='Alberta',\n            # Have to manually specify column names because of\n            # how the table is laid out\n            header=None,\n            usecols='D:G',\n            names=['budget_yr', 'resource_allocation', 'deposits', 'advance_edu'],\n            skiprows=71,\n            skipfooter=1\n        )\n        # more fiddly cleaning because of how the table is set up\n        # there's a blank row between 1986-87 and when\n        # contributions resume in 2005-06\n        .loc[lambda df: ~df['budget_yr'].isna()]\n        .set_index('budget_yr')\n        # missing entries have 0 contributions for that\n        # category in that year\n        .fillna(0)\n        # The three columns are all counted the same\n        # for the purposes of this analysis, they just have\n        # different labels/classifications depending on the year\n        .assign(total_heritage=lambda df: df.sum(axis='columns'))\n        # Add a dummy variable to indicate heritage fund deposit years\n        .assign(heritage_dummy=1)\n        .reset_index()\n        # convert the fiscal year column to a datetime index\n        .assign(budget_dt=lambda df: get_date_index(df))\n        .drop(columns='budget_yr')\n        .set_index('budget_dt')\n    )\n    return df\n\n\ndef clean_budget() -&gt; pd.DataFrame:\n    \"\"\"Combine base budget with heritage deposits.\n\n    Pull all the logic together to create one dataframe with all the\n    fiscal data for the period of interest.\n\n    Returns\n    -------\n    pd.DataFrame\n        The full nominal budget data set.\n    \"\"\"\n    budg = read_ab_budget()\n    heritage = read_heritage()\n    budg_clean = (\n        # Start with the budget dataframe\n        budg\n        # consolidate some revenue categories\n        .assign(other_revenue=lambda df: df[['retail_sales_tax', 'federal_cash_transfers', 'other_own_source_revenue']].sum(axis='columns'))\n        # Just keep the columns we still need\n        .reindex(columns=['personal_income_tax', 'corporate_income_tax', 'natural_resource_revenue', 'other_revenue', 'total_prog_exp', 'debt_service'])\n        # add in the heritage contributions data\n        .merge(heritage[['total_heritage', 'heritage_dummy']], how='left', left_index=True, right_index=True)\n        # Set contributions and the heritage dummy to 0 for years where there were no contributions\n        .fillna(0)\n        # Net out heritage contributions from natural resources revenue\n        .assign(natural_resource_revenue_before_heritage=lambda df: df['natural_resource_revenue'])\n        .assign(natural_resource_revenue=lambda df: df['natural_resource_revenue'] - df['total_heritage'])\n        # consolidate revenue\n        .assign(total_revenue=lambda df: df[['personal_income_tax', 'corporate_income_tax', 'natural_resource_revenue', 'other_revenue']].sum(axis='columns'))\n        # consolidate expenditure\n        .assign(total_expenditure=lambda df: df[['total_prog_exp', 'debt_service']].sum(axis='columns'))\n        # calculate the deficit\n        .assign(deficit=lambda df: df['total_expenditure'] - df['total_revenue'])\n        # make all the budget numbers floating point\n        .astype('float64')\n    )\n    return budg_clean\n\n\n\n\nReal Per Capita budget\nAll of the analysis in the paper is done in terms of real per-capita data. Functions in this section transform the nominal total budget numbers acquired in the previous section into real per-capita figures.\n\n\nCode\ndef periodic_to_budget_annual(df: pd.DataFrame, index_name: str, year_periods: int = 4) -&gt; pd.DataFrame:\n    \"\"\"Take a monthly or quarterly indexed dataframe and annualize it by budget period.\n\n    The inflation and population data we need to convert the budget into\n    real per-capita figures are monthly series. We need to get the average\n    population and price level for each fiscal year in the data set.\n\n    Rolling mean indexed on January year N+1 is the March to March\n    average population for fiscal year N\n    Applying a date offset of -1 year and taking only\n    January data of these rolling means gives us an average on the\n    same basis as the budget dates.\n\n    Parameters\n    ----------\n    df: pandas.DataFrame\n        DataFrame to be piped into this function\n    index_name: str\n        The name of the date index\n    year_periods: int, default 4\n        4 for quarterly data (population), 12 for monthly (inflation)\n\n    Returns\n    -------\n    pd.DataFrame\n        An annualized dataframe on a fiscal year basis for comparison\n        to annual budget figures.\n    \"\"\"\n    df = (\n        df\n        .copy()\n        .rolling(year_periods, closed='left')\n        .mean()\n        .reset_index()\n        .assign(budget_dt=lambda df: df[index_name] - pd.DateOffset(years=1))\n        .loc[lambda x: x['budget_dt'].dt.year &gt;= 1965]\n        .loc[lambda x: x['budget_dt'].dt.month == 1]\n        .drop(columns=index_name)\n        .set_index('budget_dt')\n        .copy()\n    )\n    return df\n\n\ndef per_capita_data() -&gt; pd.DataFrame:\n    \"\"\"Read in population data to calculate per capita estimates.\n\n    Quarterly population estimates for Alberta from Statistics Canada\n\n    Returns\n    -------\n    pd.DataFrame\n        Fiscal year annualized population estimates for Alberta over the\n        reference period.\n    \"\"\"\n    table = '17-10-0009-01'\n    df = (\n        stats_can.table_to_df(table, path='data')\n        .loc[lambda x: x['GEO'] == 'Alberta']\n        .loc[lambda x: x['REF_DATE'] &gt;= '1965']\n        .set_index('REF_DATE')\n        [['VALUE']]\n        .rename(columns={'VALUE' : 'population'})\n        .pipe(periodic_to_budget_annual, 'REF_DATE', 4)\n    )\n    return df\n\n\ndef inflation_data() -&gt; pd.DataFrame:\n    \"\"\"Read in inflation data to calculate real dollar estimates.\n\n    The whole series is scaled so 2017 budget year is = 1\n\n    Returns\n    -------\n    pd.DataFrame\n        Fiscal year annualized inflation data for Alberta over\n        the reference period. Normalized to 2017 = 1\n    \"\"\"\n    # Alberta inflation doesn't go back far enough, use Canada for earlier dates\n    vecs = ('v41692327', 'v41690973')\n    df = (\n        stats_can.vectors_to_df_local(vecs, path='data', start_date=dt.date(1965, 1, 1))\n        .rename(columns={'v41692327': 'ab_inflation', 'v41690973': 'ca_inflation'})\n    )\n    # fill in with Canadian inflation data where (early) Alberta inflation data is missing.\n    mask = df['ab_inflation'].isna()\n    # Could probably do some interpolation or scaling before this, but I looked\n    # at the raw series and they were pretty comparable\n    df.loc[mask, 'ab_inflation'] = df.loc[mask, 'ca_inflation']\n    df = (\n        df\n        .drop(columns='ca_inflation')\n        .pipe(periodic_to_budget_annual, 'REF_DATE', 12)\n    )\n    # Rescale to 2017 = 100 (this is fiscal year 2017,\n    # original may have done calendar year)\n    inf_2017 = float(df.loc['2017', 'ab_inflation'])\n    df = df / inf_2017\n    return df\n\n\ndef budget_real_per_capita() -&gt; pd.DataFrame:\n    \"\"\"Get budget data in real per-capita terms.\n\n    Returns\n    -------\n    pd.DataFrame\n        Budget data in real per-capita terms.\n    \"\"\"\n    # Read in budget data using the function defined in the \n    # previous section\n    clean_budget_df = clean_budget()\n    # Everything except the dummy variable gets turned into\n    # real per-capita terms\n    scale_cols = clean_budget_df.columns.drop('heritage_dummy').tolist()\n    # Get population\n    per_capita = per_capita_data()\n    # Get inflation\n    inflation = inflation_data()\n    # Combine the datasets, can just use assign because they all\n    # have a datetime index\n    dfpc = (\n        clean_budget_df\n        .assign(pop=per_capita)\n        .assign(cpi=inflation)\n    )\n    # rescale to real per capita\n    dfpc[scale_cols] = (\n        dfpc[scale_cols]\n        # original data was in millions of dollars\n        .mul(1_000_000)\n        # divide by population and inflation for\n        # real per-capita\n        .div(dfpc['pop'], axis='index')\n        .div(dfpc['cpi'], axis='index')\n    )\n    return dfpc\n\n\n\n\nExogenous factors\nThe paper lists the Alberta employment rate, the Alberta unemployment rate, and the CAD/USD exchange rate as exogenous factors included in the model. Functions in this section acquire that data. I had to do some fiddling to get long enough historical series for some of the factors as you’ll note in the code. It’s hard to say for sure how the original author sourced this data. I’ll just have to compare my tables and charts to his to see if I got close enough.\n\n\nCode\ndef download_historical_cad_usd() -&gt; pd.DataFrame:\n    \"\"\"Get exchange rates from before 1971.\n\n    FRED live data only goes back to 1971, I need a longer series\n    This was what I could find. It's annual only, so I can't do it on a budget\n    year basis, but hopefully it will be close enough\n\n    This whole function is just some gross munging to read in a table from a web page.\n    Once it's called we save it to the data folder so I don't have to re-call it every\n    time I run this notebook.\n    \"\"\"\n    url = 'https://fxtop.com/en/historical-exchange-rates.php?YA=1&C1=USD&C2=CAD&A=1&YYYY1=1953&MM1=01&DD1=01&YYYY2=2019&MM2=04&DD2=01&LANG=en'\n    df = pd.read_html(url)[29]\n    headers = df.iloc[0]\n    new_df  = (\n        pd.DataFrame(df.values[1:], columns=headers)\n        .rename(columns={'Year': 'year', 'Average USD/CAD': 'EXCAUS'})\n        .assign(month=1)\n        .assign(day=1)\n        .assign(budget_dt=lambda df: pd.to_datetime(df[['year', 'month', 'day']]))\n        .set_index('budget_dt')\n        .reindex(columns=['EXCAUS'])\n    )\n    new_df.to_csv('./data/early_cad_usd.csv')\n    return new_df\n\n\ndef read_historical_cad_usd(force_update: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Get exchange rates before 1971.\n\n    This wraps the above function to read in the downloaded data\n    if it's available and download and then read it if required.\n\n    Parameters\n    ----------\n    force_update: bool\n        Download the data set even if you already have it\n\n    Returns\n    -------\n    pd.DataFrame\n        Exchange rates from 1965 to 1971\n    \"\"\"\n    fname = Path('.').joinpath('data').joinpath('early_cad_usd.csv')\n    if not fname.exists() or force_update:\n        return download_historical_cad_usd()\n    else:\n        return pd.read_csv(fname).set_index('budget_dt')\n\n\ndef download_cad_usd() -&gt; pd.DataFrame:\n    \"\"\"Download monthly exchange data from FRED.\n\n    For most of the period of interest I can get monthly\n    data from FRED, so I'll do that where possible.\n\n    Returns\n    -------\n    pd.DataFrame\n        Most of the CAD/USD exchange data I need for this analysis.\n    \"\"\"\n    df = pdr.get_data_fred('EXCAUS', start=dt.date(1970, 1, 1))\n    df.to_csv('./data/cad_usd.csv')\n    return df\n\n\ndef read_cad_usd(force_update=False):\n    \"\"\"Get monthly exchange data from FRED.\n\n    This wraps the above function to read in the downloaded data\n    if it's available and download and then read it if required.\n\n    Parameters\n    ----------\n    force_update: bool\n        Download the data set even if you already have it\n\n    Returns\n    -------\n    pd.DataFrame\n        Exchange rate data\n    \"\"\"\n    fname = Path('.').joinpath('data').joinpath('cad_usd.csv')\n    if not fname.exists() or force_update:\n        return download_cad_usd()\n    else:\n        return pd.read_csv(fname, parse_dates=['DATE']).set_index('DATE')\n\n\ndef annual_cad_usd() -&gt; pd.DataFrame:\n    \"\"\"Full series of CAD/USD in fiscal year format.\n\n    Get FRED data and turn the monthly values into annualized on a budget\n    basis for as much as possible. Fill in the remainder with calendar annual\n    data from fxtop\n\n    Returns\n    -------\n    pd.DataFrame\n        Exchange data on an annualized basis.\n    \"\"\"\n    # Create a datetime index of all the points we need\n    annual_date_range = pd.date_range('1964-01-01', '2018-01-01', freq='AS', name='budget_dt')\n    # Get the old annual stuff to fill in later\n    old_df = read_historical_cad_usd()\n    df = (\n        # get the monthly series\n        read_cad_usd()\n        # annualize it\n        .pipe(periodic_to_budget_annual, 'DATE', 12)\n        # add in all the missing dates we need\n        .reindex(annual_date_range)\n        # fill those missing dates from the old annual data set.\n        .fillna(old_df)\n    )\n    return df\n\n\ndef stats_can_exog() -&gt; pd.DataFrame:\n    \"\"\"Bring in exogenous StatsCan data. Employment and Unemployment rates.\n\n    Returns\n    -------\n    pd.DataFrame\n        Exogenous data required from StatsCan\n    \"\"\"\n    # Vectors for monthly series where available\n    ur_vec = \"v2064516\"\n    er_vec = \"v2064518\"\n    annual_date_range = pd.date_range('1964-01-01', '2018-01-01', freq='AS', name='budget_dt')\n    # for the earlier periods we only have annual data\n    old_df = (\n        stats_can.table_to_df('36-10-0345-01', path='data')\n        # Get Alberta data only\n        .loc[lambda x: x['GEO'] == 'Alberta']\n        # Keep only the categories we care about\n        .loc[lambda x: x['Economic indicators'].isin(['Population', 'Total employment', 'Unemployment rate'])]\n        # pivot so the year is the row and the variables are the columns\n        .pivot_table(index='REF_DATE', columns='Economic indicators', values='VALUE')\n        .rename(columns={'Unemployment rate': 'unemployment_rate'})\n        # calculate the employment rate\n        .assign(employment_rate=lambda x: (x['Total employment'] / x['Population']) * 100)\n        # drop the population, just used for calculating employment rate\n        .reindex(columns=['unemployment_rate', 'employment_rate'])\n        .rename_axis('budget_dt', axis='index')\n        .rename_axis(None, axis='columns')\n    )\n    # Get monthly data where available\n    df = (\n        stats_can.vectors_to_df_local([ur_vec, er_vec], path='data', start_date=dt.date(1964, 1, 1))\n        .rename(columns={ur_vec: 'unemployment_rate', er_vec: 'employment_rate'})\n        # annualize\n        .pipe(periodic_to_budget_annual, 'REF_DATE', 12)\n        # get the full range of data we want\n        .reindex(annual_date_range)\n        # fill in the gaps with the old annual series\n        .fillna(old_df)\n        # Not ideal but even the annual series doesn't go quite back\n        # far enough so we have to backfill the earliest available\n        # data point\n        .fillna(method='bfill')\n    )\n    return df\n\n\ndef exogenous_variables() -&gt; pd.DataFrame:\n    \"\"\"Bring in exogenous parameters together.\n\n    From the paper:\n    We also include other exogenous variables that are likely to affect\n    the province’s budget. It is known that the various components of the\n    provincial budget can be influenced by the business cycle. Thus, following\n    Buettner and Wildsain (2006), we account for the potential effects of the\n    business cycle by including one-period lagged changes in the provincial\n    employment and unemployment rates. Another important exogenous factor\n    that is often cited in provincial budget documents as being important in\n    influencing the provincial government’s oil royalty revenue is the Canadian-U.S.\n    dollar exchange rate. For this reason, we control for this factor by\n    including one period lagged changes in the Canadian-U.S. dollar exchange rate\n\n    Returns\n    -------\n    pd.DataFrame\n        All the necessary exogenous factors for reproducing the paper.\n    \"\"\"\n    cadusd = annual_cad_usd()\n    ur_er = stats_can_exog()\n    df = pd.concat([cadusd, ur_er], axis='columns')\n    return df"
  },
  {
    "objectID": "posts/2021-02-26-ferede.html#exploratory-figures",
    "href": "posts/2021-02-26-ferede.html#exploratory-figures",
    "title": "Alberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue",
    "section": "Exploratory Figures",
    "text": "Exploratory Figures\n\nFigure 1\nPage 5 of the report charts Non-renewable Resource Revenue, Total Expenditure, and Total Revenue. All are in per-capita 2017 dollars. Reproducing this chart will be a good starting check that my data extraction and transformation matches the original author’s strategy.\n\n\nCode\ndef fig1(df: pd.DataFrame) -&gt; alt.Chart:\n    \"\"\"Reproduce Figure 1 from the paper.\n\n    Parameters\n    ----------\n    df: pd.DataFrame\n        A dataframe with non-renewable resource revenue, total expenditure, and total revenue time series\n    \"\"\"\n    chart_df = (\n        df\n        .loc['1970':'2016', ['natural_resource_revenue', 'total_revenue', 'total_expenditure']]\n        .rename(columns={\n            'natural_resource_revenue': 'Non-renewable Resource Revenue',\n            'total_revenue': 'Total Revenue',\n            'total_expenditure': 'Total Expenditure'\n        })\n        .reset_index()\n        .melt(id_vars='budget_dt')\n    )\n    c_domain = [\"Non-renewable Resource Revenue\", \"Total Expenditure\", \"Total Revenue\"]\n    c_range = [\"green\", \"red\", \"blue\"]\n    chart = (\n        alt.Chart(chart_df)\n        .mark_line()\n        .encode(\n            x=alt.X('budget_dt:T', axis=alt.Axis(title=None)),\n            y=alt.Y('value:Q', axis=alt.Axis(title='Per capita in 2017 dollars'), scale=alt.Scale(domain=(0, 14_000))),\n            color=alt.Color('variable:N', legend=alt.Legend(title=None, orient='bottom'), scale=alt.Scale(domain=c_domain, range=c_range))\n        )\n        .properties(width=1250, height=500)\n    )\n    return chart\n\n\nHere’s the original chart from the paper:\n\nImage(filename=\"img/ferede_fig_1.png\")\n\n\n\n\nAnd here’s mine:\n\ndf = budget_real_per_capita()\nfig1(df)\n\n\n\n\nThis graph looks very similar to the chart in the paper, with a notable exception of the 1976/1977 budget year. My chart shows Non-renewable Resource Revenue as slightly negative, whereas the original chart has it largely in line with 1975/1976 and 1977/1978. NRR is negative in my chart because I have netted out contributions to the Alberta Heritage Savings Trust Fund (AHSTF). To the best of my understanding, the original paper does the same, and the consistent values between the two in all other years supports that. Quoting the original paper:\n\nThe part of resource revenue that is saved in the AHSTF is not expected to influence the provincial government’s spending and revenue-raising choices. For this reason, in our analysis, we exclude the part of the resource revenue that is saved in the AHSTF from the non-renewable-resource revenue data.\n\nFor comparison, here is the same chart, but without netting AHSTF contributions from revenue:\n\nno_net_df = (\n    df\n    .assign(total_revenue=lambda df: df[\"total_revenue\"] + df[\"total_heritage\"])\n    .assign(natural_resource_revenue=lambda df: df[\"natural_resource_revenue\"] + df[\"total_heritage\"])\n    .assign(deficit=lambda df: df[\"total_expenditure\"] - df[\"total_revenue\"])\n)\nfig1(no_net_df)\n\n\n\n\n1976/1977 more closely matches the original chart in the paper, but the remaining years in the period of mid 70s to mid 80s when there were significant contributions clearly do not match. Let’s try one more where I just substitute that one year.\n\nerror_df = df.copy()\nheritage_76 = error_df.loc[\"1976\", \"total_heritage\"]\nerror_df.loc[\"1976\", \"natural_resource_revenue\"] += heritage_76\nerror_df.loc[\"1976\", \"total_revenue\"] += heritage_76\nerror_df.loc[\"1976\", \"deficit\"] -= heritage_76\nfig1(error_df)\n\n\n\n\nHere’s the original figure again for easier comparison\n\nImage(filename=\"img/ferede_fig_1.png\")\n\n\n\n\nFrom eyeballing it that looks exactly like Figure 1 in the paper. It appears there’s a data error in the original paper. For the rest of this analysis I’ll compare both my base implementation of the data, as well as the one with the data error.\n\n\nFigure 2\nPage 6 of the paper produces a scatter plot of Real per capita non-renewable resource revenue on the X axis vs. Real per capita budget balance on the Y, along with a linear trend fit.\n\n\nCode\ndef fig2(df: pd.DataFrame) -&gt; None:\n    \"\"\"Reproduce Figure 2 from the paper.\n    \n    Parameters\n    ----------\n    df: pd.DataFrame\n        The table with historical revenue and expenditure data.\n    \"\"\"\n    sns.set(rc={'figure.figsize':(11.7,8.27)})\n    chart_df = (\n        df\n        .loc['1970':'2016', ['natural_resource_revenue', 'deficit']]\n        .rename(columns={\n            'natural_resource_revenue': 'Non-renewable Resource Revenue',\n            'deficit': 'Deficit'\n        })\n        .assign(balance=lambda df: df['Deficit'] * -1)\n        .rename(columns={'balance': 'Budget Balance'})\n        .copy()\n    )\n    sns.regplot(x='Non-renewable Resource Revenue', y='Budget Balance', data=chart_df)\n\n\nHere’s the original figure\n\nImage(filename=\"img/ferede_fig_2.png\")\n\n\n\n\nHere’s the figure using my original data:\n\nfig2(df)\n\n\n\n\nAnd here’s the figure using the version with a data error:\n\nfig2(error_df)\n\n\n\n\nAgain, this chart is more consistent with the dataframe where I don’t net heritage fund contributions out of 1976 but do for all other years"
  },
  {
    "objectID": "posts/2021-02-26-ferede.html#model-specification-and-estimation",
    "href": "posts/2021-02-26-ferede.html#model-specification-and-estimation",
    "title": "Alberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue",
    "section": "Model Specification and estimation",
    "text": "Model Specification and estimation\nThis section combines the previously specified data extraction with transformations necessary to produce summary statistics, statistical tests, and the VAR model itself.\n\ndef model_df_levels(budg: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Combine real per capita budget data to get model data in levels.\n\n    lag exogenous variables (unemployment and employment rates, CAD/USD exchange)\n\n    Parameters\n    ----------\n    budg: pd.DataFrame\n        Budget data, either with or without data error\n\n    Returns\n    -------\n    pd.DataFrame\n        Budget data combined with exogenous factors\n    \"\"\"\n    exog = exogenous_variables()\n    df = (\n        pd.concat([budg, exog], axis='columns')\n        .rename(columns={'total_prog_exp': 'program_expenditure', 'EXCAUS': 'cad_usd'})\n        .assign(ur_lag=lambda df: df['unemployment_rate'].shift(periods=1))\n        .assign(er_lag=lambda df: df['employment_rate'].shift(periods=1))\n        .assign(cad_usd_lag=lambda df: df['cad_usd'].shift(periods=1))\n        .reindex(columns=[\n            'program_expenditure', 'debt_service', 'corporate_income_tax',\n            'personal_income_tax', 'other_revenue', 'natural_resource_revenue',\n            'deficit', 'heritage_dummy', 'ur_lag', 'er_lag', 'cad_usd_lag'\n        ])\n    )\n    return df\n\n\nmdfl = model_df_levels(df)\nmdfl_err = model_df_levels(error_df)\n\n\nSumary statistics for key variables, 1970-71, 2016-17 in levels\nPrior to any modeling, let’s compare the summary statistics for the data sets I’ve created against those in the paper:\n\n\nCode\nnumber = \"{:0&lt;4,.1f}\"\npercent = '{:.1%}'\ncount = \"{:0.0f}\"\n\n\ndef tbl1_level(model_df: pd.DataFrame):\n    \"\"\"Produce summary statistics of the input data in levels.\n\n    Parameters\n    ----------\n    model_df: pd.DataFrame\n        Input data set\n\n    Returns\n    -------\n    pd.io.formats.style.Styler:\n        Nicely formatted summary statistics\n    \"\"\"\n    df = (\n        model_df\n        .loc['1970':'2016']\n        .copy()\n        .drop(columns=['heritage_dummy'])\n        .reindex(columns=[\n            'natural_resource_revenue', 'corporate_income_tax', 'personal_income_tax',\n            'other_revenue', 'debt_service', 'program_expenditure', 'deficit', 'ur_lag',\n            'er_lag', 'cad_usd_lag'\n        ])\n        .describe()\n        .T\n        .style.format({\n            'count': count,\n            'mean': number,\n            'std': number,\n            'min': number,\n            '25%': number,\n            '50%': number,\n            '75%': number,\n            'max': number\n        })\n    )\n    return df\n\n\nHere’s Table 1 from the paper:\n\nImage(filename=\"img/ferede_tbl_1.png\")\n\n\n\n\nHere’s my summary of the top half\n\ntbl1_level(mdfl)\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nnatural_resource_revenue\n47\n2,737.2\n1,358.9\n-125.3\n1,726.4\n2,371.7\n3,941.6\n5,181.6\n\n\ncorporate_income_tax\n47\n792.1\n349.6\n245.8\n544.6\n723.6\n1,010.9\n1,560.6\n\n\npersonal_income_tax\n47\n1,862.1\n607.8\n767.1\n1,403.8\n1,889.6\n2,359.8\n2,830.0\n\n\nother_revenue\n47\n4,394.6\n1,084.9\n2,352.8\n3,632.2\n4,699.2\n5,209.2\n5,818.2\n\n\ndebt_service\n47\n322.8\n334.9\n31.4\n82.7\n158.5\n486.9\n1,075.6\n\n\nprogram_expenditure\n47\n9,399.1\n2,161.2\n4,745.8\n7,623.0\n10,060.9\n11,142.6\n12,869.2\n\n\ndeficit\n47\n-64.0\n1,618.0\n-3,184.0\n-1,320.5\n-140.6\n1,092.3\n3,775.8\n\n\nur_lag\n47\n6.10\n2.20\n3.40\n4.50\n5.40\n7.50\n11.4\n\n\ner_lag\n47\n63.7\n9.90\n38.5\n65.1\n67.3\n69.0\n71.8\n\n\ncad_usd_lag\n45\n1.20\n0.20\n1.00\n1.10\n1.20\n1.40\n1.60\n\n\n\n\n\nAnd the same summary on the data with the introduced error\n\ntbl1_level(mdfl_err)\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nnatural_resource_revenue\n47\n2,843.8\n1,325.6\n691.9\n1,760.7\n2,532.1\n3,972.6\n5,181.6\n\n\ncorporate_income_tax\n47\n792.1\n349.6\n245.8\n544.6\n723.6\n1,010.9\n1,560.6\n\n\npersonal_income_tax\n47\n1,862.1\n607.8\n767.1\n1,403.8\n1,889.6\n2,359.8\n2,830.0\n\n\nother_revenue\n47\n4,394.6\n1,084.9\n2,352.8\n3,632.2\n4,699.2\n5,209.2\n5,818.2\n\n\ndebt_service\n47\n322.8\n334.9\n31.4\n82.7\n158.5\n486.9\n1,075.6\n\n\nprogram_expenditure\n47\n9,399.1\n2,161.2\n4,745.8\n7,623.0\n10,060.9\n11,142.6\n12,869.2\n\n\ndeficit\n47\n-170.7\n1,587.7\n-3,184.0\n-1,425.7\n-202.8\n945.9\n3,775.8\n\n\nur_lag\n47\n6.10\n2.20\n3.40\n4.50\n5.40\n7.50\n11.4\n\n\ner_lag\n47\n63.7\n9.90\n38.5\n65.1\n67.3\n69.0\n71.8\n\n\ncad_usd_lag\n45\n1.20\n0.20\n1.00\n1.10\n1.20\n1.40\n1.60\n\n\n\n\n\nAll the figures that I can validate against (exogenous variables aren’t reported in the paper) are reasonably close. The one noted difference is the previously described outlier in natural resource revenue which leads to my minimum for that variable being significantly lower than in the paper. That large one goes away again if I introduce the same data error described above. My guess for the remaining small discrepancies are differences in calculating population or CPI.\n\n\nSumary statistics for key variables, 1970-71, 2016-17, first difference\nReproduce the bottom half of table 1 from the paper\n\n\nCode\ndef model_df_first_diff(mdfl: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Produce the first difference of the level model df.\n\n    Parameters\n    ----------\n    mdfl: pd.DataFrame\n        The model dataframe in levels\n\n    Returns\n    -------\n    pd.DataFrame\n        The first differenced model dataframe\n    \"\"\"\n    df = (\n        mdfl\n        .diff()\n        .loc['1970':'2016']\n        .copy()\n        .assign(heritage_dummy=mdfl['heritage_dummy']) # don't want to lag diff this\n        .assign(constant=1)\n        .assign(zero=0)\n        .assign(nrrd=lambda df: df[['natural_resource_revenue', 'zero']].min(axis='columns'))\n        .assign(nrri=lambda df: df[['natural_resource_revenue', 'zero']].max(axis='columns'))\n        .reindex(columns=[\n            'natural_resource_revenue', 'nrri', 'nrrd', 'corporate_income_tax', 'personal_income_tax',\n            'other_revenue', 'debt_service', 'program_expenditure', 'deficit', 'ur_lag',\n            'er_lag', 'cad_usd_lag', 'heritage_dummy', 'constant'\n        ])\n    )\n    return df\n\n\n\n\nCode\ndef tbl1_diff(model_df: pd.DataFrame) -&gt; pd.io.formats.style.Styler:\n    \"\"\"Produce summary statistics of the first differenced data set.\n\n    Parameters\n    ----------\n    model_df: pd.DataFrame\n        Input data set\n\n    Returns\n    -------\n    pd.io.formats.style.Styler:\n        Nicely styled summary statistics\n    \"\"\"\n\n    df = (\n        model_df_first_diff(model_df)\n        .drop(columns=['heritage_dummy', 'constant'])\n        .describe()\n        .T\n        .style.format({\n            'count': count,\n            'mean': number,\n            'std': number,\n            'min': number,\n            '25%': number,\n            '50%': number,\n            '75%': number,\n            'max': number\n        })\n    )\n    return df\n\n\nHere’s Table 1 from the paper again:\n\nImage(filename=\"img/ferede_tbl_1.png\")\n\n\n\n\nHere’s mine:\n\ntbl1_diff(mdfl)\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nnatural_resource_revenue\n47\n-8.3\n1,365.3\n-4,543.2\n-551.3\n64.2\n463.5\n4,638.7\n\n\nnrri\n47\n437.8\n852.9\n0.00\n0.00\n64.2\n463.5\n4,638.7\n\n\nnrrd\n47\n-446.1\n858.9\n-4,543.2\n-551.3\n0.00\n0.00\n0.00\n\n\ncorporate_income_tax\n47\n13.3\n201.2\n-463.0\n-108.5\n25.9\n137.8\n447.1\n\n\npersonal_income_tax\n47\n42.7\n227.2\n-690.7\n-28.7\n40.0\n154.2\n936.9\n\n\nother_revenue\n47\n41.8\n577.2\n-1,268.7\n-288.9\n-28.0\n162.8\n2,417.6\n\n\ndebt_service\n47\n4.50\n86.2\n-247.5\n-21.6\n2.00\n36.0\n222.3\n\n\nprogram_expenditure\n47\n147.4\n734.5\n-1,263.2\n-245.9\n202.7\n552.7\n2,477.9\n\n\ndeficit\n47\n62.3\n1,578.2\n-4,722.8\n-806.6\n-39.0\n836.0\n4,431.3\n\n\nur_lag\n47\n0.10\n1.10\n-1.8\n-0.7\n-0.1\n0.30\n3.90\n\n\ner_lag\n47\n0.60\n3.30\n-3.2\n-0.2\n0.40\n0.90\n21.4\n\n\ncad_usd_lag\n44\n0.00\n0.10\n-0.2\n-0.0\n0.00\n0.10\n0.20\n\n\n\n\n\nAnd with the data error:\n\ntbl1_diff(mdfl_err)\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nnatural_resource_revenue\n47\n-8.3\n977.5\n-2,451.6\n-471.1\n64.2\n458.3\n2,757.8\n\n\nnrri\n47\n349.1\n579.4\n0.00\n0.00\n64.2\n458.3\n2,757.8\n\n\nnrrd\n47\n-357.4\n604.0\n-2,451.6\n-471.1\n0.00\n0.00\n0.00\n\n\ncorporate_income_tax\n47\n13.3\n201.2\n-463.0\n-108.5\n25.9\n137.8\n447.1\n\n\npersonal_income_tax\n47\n42.7\n227.2\n-690.7\n-28.7\n40.0\n154.2\n936.9\n\n\nother_revenue\n47\n41.8\n577.2\n-1,268.7\n-288.9\n-28.0\n162.8\n2,417.6\n\n\ndebt_service\n47\n4.50\n86.2\n-247.5\n-21.6\n2.00\n36.0\n222.3\n\n\nprogram_expenditure\n47\n147.4\n734.5\n-1,263.2\n-245.9\n202.7\n552.7\n2,477.9\n\n\ndeficit\n47\n62.3\n1,260.2\n-2,615.7\n-758.1\n-39.0\n747.4\n3,117.0\n\n\nur_lag\n47\n0.10\n1.10\n-1.8\n-0.7\n-0.1\n0.30\n3.90\n\n\ner_lag\n47\n0.60\n3.30\n-3.2\n-0.2\n0.40\n0.90\n21.4\n\n\ncad_usd_lag\n44\n0.00\n0.10\n-0.2\n-0.0\n0.00\n0.10\n0.20\n\n\n\n\n\nAs with everything so far, the data overall matches, and Natural Resource Revenue matches a lot better if I neglect to net out the heritage fund in 1976.\n\n\nUnit-Root Tests\nTable A1 in the paper shows the results of unit root tests for both the level and first differenced variables in the model. This section will reproduce those tables\n\ndef stationarity_tests(df: pd.DataFrame, first_diff: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Compute stationarity test statistics.\n\n    Parameters\n    ----------\n    df: pd.DataFrame\n        The model input data\n    first_diff: bool, default False\n        Perform tests on first differenced version of the data\n    \"\"\"\n    if first_diff:\n        df = (\n            model_df_first_diff(df)\n            .drop(columns=[\"heritage_dummy\", \"constant\"])\n        )\n    else:\n        df = (\n            df\n            .loc['1970':'2016']\n            .copy()\n            .drop(columns=['heritage_dummy'])\n            .reindex(\n                columns=[\n                    'natural_resource_revenue',\n                    'corporate_income_tax',\n                    'personal_income_tax',\n                    'other_revenue',\n                    'debt_service',\n                    'program_expenditure',\n                    'deficit',\n                    'ur_lag',\n                    'er_lag',\n                    'cad_usd_lag',\n                ]\n            )\n        )\n    tests_dict = {'ADF': ADF, 'Phillips-Perron': PhillipsPerron, 'DF-GLS': DFGLS}\n    cols = df.columns\n    tests_df = pd.DataFrame()\n    for test_label, test in tests_dict.items():\n        for col in cols:\n            if test_label != 'Phillips-Perron':\n                col_test = test(df[col].dropna(), method='BIC')\n            else:\n                col_test = test(df[col].dropna())\n            test_val = col_test.stat\n            test_p = col_test.pvalue\n            test_summary = f'val: {test_val:0.3f}, p: {test_p:.1%}'\n            tests_df.loc[col, test_label] = test_summary\n    return tests_df\n\nHere’s the table from the paper:\n\nImage(filename=\"img/ferede_tbl_a1.png\")\n\n\n\n\nHere is mine with my data set\n\nstationarity_tests(mdfl, first_diff=False)\n\n\n\n\n\n\n\n\nADF\nPhillips-Perron\nDF-GLS\n\n\n\n\nnatural_resource_revenue\nval: -3.927, p: 0.2%\nval: -4.056, p: 0.1%\nval: -3.125, p: 0.2%\n\n\ncorporate_income_tax\nval: -2.340, p: 15.9%\nval: -2.285, p: 17.7%\nval: -1.676, p: 9.2%\n\n\npersonal_income_tax\nval: -1.650, p: 45.7%\nval: -1.440, p: 56.3%\nval: -0.688, p: 43.2%\n\n\nother_revenue\nval: -2.196, p: 20.8%\nval: -2.057, p: 26.2%\nval: -1.585, p: 11.0%\n\n\ndebt_service\nval: -1.548, p: 51.0%\nval: -1.697, p: 43.3%\nval: -1.377, p: 16.2%\n\n\nprogram_expenditure\nval: -2.966, p: 3.8%\nval: -2.137, p: 23.0%\nval: -0.682, p: 43.4%\n\n\ndeficit\nval: -3.510, p: 0.8%\nval: -3.924, p: 0.2%\nval: -3.537, p: 0.0%\n\n\nur_lag\nval: -2.709, p: 7.3%\nval: -2.071, p: 25.6%\nval: -2.319, p: 2.1%\n\n\ner_lag\nval: -10.652, p: 0.0%\nval: -2.931, p: 4.2%\nval: -0.763, p: 39.7%\n\n\ncad_usd_lag\nval: -2.454, p: 12.7%\nval: -1.796, p: 38.2%\nval: -1.870, p: 6.1%\n\n\n\n\n\n\n\n\nstationarity_tests(mdfl, first_diff=True)\n\n\n\n\n\n\n\n\nADF\nPhillips-Perron\nDF-GLS\n\n\n\n\nnatural_resource_revenue\nval: -7.403, p: 0.0%\nval: -10.300, p: 0.0%\nval: -7.430, p: 0.0%\n\n\nnrri\nval: -6.885, p: 0.0%\nval: -6.896, p: 0.0%\nval: -6.533, p: 0.0%\n\n\nnrrd\nval: -7.835, p: 0.0%\nval: -9.218, p: 0.0%\nval: -7.634, p: 0.0%\n\n\ncorporate_income_tax\nval: -7.228, p: 0.0%\nval: -8.484, p: 0.0%\nval: -7.103, p: 0.0%\n\n\npersonal_income_tax\nval: -5.560, p: 0.0%\nval: -8.831, p: 0.0%\nval: -6.703, p: 0.0%\n\n\nother_revenue\nval: -7.914, p: 0.0%\nval: -8.169, p: 0.0%\nval: -7.717, p: 0.0%\n\n\ndebt_service\nval: -3.918, p: 0.2%\nval: -4.252, p: 0.1%\nval: -3.931, p: 0.0%\n\n\nprogram_expenditure\nval: -2.545, p: 10.5%\nval: -6.105, p: 0.0%\nval: -2.461, p: 1.4%\n\n\ndeficit\nval: -2.485, p: 11.9%\nval: -10.105, p: 0.0%\nval: -2.509, p: 1.2%\n\n\nur_lag\nval: -4.924, p: 0.0%\nval: -4.056, p: 0.1%\nval: -4.901, p: 0.0%\n\n\ner_lag\nval: -2.697, p: 7.5%\nval: -6.507, p: 0.0%\nval: -2.742, p: 0.6%\n\n\ncad_usd_lag\nval: -3.318, p: 1.4%\nval: -3.042, p: 3.1%\nval: -3.384, p: 0.1%\n\n\n\n\n\n\n\nAnd with the error data set:\n\nstationarity_tests(mdfl_err, first_diff=False)\n\n\n\n\n\n\n\n\nADF\nPhillips-Perron\nDF-GLS\n\n\n\n\nnatural_resource_revenue\nval: -2.652, p: 8.3%\nval: -2.875, p: 4.8%\nval: -2.113, p: 3.5%\n\n\ncorporate_income_tax\nval: -2.340, p: 15.9%\nval: -2.285, p: 17.7%\nval: -1.676, p: 9.2%\n\n\npersonal_income_tax\nval: -1.650, p: 45.7%\nval: -1.440, p: 56.3%\nval: -0.688, p: 43.2%\n\n\nother_revenue\nval: -2.196, p: 20.8%\nval: -2.057, p: 26.2%\nval: -1.585, p: 11.0%\n\n\ndebt_service\nval: -1.548, p: 51.0%\nval: -1.697, p: 43.3%\nval: -1.377, p: 16.2%\n\n\nprogram_expenditure\nval: -2.966, p: 3.8%\nval: -2.137, p: 23.0%\nval: -0.682, p: 43.4%\n\n\ndeficit\nval: -2.566, p: 10.0%\nval: -2.763, p: 6.4%\nval: -2.606, p: 0.9%\n\n\nur_lag\nval: -2.709, p: 7.3%\nval: -2.071, p: 25.6%\nval: -2.319, p: 2.1%\n\n\ner_lag\nval: -10.652, p: 0.0%\nval: -2.931, p: 4.2%\nval: -0.763, p: 39.7%\n\n\ncad_usd_lag\nval: -2.454, p: 12.7%\nval: -1.796, p: 38.2%\nval: -1.870, p: 6.1%\n\n\n\n\n\n\n\n\nstationarity_tests(mdfl_err, first_diff=True)\n\n\n\n\n\n\n\n\nADF\nPhillips-Perron\nDF-GLS\n\n\n\n\nnatural_resource_revenue\nval: -5.913, p: 0.0%\nval: -7.791, p: 0.0%\nval: -5.925, p: 0.0%\n\n\nnrri\nval: -6.119, p: 0.0%\nval: -6.149, p: 0.0%\nval: -5.731, p: 0.0%\n\n\nnrrd\nval: -7.846, p: 0.0%\nval: -8.008, p: 0.0%\nval: -7.640, p: 0.0%\n\n\ncorporate_income_tax\nval: -7.228, p: 0.0%\nval: -8.484, p: 0.0%\nval: -7.103, p: 0.0%\n\n\npersonal_income_tax\nval: -5.560, p: 0.0%\nval: -8.831, p: 0.0%\nval: -6.703, p: 0.0%\n\n\nother_revenue\nval: -7.914, p: 0.0%\nval: -8.169, p: 0.0%\nval: -7.717, p: 0.0%\n\n\ndebt_service\nval: -3.918, p: 0.2%\nval: -4.252, p: 0.1%\nval: -3.931, p: 0.0%\n\n\nprogram_expenditure\nval: -2.545, p: 10.5%\nval: -6.105, p: 0.0%\nval: -2.461, p: 1.4%\n\n\ndeficit\nval: -6.642, p: 0.0%\nval: -8.574, p: 0.0%\nval: -6.609, p: 0.0%\n\n\nur_lag\nval: -4.924, p: 0.0%\nval: -4.056, p: 0.1%\nval: -4.901, p: 0.0%\n\n\ner_lag\nval: -2.697, p: 7.5%\nval: -6.507, p: 0.0%\nval: -2.742, p: 0.6%\n\n\ncad_usd_lag\nval: -3.318, p: 1.4%\nval: -3.042, p: 3.1%\nval: -3.384, p: 0.1%\n\n\n\n\n\n\n\nDocumentation on the test tools I used can be found here\nThere are some interesting differences. Most notable is that on the levels of the deficit series I reject the null hypothesis of a unit root using all three tests at a significance level &lt; 1%. The paper specifically notes that if the deficit is stationary in levels then a Vector Error Correction model can be applied. As the original author’s fails to reject the null he implements a Vector AutoRegression model on the first differenced data. In levels the only other series that I find to be stationary is natural resource revenue. ADF on program expenditure would also reject the null at 5% significance, but would fail to reject it using the other two tests.\nLooking at the first differenced series, since that’s what the paper ultimately ends up using, I also reject the null hypothesis of a unit root for all variables using all tests at a 1% significant except program expenditure and deficit using Augmented Dickey Fuller. Those last two tests differ from what’s reported in the paper.\nThe paper notes that it uses the Schwarz Information Criterion (SIC) for determining optimal lags in the DF-GLS test. It doesn’t specify what it’s using in the other two tests. For ADF and DF-GLS I used the Schwarz/Bayesian IC (BIC), which is just another name for SIC. Phillips-Perron only uses 1 lag and then Newey-West for a long run variance estimator. I also ran these tests using Akaike IC (AIC) for optimal lags for ADF and DF-GLS, with similar results.\nAgain we can see that using the results with the data error more closely matches the table in the paper, specifically around resource revenue and deficits.\n\n\nThe Model\nThe conclusions from the paper are based on fitting a VAR to the first differenced data set we’ve been analyzing above. Let’s do that now and compare the results to the paper.\n\ndef fit_var(mdfl: pd.DataFrame) -&gt; statsmodels.tsa.vector_ar.var_model.VARResults:\n    \"\"\"Fit a VAR to the model data.\n\n    Parameters\n    ----------\n    mdfl: pd.DataFrame\n        Input model data\n\n    Returns\n    -------\n    statsmodels.tsa.vector_ar.var_model.VarResults\n        The fitted model\n    \"\"\"\n    vec_df = (\n        model_df_first_diff(mdfl)\n        .drop(columns='natural_resource_revenue')\n        .dropna()\n    )\n    endog_df = vec_df[[\n        'nrri', 'nrrd', 'program_expenditure', 'debt_service', 'corporate_income_tax',\n         'personal_income_tax', 'other_revenue'\n    ]]\n    exog_df = vec_df[['ur_lag', 'er_lag', 'cad_usd_lag', 'heritage_dummy']]\n    model = VAR(endog=endog_df, exog=exog_df, freq='AS')\n    # Fit the model with 2 lags\n    results = model.fit(2)\n    return results\n\n\n\nCode\ndef highlight_significance(val):\n    \"\"\"Colour code statistical significance.\n\n    Takes a scalar and returns a string with\n    the css property `'color: &lt;color&gt;'` where\n    color is maroon for 1% significance, \n    red for 5% significance,\n    orange for 10%, and black otherwise\n\n    Parameters\n    ----------\n    val: float\n        The p value of a test\n\n    Returns\n    -------\n    str:\n        A formatted colour coded p value\n    \"\"\"\n\n    if val &lt;= 0.01:\n        color = 'maroon'\n    elif val &lt;= 0.05:\n        color = 'red'\n    elif val &lt;= 0.1:\n        color = 'orange'\n    else:\n        color = 'black'\n    return f'color: {color}'\n\n\n\nresults = fit_var(mdfl)\nresults_err = fit_var(mdfl_err)\nsummary = results.summary()\n\n\n# Set up the rows and columns of my parameters to match the paper\nreindex_cols = [\"program_expenditure\", \"debt_service\", \"corporate_income_tax\", \"personal_income_tax\", \"other_revenue\", \"nrri\", \"nrrd\"]\nindex_order = [\"nrri\", \"nrrd\", \"program_expenditure\", \"debt_service\", \"corporate_income_tax\", \"personal_income_tax\", \"other_revenue\"]\nreindex_rows = list(chain.from_iterable((f\"L1.{row}\", f\"L2.{row}\") for row in index_order))\n\nHere’s the table from the paper:\n\nImage(filename=\"img/ferede_tbl_a2.png\")\n\n\n\n\n\nresults.params.reindex(index=reindex_rows, columns=reindex_cols)\n\n\n\n\n\n\n\n\nprogram_expenditure\ndebt_service\ncorporate_income_tax\npersonal_income_tax\nother_revenue\nnrri\nnrrd\n\n\n\n\nL1.nrri\n0.307014\n-0.028267\n0.081846\n-0.052581\n-0.197417\n0.157775\n0.002814\n\n\nL2.nrri\n0.328607\n0.006210\n-0.044256\n-0.031283\n-0.073461\n0.059988\n-0.052033\n\n\nL1.nrrd\n0.131802\n-0.034380\n0.026578\n-0.144894\n0.193682\n-0.067386\n-0.333322\n\n\nL2.nrrd\n0.353466\n-0.020362\n0.089569\n-0.037287\n-0.147460\n0.110970\n-0.055394\n\n\nL1.program_expenditure\n-0.089949\n0.024739\n-0.000802\n-0.053868\n0.070080\n-0.065026\n-0.105625\n\n\nL2.program_expenditure\n-0.262484\n0.009716\n-0.031223\n0.042743\n0.410439\n0.025392\n-0.456895\n\n\nL1.debt_service\n0.794685\n0.401266\n-0.108643\n0.336733\n0.400447\n-2.370450\n-0.278888\n\n\nL2.debt_service\n-2.693856\n0.228409\n-0.088742\n-0.179124\n-0.041884\n0.290135\n-0.215989\n\n\nL1.corporate_income_tax\n-0.972853\n-0.092308\n-0.210123\n0.169059\n-0.071714\n-0.914462\n0.730905\n\n\nL2.corporate_income_tax\n0.088413\n-0.199031\n-0.206852\n0.245321\n-0.509617\n-0.683630\n-0.578293\n\n\nL1.personal_income_tax\n0.959002\n-0.041882\n0.292569\n-0.378646\n-0.250158\n0.347750\n0.862515\n\n\nL2.personal_income_tax\n0.814557\n0.084507\n-0.112748\n-0.099020\n-0.528812\n0.385575\n-0.226230\n\n\nL1.other_revenue\n0.347131\n-0.015834\n-0.042866\n-0.022883\n-0.296401\n0.224576\n-0.102018\n\n\nL2.other_revenue\n-0.140592\n-0.010601\n-0.009822\n-0.010749\n-0.339955\n0.385613\n0.043835\n\n\n\n\n\n\n\n\nresults.pvalues.reindex(index=reindex_rows, columns=reindex_cols).style.applymap(highlight_significance).format(\"{:.2%}\")\n\n\n\n\n\n\nprogram_expenditure\ndebt_service\ncorporate_income_tax\npersonal_income_tax\nother_revenue\nnrri\nnrrd\n\n\n\n\nL1.nrri\n17.85%\n28.23%\n19.05%\n46.54%\n31.40%\n35.35%\n99.27%\n\n\nL2.nrri\n3.41%\n72.82%\n29.76%\n52.28%\n58.14%\n60.37%\n80.42%\n\n\nL1.nrrd\n54.73%\n17.31%\n65.78%\n3.61%\n30.34%\n67.97%\n26.07%\n\n\nL2.nrrd\n8.86%\n39.46%\n11.53%\n56.93%\n40.83%\n47.32%\n84.37%\n\n\nL1.program_expenditure\n63.72%\n26.03%\n98.78%\n37.09%\n66.89%\n64.73%\n68.24%\n\n\nL2.program_expenditure\n16.67%\n65.69%\n54.83%\n47.56%\n1.18%\n85.75%\n7.53%\n\n\nL1.debt_service\n62.80%\n3.37%\n80.89%\n51.54%\n77.63%\n5.24%\n90.00%\n\n\nL2.debt_service\n9.99%\n22.60%\n84.32%\n72.89%\n97.63%\n81.21%\n92.24%\n\n\nL1.corporate_income_tax\n17.66%\n26.58%\n28.68%\n45.69%\n90.77%\n8.83%\n45.31%\n\n\nL2.corporate_income_tax\n90.24%\n1.66%\n29.52%\n28.12%\n41.09%\n20.34%\n55.35%\n\n\nL1.personal_income_tax\n11.91%\n55.47%\n8.27%\n5.12%\n63.61%\n44.83%\n30.03%\n\n\nL2.personal_income_tax\n12.28%\n16.47%\n43.56%\n55.23%\n24.36%\n32.70%\n75.15%\n\n\nL1.other_revenue\n11.25%\n52.98%\n47.45%\n74.03%\n11.48%\n16.83%\n73.04%\n\n\nL2.other_revenue\n54.20%\n68.98%\n87.64%\n88.26%\n8.61%\n2.48%\n88.83%\n\n\n\n\n\n\nresults_err.params.reindex(index=reindex_rows, columns=reindex_cols)\n\n\n\n\n\n\n\n\nprogram_expenditure\ndebt_service\ncorporate_income_tax\npersonal_income_tax\nother_revenue\nnrri\nnrrd\n\n\n\n\nL1.nrri\n0.531582\n-0.024542\n0.107731\n-0.123072\n-0.150617\n0.208281\n-0.080868\n\n\nL2.nrri\n-0.184916\n-0.003392\n-0.079855\n-0.031832\n-0.134564\n0.023580\n0.076075\n\n\nL1.nrrd\n-0.164689\n-0.033454\n0.002593\n-0.085661\n0.110608\n-0.058941\n-0.105753\n\n\nL2.nrrd\n0.179398\n-0.020179\n0.072508\n0.027891\n-0.178763\n0.081864\n-0.032297\n\n\nL1.program_expenditure\n-0.148569\n0.024375\n0.010583\n-0.035676\n0.095480\n-0.051153\n-0.027252\n\n\nL2.program_expenditure\n-0.274168\n0.012469\n-0.028204\n0.060072\n0.419366\n0.043190\n-0.266589\n\n\nL1.debt_service\n-0.138827\n0.391381\n-0.192682\n0.456206\n0.250624\n-2.379002\n0.182134\n\n\nL2.debt_service\n-1.839879\n0.244616\n0.030729\n-0.321701\n0.208719\n0.387627\n0.026273\n\n\nL1.corporate_income_tax\n-0.591167\n-0.087419\n-0.166213\n0.193455\n-0.009423\n-1.069105\n0.084121\n\n\nL2.corporate_income_tax\n0.986182\n-0.171169\n-0.180347\n0.219469\n-0.439124\n-0.441616\n0.337464\n\n\nL1.personal_income_tax\n0.446547\n-0.050872\n0.281996\n-0.364826\n-0.291701\n0.365623\n0.499642\n\n\nL2.personal_income_tax\n0.537147\n0.068406\n-0.161794\n-0.159843\n-0.598672\n0.542697\n-0.054906\n\n\nL1.other_revenue\n0.319686\n-0.014068\n-0.036482\n-0.021636\n-0.280981\n0.235076\n-0.052564\n\n\nL2.other_revenue\n-0.328932\n-0.009975\n-0.017606\n0.013375\n-0.362590\n0.255418\n-0.122366\n\n\n\n\n\n\n\n\nresults_err.pvalues.reindex(index=reindex_rows, columns=reindex_cols).style.applymap(highlight_significance).format(\"{:.2%}\")\n\n\n\n\n\n\nprogram_expenditure\ndebt_service\ncorporate_income_tax\npersonal_income_tax\nother_revenue\nnrri\nnrrd\n\n\n\n\nL1.nrri\n6.09%\n42.34%\n12.95%\n12.90%\n50.73%\n26.95%\n75.32%\n\n\nL2.nrri\n53.56%\n91.63%\n28.56%\n70.91%\n57.36%\n90.54%\n77.87%\n\n\nL1.nrrd\n54.19%\n25.17%\n96.94%\n26.71%\n60.91%\n74.28%\n66.59%\n\n\nL2.nrrd\n49.68%\n47.94%\n27.30%\n71.17%\n39.79%\n64.10%\n89.27%\n\n\nL1.program_expenditure\n47.97%\n28.33%\n84.07%\n55.26%\n57.06%\n71.44%\n88.63%\n\n\nL2.program_expenditure\n18.15%\n57.39%\n58.33%\n30.58%\n1.07%\n75.16%\n15.20%\n\n\nL1.debt_service\n93.96%\n4.82%\n67.48%\n38.39%\n86.44%\n5.10%\n91.27%\n\n\nL2.debt_service\n31.50%\n21.64%\n94.66%\n53.88%\n88.68%\n75.02%\n98.74%\n\n\nL1.corporate_income_tax\n47.78%\n33.14%\n42.56%\n41.64%\n98.87%\n5.35%\n91.13%\n\n\nL2.corporate_income_tax\n17.90%\n3.09%\n32.66%\n29.54%\n45.50%\n36.55%\n61.21%\n\n\nL1.personal_income_tax\n51.65%\n49.41%\n10.20%\n6.37%\n59.67%\n42.44%\n42.34%\n\n\nL2.personal_income_tax\n39.82%\n31.95%\n30.98%\n37.91%\n23.97%\n19.93%\n92.41%\n\n\nL1.other_revenue\n16.84%\n57.49%\n53.04%\n74.43%\n13.07%\n12.78%\n80.28%\n\n\nL2.other_revenue\n18.72%\n71.13%\n77.81%\n85.12%\n6.95%\n12.35%\n58.85%\n\n\n\n\n\nOnce again, my results with the data error included are much closer to the original paper results. Note that without the data error, program spending is shown to rise in response to an increase or decrease in natural resource revenue, but only on the second lag (at least at a statistically significant level. That’s completely contrary to the main thesis of the paper. Now, given that I’ve shown with the corrected data set that budget deficits are stationary in levels, maybe a more appropriate form of analysis would have been to use a VECM as the paper states it would have been, but from this I can say that the data error has led to a significant change in the outcome of the analysis.\n\n\nImpulse Response Functions\nThe actual results of the paper involve taking the estimated impulse response functions derived from the VAR model and examining their implications. Given the results above I’m not sure there’s a lot of value in reproducing all of the other results of this model, but I do want to at least reproduce the IRFs.\n\n\nCode\ndef irf_tbl(result: statsmodels.tsa.vector_ar.var_model.VARResults, impulse: str) -&gt; pd.DataFrame:\n    \"\"\"Show the IRF as in table 3 of the paper.\n\n    Parameters\n    ----------\n    result: statsmodels.tsa.vector_ar.var_model.VARResults\n        The fitted VAR\n\n    impulse: str\n        The impulse function, either an increase or decrease in natural resource revenue\n\n    Returns\n    -------\n    pd.DataFrame\n        A summary table\n    \"\"\"\n    irf = result.irf()\n    irf_stderr = irf.stderr()\n    irfs = irf.irfs\n    params = list(results.params.columns)\n\n    def _impulse_response(impulse: str, response: str) -&gt; pd.DataFrame:\n        \"\"\"Get a specific IRF out of the big array.\n\n        Parameters\n        ----------\n        impulse: str\n            The impulse function\n        response: str\n            The response function\n\n        Returns\n        -------\n        pd.DataFrame\n            The 3 period IRF\n        \"\"\"\n        imp_ind = params.index(impulse)\n        res_ind = params.index(response)\n        ir = irfs[:, res_ind, imp_ind]\n        se = irf_stderr[:, res_ind, imp_ind]\n        imp_name = response + '_impulse'\n        se_name = response + '_se'\n        df = pd.DataFrame({imp_name: ir, se_name: se})\n        return df.loc[1:3].T\n\n    responses = params[2:]\n    return pd.concat([_impulse_response(impulse, response) for response in responses])\n\n\n\nTable 3\nIMPACTS ON ALBERTA’S BUDGET OF A ONE-DOLLAR INNOVATION IN NON-RENEWABLE-RESOURCE REVENUE (ASYMMETRIC CASE), 1970/71–2016/17\n\nImage(filename=\"img/ferede_tbl_3.png\")\n\n\n\n\n\nirf_tbl(results, \"nrrd\")\n\n\n\n\n\n\n\n\n1\n2\n3\n\n\n\n\nprogram_expenditure_impulse\n0.131802\n0.152091\n-0.263057\n\n\nprogram_expenditure_se\n0.219009\n0.213435\n0.189561\n\n\ndebt_service_impulse\n-0.034380\n-0.016984\n-0.025572\n\n\ndebt_service_se\n0.025235\n0.026153\n0.023777\n\n\ncorporate_income_tax_impulse\n0.026578\n0.022545\n0.014045\n\n\ncorporate_income_tax_se\n0.060010\n0.062050\n0.049948\n\n\npersonal_income_tax_impulse\n-0.144894\n0.050801\n0.023045\n\n\npersonal_income_tax_se\n0.069135\n0.090279\n0.082072\n\n\nother_revenue_impulse\n0.193682\n-0.226313\n0.116648\n\n\nother_revenue_se\n0.188179\n0.190130\n0.169276\n\n\n\n\n\n\n\n\nirf_tbl(results, \"nrri\")\n\n\n\n\n\n\n\n\n1\n2\n3\n\n\n\n\nprogram_expenditure_impulse\n0.307014\n0.128759\n0.032942\n\n\nprogram_expenditure_se\n0.228184\n0.200129\n0.172849\n\n\ndebt_service_impulse\n-0.028267\n-0.004321\n-0.011658\n\n\ndebt_service_se\n0.026292\n0.026157\n0.025746\n\n\ncorporate_income_tax_impulse\n0.081846\n-0.052562\n-0.019982\n\n\ncorporate_income_tax_se\n0.062524\n0.057369\n0.041195\n\n\npersonal_income_tax_impulse\n-0.052581\n-0.027779\n0.040890\n\n\npersonal_income_tax_se\n0.072031\n0.085436\n0.068562\n\n\nother_revenue_impulse\n-0.197417\n-0.028068\n0.187637\n\n\nother_revenue_se\n0.196063\n0.172347\n0.147472\n\n\n\n\n\n\n\n\nirf_tbl(results_err, \"nrrd\")\n\n\n\n\n\n\n\n\n1\n2\n3\n\n\n\n\nprogram_expenditure_impulse\n-0.164689\n0.190170\n0.024520\n\n\nprogram_expenditure_se\n0.270039\n0.264733\n0.231589\n\n\ndebt_service_impulse\n-0.033454\n-0.029727\n-0.027865\n\n\ndebt_service_se\n0.029185\n0.033489\n0.028892\n\n\ncorporate_income_tax_impulse\n0.002593\n0.041965\n0.055297\n\n\ncorporate_income_tax_se\n0.067657\n0.068952\n0.054178\n\n\npersonal_income_tax_impulse\n-0.085661\n0.064177\n-0.029006\n\n\npersonal_income_tax_se\n0.077188\n0.085444\n0.072486\n\n\nother_revenue_impulse\n0.110608\n-0.211807\n-0.018863\n\n\nother_revenue_se\n0.216285\n0.206303\n0.197149\n\n\n\n\n\n\n\n\nirf_tbl(results_err, \"nrri\")\n\n\n\n\n\n\n\n\n1\n2\n3\n\n\n\n\nprogram_expenditure_impulse\n0.531582\n-0.303243\n-0.034754\n\n\nprogram_expenditure_se\n0.283645\n0.282106\n0.247443\n\n\ndebt_service_impulse\n-0.024542\n-0.003484\n-0.020839\n\n\ndebt_service_se\n0.030655\n0.033327\n0.030126\n\n\ncorporate_income_tax_impulse\n0.107731\n-0.094389\n-0.033291\n\n\ncorporate_income_tax_se\n0.071066\n0.074045\n0.058701\n\n\npersonal_income_tax_impulse\n-0.123072\n-0.011699\n0.079664\n\n\npersonal_income_tax_se\n0.081077\n0.093111\n0.078465\n\n\nother_revenue_impulse\n-0.150617\n-0.053069\n0.289039\n\n\nother_revenue_se\n0.227183\n0.225741\n0.202924\n\n\n\n\n\n\n\nSame patters as we’ve seen above, the results with the data error included are pretty close to what the paper reports.\n\n\nPlot the IRFs\nOk, this is the last bit I want to reproduce, just because IRF charts look cool so it would be a shame to leave them out.\n\nImage(filename=\"img/ferede_fig_3.png\")\n\n\n\n\n\nresults.irf().plot(impulse='nrri', response='program_expenditure');\n\n\n\n\n\nresults.irf().plot(impulse='nrrd', response='program_expenditure');\n\n\n\n\n\nresults_err.irf().plot(impulse='nrri', response='program_expenditure');\n\n\n\n\n\nresults_err.irf().plot(impulse='nrrd', response='program_expenditure');\n\n\n\n\nThese are harder to eyeball because I’m showing the individual responses and the paper is showing the cumulative ones. But again, if I think about what adding up the points in my charts would look like, they end up closer to the data set with the error."
  },
  {
    "objectID": "posts/2021-02-26-ferede.html#conclusion",
    "href": "posts/2021-02-26-ferede.html#conclusion",
    "title": "Alberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue",
    "section": "Conclusion",
    "text": "Conclusion\nThe initial goal of this exercise was to practice my python and econometrics. I’ve certainly done that over the course of working on it, but as a bonus I’ve also demonstrated how sensitive results can be to even a single data point (at least when you have relatively few samples."
  },
  {
    "objectID": "posts/2021-02-26-ferede.html#appendix-raw-tables",
    "href": "posts/2021-02-26-ferede.html#appendix-raw-tables",
    "title": "Alberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue",
    "section": "Appendix: Raw tables",
    "text": "Appendix: Raw tables\nFor the purposes of validation, here are the full tables I used to produce both the summary statistics above, as well as all statistical models and tests below\n\nmdfl\n\n\n\n\n\n\n\n\nprogram_expenditure\ndebt_service\ncorporate_income_tax\npersonal_income_tax\nother_revenue\nnatural_resource_revenue\ndeficit\nheritage_dummy\nur_lag\ner_lag\ncad_usd_lag\n\n\nbudget_dt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1964-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1965-01-01\n2327.844291\n11.272854\n174.729232\n231.093501\n1442.925275\n1397.833860\n-907.464724\n0.0\n2.500000\n37.935748\nNaN\n\n\n1966-01-01\n2891.984579\n10.750872\n145.136773\n301.024417\n1542.750138\n1290.104645\n-376.280521\n0.0\n2.500000\n37.935748\nNaN\n\n\n1967-01-01\n4436.875339\n10.199713\n203.994268\n407.988537\n2351.033944\n1116.868620\n367.189683\n0.0\n2.500000\n37.935748\nNaN\n\n\n1968-01-01\n4368.535726\n19.160244\n239.503055\n469.425988\n2639.323668\n1360.377353\n-320.934094\n0.0\n2.700000\n38.322148\nNaN\n\n\n1969-01-01\n4396.849967\n17.909776\n286.556413\n599.977490\n2444.684401\n1141.748209\n-58.206771\n0.0\n3.300000\n39.041995\nNaN\n\n\n1970-01-01\n4745.762712\n38.135593\n245.762712\n771.186441\n2737.288136\n978.813559\n50.847458\n0.0\n3.400000\n39.833226\nNaN\n\n\n1971-01-01\n4993.823780\n67.916003\n271.664014\n767.051333\n2848.477084\n1090.651114\n83.896240\n0.0\n5.200000\n39.686520\nNaN\n\n\n1972-01-01\n4988.477844\n85.114978\n362.663820\n858.551083\n2768.087112\n1224.915554\n-140.624746\n0.0\n5.700000\n38.456938\n1.009883\n\n\n1973-01-01\n5004.356719\n81.151731\n382.089398\n977.202089\n2387.213408\n2160.664826\n-821.661272\n0.0\n5.700000\n39.470588\n0.990792\n\n\n1974-01-01\n6124.378806\n92.793618\n829.155879\n1038.689856\n2352.767225\n4549.880638\n-2553.321174\n0.0\n5.300000\n40.843443\n1.000233\n\n\n1975-01-01\n7071.218716\n84.149869\n686.347373\n925.648564\n2721.722340\n4417.868145\n-1596.217836\n0.0\n3.500000\n42.840909\n0.978133\n\n\n1976-01-01\n6845.363213\n59.113672\n326.307469\n1038.036080\n2830.362613\n-125.320985\n2835.091707\n1.0\n4.200000\n43.439912\n1.017267\n\n\n1977-01-01\n7061.151803\n46.247496\n554.969954\n1206.639219\n2720.193639\n4513.335196\n-1887.738708\n1.0\n3.891667\n64.800000\n0.986075\n\n\n1978-01-01\n6825.084687\n48.260049\n545.709790\n1225.062794\n2643.165786\n4701.642511\n-2242.236144\n1.0\n4.475000\n64.633333\n1.063525\n\n\n1979-01-01\n9302.990086\n32.774318\n342.491620\n1265.088664\n2525.261181\n5181.619632\n21.303307\n1.0\n4.750000\n65.341667\n1.140767\n\n\n1980-01-01\n9937.742745\n31.376340\n620.395823\n1337.773062\n2676.972321\n4797.727697\n536.250183\n1.0\n3.966667\n67.125000\n1.171558\n\n\n1981-01-01\n10441.551444\n109.885646\n701.577586\n1597.568239\n5094.588359\n4001.769572\n-844.066666\n1.0\n3.866667\n68.050000\n1.169408\n\n\n1982-01-01\n12281.969722\n57.716023\n639.073783\n1731.480694\n4808.269418\n2887.899921\n2272.961929\n1.0\n3.883333\n69.500000\n1.198892\n\n\n1983-01-01\n11489.190564\n168.596206\n771.007210\n1488.773513\n4813.372379\n4001.941517\n582.692150\n1.0\n7.750000\n66.275000\n1.233858\n\n\n1984-01-01\n11320.287262\n218.545766\n793.666203\n1396.584127\n5529.782999\n4306.693537\n-487.893837\n1.0\n11.008333\n63.816667\n1.232583\n\n\n1985-01-01\n12869.198063\n168.831114\n723.561918\n1410.945740\n5622.447161\n3939.701880\n1341.372479\n1.0\n11.408333\n63.933333\n1.295150\n\n\n1986-01-01\n11863.166376\n263.862833\n351.817111\n1570.739021\n4940.542816\n1488.115306\n3775.814955\n1.0\n9.775000\n65.491667\n1.365900\n\n\n1987-01-01\n10791.884860\n486.136253\n505.683690\n1900.350807\n5480.081397\n2231.807343\n1160.097876\n0.0\n10.008333\n65.350000\n1.389767\n\n\n1988-01-01\n10774.989850\n655.047057\n572.857965\n1675.835569\n5818.165763\n1713.642551\n1649.535059\n0.0\n9.525000\n65.450000\n1.326175\n\n\n1989-01-01\n10745.455671\n865.692419\n543.484030\n1968.964999\n5716.675586\n1739.148895\n1642.874581\n0.0\n7.983333\n66.925000\n1.230942\n\n\n1990-01-01\n10614.752643\n923.523101\n578.462597\n2014.173627\n5689.536231\n1936.372929\n1319.730359\n0.0\n7.158333\n67.516667\n1.184108\n\n\n1991-01-01\n10060.915107\n878.291420\n488.608088\n2043.330951\n5298.490171\n1351.526066\n1757.251250\n0.0\n6.916667\n67.500000\n1.167017\n\n\n1992-01-01\n10490.255898\n920.232018\n413.099221\n1811.929709\n5614.128666\n1415.691681\n2155.638638\n0.0\n8.250000\n66.591667\n1.145975\n\n\n1993-01-01\n9567.673764\n1046.414892\n540.289188\n1820.154561\n5604.076850\n1782.195133\n867.372924\n0.0\n9.458333\n65.383333\n1.208808\n\n\n1994-01-01\n8304.447335\n1075.635389\n661.029079\n1886.982358\n5328.892392\n2081.040289\n-577.861395\n0.0\n9.583333\n64.875000\n1.290167\n\n\n1995-01-01\n7542.256294\n1000.994980\n792.231321\n1889.578759\n4888.995090\n1657.024370\n-684.578266\n0.0\n8.783333\n65.725000\n1.365892\n\n\n1996-01-01\n7286.510699\n838.743299\n807.190029\n1976.382124\n4455.321793\n2314.288966\n-1427.928914\n0.0\n7.841667\n66.650000\n1.372650\n\n\n1997-01-01\n7598.400009\n729.331650\n1020.071271\n2138.894710\n4551.426710\n2084.277589\n-1466.938621\n0.0\n6.883333\n67.316667\n1.363700\n\n\n1998-01-01\n7647.600932\n735.120709\n884.383797\n2452.712386\n4366.478407\n1262.339259\n-583.192208\n0.0\n5.875000\n67.925000\n1.384867\n\n\n1999-01-01\n8342.393922\n487.608742\n640.113987\n2601.260027\n4640.443868\n2371.737084\n-1423.552301\n0.0\n5.583333\n68.566667\n1.483633\n\n\n2000-01-01\n8710.352822\n474.863472\n980.253881\n1910.598641\n4348.877202\n5129.494603\n-3184.008033\n0.0\n5.666667\n68.500000\n1.485825\n\n\n2001-01-01\n9338.287124\n360.113309\n1037.070500\n1946.193764\n4320.894451\n2897.190669\n-502.948950\n0.0\n4.966667\n68.675000\n1.485517\n\n\n2002-01-01\n8826.496403\n209.515399\n888.679810\n2127.725707\n3820.134757\n3138.329395\n-938.857868\n0.0\n4.725000\n68.975000\n1.549017\n\n\n2003-01-01\n8889.845744\n112.157737\n701.917057\n1909.164731\n4925.835384\n3176.836868\n-1711.750559\n0.0\n5.341667\n68.975000\n1.570592\n\n\n2004-01-01\n9563.476337\n121.092191\n947.887219\n1864.098004\n5040.562703\n3907.027522\n-2075.006920\n0.0\n5.116667\n69.741667\n1.401167\n\n\n2005-01-01\n10252.960247\n95.080363\n1118.344428\n1793.108293\n5214.467798\n4829.545684\n-2607.425593\n1.0\n4.591667\n70.125000\n1.301575\n\n\n2006-01-01\n10491.526962\n77.006633\n1291.562414\n2729.974686\n5203.857546\n3943.455955\n-2600.317006\n1.0\n3.941667\n69.850000\n1.211483\n\n\n2007-01-01\n11093.119793\n71.131049\n1560.562037\n2749.181812\n4712.930591\n3359.113940\n-1217.537538\n1.0\n3.458333\n70.791667\n1.134375\n\n\n2008-01-01\n11481.223206\n65.508008\n1339.134853\n2742.517945\n3444.209491\n3752.538047\n268.330878\n0.0\n3.566667\n71.383333\n1.074183\n\n\n2009-01-01\n11192.177199\n111.838586\n1464.684956\n2426.866512\n5009.321141\n2085.188848\n317.954328\n0.0\n3.691667\n71.800000\n1.066767\n\n\n2010-01-01\n11408.370379\n141.808459\n1001.672465\n2292.670240\n4699.207866\n2532.122236\n1024.506031\n0.0\n6.575000\n69.416667\n1.141442\n\n\n2011-01-01\n11288.615235\n144.188676\n1062.777455\n2474.323912\n4526.773140\n3362.283433\n6.645971\n0.0\n6.516667\n68.133333\n1.030125\n\n\n2012-01-01\n11491.360391\n143.201492\n1330.207614\n2690.901483\n4642.860889\n2175.711738\n794.880159\n0.0\n5.433333\n69.508333\n0.989025\n\n\n2013-01-01\n11803.343431\n158.459375\n1473.940765\n2829.977012\n5288.245930\n2572.413384\n-202.774285\n0.0\n4.700000\n69.958333\n0.999408\n\n\n2014-01-01\n11006.721496\n182.199224\n1479.028999\n2817.708454\n4975.008515\n2283.359468\n-366.184716\n0.0\n4.658333\n69.591667\n1.029992\n\n\n2015-01-01\n10664.784806\n176.142114\n1040.727000\n2817.529567\n4492.864354\n691.915996\n1797.890004\n0.0\n4.775000\n69.058333\n1.104683\n\n\n2016-01-01\n11322.810385\n230.029051\n913.571647\n2608.854241\n4407.637789\n750.684901\n2872.090858\n0.0\n6.091667\n68.408333\n1.278808\n\n\n2017-01-01\n11433.853017\n319.856119\n813.921696\n2543.505301\n5094.564399\n1175.559759\n2126.157981\n0.0\n8.133333\n66.250000\n1.325583\n\n\n2018-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n7.883333\n66.316667\n1.297858\n\n\n\n\n\n\n\n\nmodel_df_first_diff(mdfl)\n\n\n\n\n\n\n\n\nnatural_resource_revenue\nnrri\nnrrd\ncorporate_income_tax\npersonal_income_tax\nother_revenue\ndebt_service\nprogram_expenditure\ndeficit\nur_lag\ner_lag\ncad_usd_lag\nheritage_dummy\nconstant\n\n\nbudget_dt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1970-01-01\n-162.934650\n0.000000\n-162.934650\n-40.793701\n171.208950\n292.603734\n20.225817\n348.912745\n109.054229\n0.100000\n0.791232\nNaN\n0.0\n1\n\n\n1971-01-01\n111.837554\n111.837554\n0.000000\n25.901302\n-4.135108\n111.188949\n29.780410\n248.061068\n33.048782\n1.800000\n-0.146706\nNaN\n0.0\n1\n\n\n1972-01-01\n134.264440\n134.264440\n0.000000\n90.999806\n91.499750\n-80.389972\n17.198975\n-5.345936\n-224.520986\n0.500000\n-1.229583\nNaN\n0.0\n1\n\n\n1973-01-01\n935.749273\n935.749273\n0.000000\n19.425579\n118.651006\n-380.873704\n-3.963247\n15.878875\n-681.036526\n0.000000\n1.013650\n-0.019092\n0.0\n1\n\n\n1974-01-01\n2389.215812\n2389.215812\n0.000000\n447.066481\n61.487767\n-34.446183\n11.641888\n1120.022088\n-1731.659902\n-0.400000\n1.372855\n0.009442\n0.0\n1\n\n\n1975-01-01\n-132.012493\n0.000000\n-132.012493\n-142.808507\n-113.041292\n368.955115\n-8.643749\n946.839909\n957.103338\n-1.800000\n1.997466\n-0.022100\n0.0\n1\n\n\n1976-01-01\n-4543.189130\n0.000000\n-4543.189130\n-360.039903\n112.387516\n108.640274\n-25.036197\n-225.855503\n4431.309543\n0.700000\n0.599003\n0.039133\n1.0\n1\n\n\n1977-01-01\n4638.656180\n4638.656180\n0.000000\n228.662485\n168.603139\n-110.168974\n-12.866176\n215.788591\n-4722.830415\n-0.308333\n21.360088\n-0.031192\n1.0\n1\n\n\n1978-01-01\n188.307315\n188.307315\n0.000000\n-9.260164\n18.423575\n-77.027853\n2.012553\n-236.067116\n-354.497436\n0.583333\n-0.166667\n0.077450\n1.0\n1\n\n\n1979-01-01\n479.977121\n479.977121\n0.000000\n-203.218170\n40.025870\n-117.904605\n-15.485732\n2477.905398\n2263.539451\n0.275000\n0.708333\n0.077242\n1.0\n1\n\n\n1980-01-01\n-383.891935\n0.000000\n-383.891935\n277.904203\n72.684398\n151.711140\n-1.397977\n634.752660\n514.946876\n-0.783333\n1.783333\n0.030792\n1.0\n1\n\n\n1981-01-01\n-795.958126\n0.000000\n-795.958126\n81.181763\n259.795177\n2417.616038\n78.509306\n503.808699\n-1380.316848\n-0.100000\n0.925000\n-0.002150\n1.0\n1\n\n\n1982-01-01\n-1113.869651\n0.000000\n-1113.869651\n-62.503803\n133.912455\n-286.318941\n-52.169623\n1840.418278\n3117.028595\n0.016667\n1.450000\n0.029483\n1.0\n1\n\n\n1983-01-01\n1114.041596\n1114.041596\n0.000000\n131.933427\n-242.707181\n5.102961\n110.880183\n-792.779158\n-1690.269779\n3.866667\n-3.225000\n0.034967\n1.0\n1\n\n\n1984-01-01\n304.752020\n304.752020\n0.000000\n22.658992\n-92.189386\n716.410620\n49.949560\n-168.903301\n-1070.585987\n3.258333\n-2.458333\n-0.001275\n1.0\n1\n\n\n1985-01-01\n-366.991657\n0.000000\n-366.991657\n-70.104285\n14.361613\n92.664162\n-49.714652\n1548.910801\n1829.266316\n0.400000\n0.116667\n0.062567\n1.0\n1\n\n\n1986-01-01\n-2451.586574\n0.000000\n-2451.586574\n-371.744807\n159.793281\n-681.904345\n95.031719\n-1006.031688\n2434.442476\n-1.633333\n1.558333\n0.070750\n1.0\n1\n\n\n1987-01-01\n743.692037\n743.692037\n0.000000\n153.866579\n329.611786\n539.538581\n222.273420\n-1071.281516\n-2615.717079\n0.233333\n-0.141667\n0.023867\n0.0\n1\n\n\n1988-01-01\n-518.164793\n0.000000\n-518.164793\n67.174276\n-224.515238\n338.084366\n168.910804\n-16.895010\n489.437183\n-0.483333\n0.100000\n-0.063592\n0.0\n1\n\n\n1989-01-01\n25.506344\n25.506344\n0.000000\n-29.373936\n293.129430\n-101.490177\n210.645362\n-29.534180\n-6.660479\n-1.541667\n1.475000\n-0.095233\n0.0\n1\n\n\n1990-01-01\n197.224034\n197.224034\n0.000000\n34.978568\n45.208628\n-27.139354\n57.830682\n-130.703028\n-323.144222\n-0.825000\n0.591667\n-0.046833\n0.0\n1\n\n\n1991-01-01\n-584.846863\n0.000000\n-584.846863\n-89.854509\n29.157325\n-391.046061\n-45.231681\n-553.837536\n437.520891\n-0.241667\n-0.016667\n-0.017092\n0.0\n1\n\n\n1992-01-01\n64.165615\n64.165615\n0.000000\n-75.508867\n-231.401242\n315.638496\n41.940598\n429.340791\n398.387388\n1.333333\n-0.908333\n-0.021042\n0.0\n1\n\n\n1993-01-01\n366.503452\n366.503452\n0.000000\n127.189967\n8.224851\n-10.051816\n126.182874\n-922.582134\n-1288.265714\n1.208333\n-1.208333\n0.062833\n0.0\n1\n\n\n1994-01-01\n298.845156\n298.845156\n0.000000\n120.739891\n66.827798\n-275.184458\n29.220498\n-1263.226429\n-1445.234319\n0.125000\n-0.508333\n0.081358\n0.0\n1\n\n\n1995-01-01\n-424.015919\n0.000000\n-424.015919\n131.202242\n2.596401\n-439.897302\n-74.640409\n-762.191041\n-106.716872\n-0.800000\n0.850000\n0.075725\n0.0\n1\n\n\n1996-01-01\n657.264596\n657.264596\n0.000000\n14.958707\n86.803365\n-433.673297\n-162.251681\n-255.745595\n-743.350647\n-0.941667\n0.925000\n0.006758\n0.0\n1\n\n\n1997-01-01\n-230.011377\n0.000000\n-230.011377\n212.881242\n162.512585\n96.104917\n-109.411649\n311.889310\n-39.009707\n-0.958333\n0.666667\n-0.008950\n0.0\n1\n\n\n1998-01-01\n-821.938330\n0.000000\n-821.938330\n-135.687474\n313.817676\n-184.948302\n5.789059\n49.200923\n883.746412\n-1.008333\n0.608333\n0.021167\n0.0\n1\n\n\n1999-01-01\n1109.397825\n1109.397825\n0.000000\n-244.269810\n148.547641\n273.965461\n-247.511966\n694.792990\n-840.360093\n-0.291667\n0.641667\n0.098767\n0.0\n1\n\n\n2000-01-01\n2757.757519\n2757.757519\n0.000000\n340.139894\n-690.661386\n-291.566666\n-12.745271\n367.958900\n-1760.455732\n0.083333\n-0.066667\n0.002192\n0.0\n1\n\n\n2001-01-01\n-2232.303933\n0.000000\n-2232.303933\n56.816619\n35.595123\n-27.982751\n-114.750162\n627.934302\n2681.059082\n-0.700000\n0.175000\n-0.000308\n0.0\n1\n\n\n2002-01-01\n241.138725\n241.138725\n0.000000\n-148.390689\n181.531943\n-500.759694\n-150.597911\n-511.790721\n-435.908918\n-0.241667\n0.300000\n0.063500\n0.0\n1\n\n\n2003-01-01\n38.507474\n38.507474\n0.000000\n-186.762753\n-218.560977\n1105.700627\n-97.357661\n63.349341\n-772.892691\n0.616667\n0.000000\n0.021575\n0.0\n1\n\n\n2004-01-01\n730.190654\n730.190654\n0.000000\n245.970162\n-45.066727\n114.727320\n8.934454\n673.630593\n-363.256361\n-0.225000\n0.766667\n-0.169425\n0.0\n1\n\n\n2005-01-01\n922.518162\n922.518162\n0.000000\n170.457209\n-70.989711\n173.905095\n-26.011829\n689.483910\n-532.418673\n-0.525000\n0.383333\n-0.099592\n1.0\n1\n\n\n2006-01-01\n-886.089729\n0.000000\n-886.089729\n173.217986\n936.866393\n-10.610252\n-18.073730\n238.566715\n7.108587\n-0.650000\n-0.275000\n-0.090092\n1.0\n1\n\n\n2007-01-01\n-584.342016\n0.000000\n-584.342016\n268.999623\n19.207126\n-490.926955\n-5.875584\n601.592831\n1382.779468\n-0.483333\n0.941667\n-0.077108\n1.0\n1\n\n\n2008-01-01\n393.424107\n393.424107\n0.000000\n-221.427184\n-6.663868\n-1268.721100\n-5.623041\n388.103413\n1485.868417\n0.108333\n0.591667\n-0.060192\n0.0\n1\n\n\n2009-01-01\n-1667.349199\n0.000000\n-1667.349199\n125.550103\n-315.651432\n1565.111650\n46.330578\n-289.046007\n49.623450\n0.125000\n0.416667\n-0.007417\n0.0\n1\n\n\n2010-01-01\n446.933388\n446.933388\n0.000000\n-463.012491\n-134.196272\n-310.113274\n29.969873\n216.193180\n706.551703\n2.883333\n-2.383333\n0.074675\n0.0\n1\n\n\n2011-01-01\n830.161197\n830.161197\n0.000000\n61.104990\n181.653671\n-172.434726\n2.380217\n-119.755144\n-1017.860060\n-0.058333\n-1.283333\n-0.111317\n0.0\n1\n\n\n2012-01-01\n-1186.571695\n0.000000\n-1186.571695\n267.430159\n216.577571\n116.087749\n-0.987183\n202.745156\n788.234188\n-1.083333\n1.375000\n-0.041100\n0.0\n1\n\n\n2013-01-01\n396.701645\n396.701645\n0.000000\n143.733151\n139.075529\n645.385041\n15.257883\n311.983040\n-997.654445\n-0.733333\n0.450000\n0.010383\n0.0\n1\n\n\n2014-01-01\n-289.053915\n0.000000\n-289.053915\n5.088233\n-12.268558\n-313.237415\n23.739849\n-796.621934\n-163.410431\n-0.041667\n-0.366667\n0.030583\n0.0\n1\n\n\n2015-01-01\n-1591.443473\n0.000000\n-1591.443473\n-438.301999\n-0.178887\n-482.144162\n-6.057110\n-341.936691\n2164.074720\n0.116667\n-0.533333\n0.074692\n0.0\n1\n\n\n2016-01-01\n58.768905\n58.768905\n0.000000\n-127.155353\n-208.675326\n-85.226565\n53.886937\n658.025579\n1074.200854\n1.316667\n-0.650000\n0.174125\n0.0\n1\n\n\n\n\n\n\n\n\nmdfl_err\n\n\n\n\n\n\n\n\nprogram_expenditure\ndebt_service\ncorporate_income_tax\npersonal_income_tax\nother_revenue\nnatural_resource_revenue\ndeficit\nheritage_dummy\nur_lag\ner_lag\ncad_usd_lag\n\n\nbudget_dt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1964-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1965-01-01\n2327.844291\n11.272854\n174.729232\n231.093501\n1442.925275\n1397.833860\n-907.464724\n0.0\n2.500000\n37.935748\nNaN\n\n\n1966-01-01\n2891.984579\n10.750872\n145.136773\n301.024417\n1542.750138\n1290.104645\n-376.280521\n0.0\n2.500000\n37.935748\nNaN\n\n\n1967-01-01\n4436.875339\n10.199713\n203.994268\n407.988537\n2351.033944\n1116.868620\n367.189683\n0.0\n2.500000\n37.935748\nNaN\n\n\n1968-01-01\n4368.535726\n19.160244\n239.503055\n469.425988\n2639.323668\n1360.377353\n-320.934094\n0.0\n2.700000\n38.322148\nNaN\n\n\n1969-01-01\n4396.849967\n17.909776\n286.556413\n599.977490\n2444.684401\n1141.748209\n-58.206771\n0.0\n3.300000\n39.041995\nNaN\n\n\n1970-01-01\n4745.762712\n38.135593\n245.762712\n771.186441\n2737.288136\n978.813559\n50.847458\n0.0\n3.400000\n39.833226\nNaN\n\n\n1971-01-01\n4993.823780\n67.916003\n271.664014\n767.051333\n2848.477084\n1090.651114\n83.896240\n0.0\n5.200000\n39.686520\nNaN\n\n\n1972-01-01\n4988.477844\n85.114978\n362.663820\n858.551083\n2768.087112\n1224.915554\n-140.624746\n0.0\n5.700000\n38.456938\n1.009883\n\n\n1973-01-01\n5004.356719\n81.151731\n382.089398\n977.202089\n2387.213408\n2160.664826\n-821.661272\n0.0\n5.700000\n39.470588\n0.990792\n\n\n1974-01-01\n6124.378806\n92.793618\n829.155879\n1038.689856\n2352.767225\n4549.880638\n-2553.321174\n0.0\n5.300000\n40.843443\n1.000233\n\n\n1975-01-01\n7071.218716\n84.149869\n686.347373\n925.648564\n2721.722340\n4417.868145\n-1596.217836\n0.0\n3.500000\n42.840909\n0.978133\n\n\n1976-01-01\n6845.363213\n59.113672\n326.307469\n1038.036080\n2830.362613\n4887.518397\n-2177.747675\n1.0\n4.200000\n43.439912\n1.017267\n\n\n1977-01-01\n7061.151803\n46.247496\n554.969954\n1206.639219\n2720.193639\n4513.335196\n-1887.738708\n1.0\n3.891667\n64.800000\n0.986075\n\n\n1978-01-01\n6825.084687\n48.260049\n545.709790\n1225.062794\n2643.165786\n4701.642511\n-2242.236144\n1.0\n4.475000\n64.633333\n1.063525\n\n\n1979-01-01\n9302.990086\n32.774318\n342.491620\n1265.088664\n2525.261181\n5181.619632\n21.303307\n1.0\n4.750000\n65.341667\n1.140767\n\n\n1980-01-01\n9937.742745\n31.376340\n620.395823\n1337.773062\n2676.972321\n4797.727697\n536.250183\n1.0\n3.966667\n67.125000\n1.171558\n\n\n1981-01-01\n10441.551444\n109.885646\n701.577586\n1597.568239\n5094.588359\n4001.769572\n-844.066666\n1.0\n3.866667\n68.050000\n1.169408\n\n\n1982-01-01\n12281.969722\n57.716023\n639.073783\n1731.480694\n4808.269418\n2887.899921\n2272.961929\n1.0\n3.883333\n69.500000\n1.198892\n\n\n1983-01-01\n11489.190564\n168.596206\n771.007210\n1488.773513\n4813.372379\n4001.941517\n582.692150\n1.0\n7.750000\n66.275000\n1.233858\n\n\n1984-01-01\n11320.287262\n218.545766\n793.666203\n1396.584127\n5529.782999\n4306.693537\n-487.893837\n1.0\n11.008333\n63.816667\n1.232583\n\n\n1985-01-01\n12869.198063\n168.831114\n723.561918\n1410.945740\n5622.447161\n3939.701880\n1341.372479\n1.0\n11.408333\n63.933333\n1.295150\n\n\n1986-01-01\n11863.166376\n263.862833\n351.817111\n1570.739021\n4940.542816\n1488.115306\n3775.814955\n1.0\n9.775000\n65.491667\n1.365900\n\n\n1987-01-01\n10791.884860\n486.136253\n505.683690\n1900.350807\n5480.081397\n2231.807343\n1160.097876\n0.0\n10.008333\n65.350000\n1.389767\n\n\n1988-01-01\n10774.989850\n655.047057\n572.857965\n1675.835569\n5818.165763\n1713.642551\n1649.535059\n0.0\n9.525000\n65.450000\n1.326175\n\n\n1989-01-01\n10745.455671\n865.692419\n543.484030\n1968.964999\n5716.675586\n1739.148895\n1642.874581\n0.0\n7.983333\n66.925000\n1.230942\n\n\n1990-01-01\n10614.752643\n923.523101\n578.462597\n2014.173627\n5689.536231\n1936.372929\n1319.730359\n0.0\n7.158333\n67.516667\n1.184108\n\n\n1991-01-01\n10060.915107\n878.291420\n488.608088\n2043.330951\n5298.490171\n1351.526066\n1757.251250\n0.0\n6.916667\n67.500000\n1.167017\n\n\n1992-01-01\n10490.255898\n920.232018\n413.099221\n1811.929709\n5614.128666\n1415.691681\n2155.638638\n0.0\n8.250000\n66.591667\n1.145975\n\n\n1993-01-01\n9567.673764\n1046.414892\n540.289188\n1820.154561\n5604.076850\n1782.195133\n867.372924\n0.0\n9.458333\n65.383333\n1.208808\n\n\n1994-01-01\n8304.447335\n1075.635389\n661.029079\n1886.982358\n5328.892392\n2081.040289\n-577.861395\n0.0\n9.583333\n64.875000\n1.290167\n\n\n1995-01-01\n7542.256294\n1000.994980\n792.231321\n1889.578759\n4888.995090\n1657.024370\n-684.578266\n0.0\n8.783333\n65.725000\n1.365892\n\n\n1996-01-01\n7286.510699\n838.743299\n807.190029\n1976.382124\n4455.321793\n2314.288966\n-1427.928914\n0.0\n7.841667\n66.650000\n1.372650\n\n\n1997-01-01\n7598.400009\n729.331650\n1020.071271\n2138.894710\n4551.426710\n2084.277589\n-1466.938621\n0.0\n6.883333\n67.316667\n1.363700\n\n\n1998-01-01\n7647.600932\n735.120709\n884.383797\n2452.712386\n4366.478407\n1262.339259\n-583.192208\n0.0\n5.875000\n67.925000\n1.384867\n\n\n1999-01-01\n8342.393922\n487.608742\n640.113987\n2601.260027\n4640.443868\n2371.737084\n-1423.552301\n0.0\n5.583333\n68.566667\n1.483633\n\n\n2000-01-01\n8710.352822\n474.863472\n980.253881\n1910.598641\n4348.877202\n5129.494603\n-3184.008033\n0.0\n5.666667\n68.500000\n1.485825\n\n\n2001-01-01\n9338.287124\n360.113309\n1037.070500\n1946.193764\n4320.894451\n2897.190669\n-502.948950\n0.0\n4.966667\n68.675000\n1.485517\n\n\n2002-01-01\n8826.496403\n209.515399\n888.679810\n2127.725707\n3820.134757\n3138.329395\n-938.857868\n0.0\n4.725000\n68.975000\n1.549017\n\n\n2003-01-01\n8889.845744\n112.157737\n701.917057\n1909.164731\n4925.835384\n3176.836868\n-1711.750559\n0.0\n5.341667\n68.975000\n1.570592\n\n\n2004-01-01\n9563.476337\n121.092191\n947.887219\n1864.098004\n5040.562703\n3907.027522\n-2075.006920\n0.0\n5.116667\n69.741667\n1.401167\n\n\n2005-01-01\n10252.960247\n95.080363\n1118.344428\n1793.108293\n5214.467798\n4829.545684\n-2607.425593\n1.0\n4.591667\n70.125000\n1.301575\n\n\n2006-01-01\n10491.526962\n77.006633\n1291.562414\n2729.974686\n5203.857546\n3943.455955\n-2600.317006\n1.0\n3.941667\n69.850000\n1.211483\n\n\n2007-01-01\n11093.119793\n71.131049\n1560.562037\n2749.181812\n4712.930591\n3359.113940\n-1217.537538\n1.0\n3.458333\n70.791667\n1.134375\n\n\n2008-01-01\n11481.223206\n65.508008\n1339.134853\n2742.517945\n3444.209491\n3752.538047\n268.330878\n0.0\n3.566667\n71.383333\n1.074183\n\n\n2009-01-01\n11192.177199\n111.838586\n1464.684956\n2426.866512\n5009.321141\n2085.188848\n317.954328\n0.0\n3.691667\n71.800000\n1.066767\n\n\n2010-01-01\n11408.370379\n141.808459\n1001.672465\n2292.670240\n4699.207866\n2532.122236\n1024.506031\n0.0\n6.575000\n69.416667\n1.141442\n\n\n2011-01-01\n11288.615235\n144.188676\n1062.777455\n2474.323912\n4526.773140\n3362.283433\n6.645971\n0.0\n6.516667\n68.133333\n1.030125\n\n\n2012-01-01\n11491.360391\n143.201492\n1330.207614\n2690.901483\n4642.860889\n2175.711738\n794.880159\n0.0\n5.433333\n69.508333\n0.989025\n\n\n2013-01-01\n11803.343431\n158.459375\n1473.940765\n2829.977012\n5288.245930\n2572.413384\n-202.774285\n0.0\n4.700000\n69.958333\n0.999408\n\n\n2014-01-01\n11006.721496\n182.199224\n1479.028999\n2817.708454\n4975.008515\n2283.359468\n-366.184716\n0.0\n4.658333\n69.591667\n1.029992\n\n\n2015-01-01\n10664.784806\n176.142114\n1040.727000\n2817.529567\n4492.864354\n691.915996\n1797.890004\n0.0\n4.775000\n69.058333\n1.104683\n\n\n2016-01-01\n11322.810385\n230.029051\n913.571647\n2608.854241\n4407.637789\n750.684901\n2872.090858\n0.0\n6.091667\n68.408333\n1.278808\n\n\n2017-01-01\n11433.853017\n319.856119\n813.921696\n2543.505301\n5094.564399\n1175.559759\n2126.157981\n0.0\n8.133333\n66.250000\n1.325583\n\n\n2018-01-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n7.883333\n66.316667\n1.297858\n\n\n\n\n\n\n\n\nmodel_df_first_diff(mdfl_err)\n\n\n\n\n\n\n\n\nnatural_resource_revenue\nnrri\nnrrd\ncorporate_income_tax\npersonal_income_tax\nother_revenue\ndebt_service\nprogram_expenditure\ndeficit\nur_lag\ner_lag\ncad_usd_lag\nheritage_dummy\nconstant\n\n\nbudget_dt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1970-01-01\n-162.934650\n0.000000\n-162.934650\n-40.793701\n171.208950\n292.603734\n20.225817\n348.912745\n109.054229\n0.100000\n0.791232\nNaN\n0.0\n1\n\n\n1971-01-01\n111.837554\n111.837554\n0.000000\n25.901302\n-4.135108\n111.188949\n29.780410\n248.061068\n33.048782\n1.800000\n-0.146706\nNaN\n0.0\n1\n\n\n1972-01-01\n134.264440\n134.264440\n0.000000\n90.999806\n91.499750\n-80.389972\n17.198975\n-5.345936\n-224.520986\n0.500000\n-1.229583\nNaN\n0.0\n1\n\n\n1973-01-01\n935.749273\n935.749273\n0.000000\n19.425579\n118.651006\n-380.873704\n-3.963247\n15.878875\n-681.036526\n0.000000\n1.013650\n-0.019092\n0.0\n1\n\n\n1974-01-01\n2389.215812\n2389.215812\n0.000000\n447.066481\n61.487767\n-34.446183\n11.641888\n1120.022088\n-1731.659902\n-0.400000\n1.372855\n0.009442\n0.0\n1\n\n\n1975-01-01\n-132.012493\n0.000000\n-132.012493\n-142.808507\n-113.041292\n368.955115\n-8.643749\n946.839909\n957.103338\n-1.800000\n1.997466\n-0.022100\n0.0\n1\n\n\n1976-01-01\n469.650252\n469.650252\n0.000000\n-360.039903\n112.387516\n108.640274\n-25.036197\n-225.855503\n-581.529839\n0.700000\n0.599003\n0.039133\n1.0\n1\n\n\n1977-01-01\n-374.183202\n0.000000\n-374.183202\n228.662485\n168.603139\n-110.168974\n-12.866176\n215.788591\n290.008967\n-0.308333\n21.360088\n-0.031192\n1.0\n1\n\n\n1978-01-01\n188.307315\n188.307315\n0.000000\n-9.260164\n18.423575\n-77.027853\n2.012553\n-236.067116\n-354.497436\n0.583333\n-0.166667\n0.077450\n1.0\n1\n\n\n1979-01-01\n479.977121\n479.977121\n0.000000\n-203.218170\n40.025870\n-117.904605\n-15.485732\n2477.905398\n2263.539451\n0.275000\n0.708333\n0.077242\n1.0\n1\n\n\n1980-01-01\n-383.891935\n0.000000\n-383.891935\n277.904203\n72.684398\n151.711140\n-1.397977\n634.752660\n514.946876\n-0.783333\n1.783333\n0.030792\n1.0\n1\n\n\n1981-01-01\n-795.958126\n0.000000\n-795.958126\n81.181763\n259.795177\n2417.616038\n78.509306\n503.808699\n-1380.316848\n-0.100000\n0.925000\n-0.002150\n1.0\n1\n\n\n1982-01-01\n-1113.869651\n0.000000\n-1113.869651\n-62.503803\n133.912455\n-286.318941\n-52.169623\n1840.418278\n3117.028595\n0.016667\n1.450000\n0.029483\n1.0\n1\n\n\n1983-01-01\n1114.041596\n1114.041596\n0.000000\n131.933427\n-242.707181\n5.102961\n110.880183\n-792.779158\n-1690.269779\n3.866667\n-3.225000\n0.034967\n1.0\n1\n\n\n1984-01-01\n304.752020\n304.752020\n0.000000\n22.658992\n-92.189386\n716.410620\n49.949560\n-168.903301\n-1070.585987\n3.258333\n-2.458333\n-0.001275\n1.0\n1\n\n\n1985-01-01\n-366.991657\n0.000000\n-366.991657\n-70.104285\n14.361613\n92.664162\n-49.714652\n1548.910801\n1829.266316\n0.400000\n0.116667\n0.062567\n1.0\n1\n\n\n1986-01-01\n-2451.586574\n0.000000\n-2451.586574\n-371.744807\n159.793281\n-681.904345\n95.031719\n-1006.031688\n2434.442476\n-1.633333\n1.558333\n0.070750\n1.0\n1\n\n\n1987-01-01\n743.692037\n743.692037\n0.000000\n153.866579\n329.611786\n539.538581\n222.273420\n-1071.281516\n-2615.717079\n0.233333\n-0.141667\n0.023867\n0.0\n1\n\n\n1988-01-01\n-518.164793\n0.000000\n-518.164793\n67.174276\n-224.515238\n338.084366\n168.910804\n-16.895010\n489.437183\n-0.483333\n0.100000\n-0.063592\n0.0\n1\n\n\n1989-01-01\n25.506344\n25.506344\n0.000000\n-29.373936\n293.129430\n-101.490177\n210.645362\n-29.534180\n-6.660479\n-1.541667\n1.475000\n-0.095233\n0.0\n1\n\n\n1990-01-01\n197.224034\n197.224034\n0.000000\n34.978568\n45.208628\n-27.139354\n57.830682\n-130.703028\n-323.144222\n-0.825000\n0.591667\n-0.046833\n0.0\n1\n\n\n1991-01-01\n-584.846863\n0.000000\n-584.846863\n-89.854509\n29.157325\n-391.046061\n-45.231681\n-553.837536\n437.520891\n-0.241667\n-0.016667\n-0.017092\n0.0\n1\n\n\n1992-01-01\n64.165615\n64.165615\n0.000000\n-75.508867\n-231.401242\n315.638496\n41.940598\n429.340791\n398.387388\n1.333333\n-0.908333\n-0.021042\n0.0\n1\n\n\n1993-01-01\n366.503452\n366.503452\n0.000000\n127.189967\n8.224851\n-10.051816\n126.182874\n-922.582134\n-1288.265714\n1.208333\n-1.208333\n0.062833\n0.0\n1\n\n\n1994-01-01\n298.845156\n298.845156\n0.000000\n120.739891\n66.827798\n-275.184458\n29.220498\n-1263.226429\n-1445.234319\n0.125000\n-0.508333\n0.081358\n0.0\n1\n\n\n1995-01-01\n-424.015919\n0.000000\n-424.015919\n131.202242\n2.596401\n-439.897302\n-74.640409\n-762.191041\n-106.716872\n-0.800000\n0.850000\n0.075725\n0.0\n1\n\n\n1996-01-01\n657.264596\n657.264596\n0.000000\n14.958707\n86.803365\n-433.673297\n-162.251681\n-255.745595\n-743.350647\n-0.941667\n0.925000\n0.006758\n0.0\n1\n\n\n1997-01-01\n-230.011377\n0.000000\n-230.011377\n212.881242\n162.512585\n96.104917\n-109.411649\n311.889310\n-39.009707\n-0.958333\n0.666667\n-0.008950\n0.0\n1\n\n\n1998-01-01\n-821.938330\n0.000000\n-821.938330\n-135.687474\n313.817676\n-184.948302\n5.789059\n49.200923\n883.746412\n-1.008333\n0.608333\n0.021167\n0.0\n1\n\n\n1999-01-01\n1109.397825\n1109.397825\n0.000000\n-244.269810\n148.547641\n273.965461\n-247.511966\n694.792990\n-840.360093\n-0.291667\n0.641667\n0.098767\n0.0\n1\n\n\n2000-01-01\n2757.757519\n2757.757519\n0.000000\n340.139894\n-690.661386\n-291.566666\n-12.745271\n367.958900\n-1760.455732\n0.083333\n-0.066667\n0.002192\n0.0\n1\n\n\n2001-01-01\n-2232.303933\n0.000000\n-2232.303933\n56.816619\n35.595123\n-27.982751\n-114.750162\n627.934302\n2681.059082\n-0.700000\n0.175000\n-0.000308\n0.0\n1\n\n\n2002-01-01\n241.138725\n241.138725\n0.000000\n-148.390689\n181.531943\n-500.759694\n-150.597911\n-511.790721\n-435.908918\n-0.241667\n0.300000\n0.063500\n0.0\n1\n\n\n2003-01-01\n38.507474\n38.507474\n0.000000\n-186.762753\n-218.560977\n1105.700627\n-97.357661\n63.349341\n-772.892691\n0.616667\n0.000000\n0.021575\n0.0\n1\n\n\n2004-01-01\n730.190654\n730.190654\n0.000000\n245.970162\n-45.066727\n114.727320\n8.934454\n673.630593\n-363.256361\n-0.225000\n0.766667\n-0.169425\n0.0\n1\n\n\n2005-01-01\n922.518162\n922.518162\n0.000000\n170.457209\n-70.989711\n173.905095\n-26.011829\n689.483910\n-532.418673\n-0.525000\n0.383333\n-0.099592\n1.0\n1\n\n\n2006-01-01\n-886.089729\n0.000000\n-886.089729\n173.217986\n936.866393\n-10.610252\n-18.073730\n238.566715\n7.108587\n-0.650000\n-0.275000\n-0.090092\n1.0\n1\n\n\n2007-01-01\n-584.342016\n0.000000\n-584.342016\n268.999623\n19.207126\n-490.926955\n-5.875584\n601.592831\n1382.779468\n-0.483333\n0.941667\n-0.077108\n1.0\n1\n\n\n2008-01-01\n393.424107\n393.424107\n0.000000\n-221.427184\n-6.663868\n-1268.721100\n-5.623041\n388.103413\n1485.868417\n0.108333\n0.591667\n-0.060192\n0.0\n1\n\n\n2009-01-01\n-1667.349199\n0.000000\n-1667.349199\n125.550103\n-315.651432\n1565.111650\n46.330578\n-289.046007\n49.623450\n0.125000\n0.416667\n-0.007417\n0.0\n1\n\n\n2010-01-01\n446.933388\n446.933388\n0.000000\n-463.012491\n-134.196272\n-310.113274\n29.969873\n216.193180\n706.551703\n2.883333\n-2.383333\n0.074675\n0.0\n1\n\n\n2011-01-01\n830.161197\n830.161197\n0.000000\n61.104990\n181.653671\n-172.434726\n2.380217\n-119.755144\n-1017.860060\n-0.058333\n-1.283333\n-0.111317\n0.0\n1\n\n\n2012-01-01\n-1186.571695\n0.000000\n-1186.571695\n267.430159\n216.577571\n116.087749\n-0.987183\n202.745156\n788.234188\n-1.083333\n1.375000\n-0.041100\n0.0\n1\n\n\n2013-01-01\n396.701645\n396.701645\n0.000000\n143.733151\n139.075529\n645.385041\n15.257883\n311.983040\n-997.654445\n-0.733333\n0.450000\n0.010383\n0.0\n1\n\n\n2014-01-01\n-289.053915\n0.000000\n-289.053915\n5.088233\n-12.268558\n-313.237415\n23.739849\n-796.621934\n-163.410431\n-0.041667\n-0.366667\n0.030583\n0.0\n1\n\n\n2015-01-01\n-1591.443473\n0.000000\n-1591.443473\n-438.301999\n-0.178887\n-482.144162\n-6.057110\n-341.936691\n2164.074720\n0.116667\n-0.533333\n0.074692\n0.0\n1\n\n\n2016-01-01\n58.768905\n58.768905\n0.000000\n-127.155353\n-208.675326\n-85.226565\n53.886937\n658.025579\n1074.200854\n1.316667\n-0.650000\n0.174125\n0.0\n1"
  },
  {
    "objectID": "posts/2023-11-19-terraform-workspaceconf.html",
    "href": "posts/2023-11-19-terraform-workspaceconf.html",
    "title": "Databricks workspace config in terraform",
    "section": "",
    "text": "Introduction\nThe Databricks Security Analysis Tool (SAT) is a pretty handy tool developed by databricks to scan your workspaces and produce alerts where your configuration deviates from best practices. To remediate some of these issues you need to modify your workspace configuration. Some things can be done from the UI, others I believe require calling the API. In either case, it would be preferable to automate these configurations as part of workspace provisioning. The databricks_workspace_conf resource in terraform can be used to accomplish this. Unfortunately, similar to the issue I documented trying to configure cluster policies, the docs are pretty limited and it’s difficult to figure out what the actual config changes should be. After some poking around and following a trail of forum posts to a random powershell script that happened to document the settings I wanted, I was able to create a config that worked. I’ve reproduced it below as a reference to both myself and anyone else interested in remediating SAT issues with terraform.\n\n\nThe code\nresource \"databricks_workspace_conf\" \"this\" {\n  custom_config = {\n    \"maxTokenLifetimeDays\" : \"180\"\n    \"enableTokensConfig\" : true\n    \"enableDeprecatedClusterNamedInitScripts\" : false\n    \"enableDeprecatedGlobalInitScripts\" : false\n    \"enforceUserIsolation\" : true\n    # set at account level, can't be done at workspace level\n    # DO NOT UNCOMMENT OR OTHERWISE ADD THIS, IT WILL BREAK YOUR STATE\n    # \"enableWebTerminal\" : true\n    \"enableNotebookTableClipboard\" : false\n    \"enableResultsDownloading\" : false\n  }\n}\n\n\nConclusion\nThat’s it, I just spent a lot of time figuring out how to make that little block of code so I wanted to share it. Put something like the above in your workspace provisioning script and you’ll address the SAT issues that are related to your workspace config."
  },
  {
    "objectID": "posts/2023-11-18-terraform-cluster.html",
    "href": "posts/2023-11-18-terraform-cluster.html",
    "title": "Databricks cluster policies in terraform",
    "section": "",
    "text": "Recently I had to define some databricks cluster policies at work using terraform. I didn’t have super sophisticated requirements (at least I didn’t think so), but I still struggled to find sample code online that covered my requirements. This post is a brief write up on what I implemented and why, as well as some notes on potential improvements I might make later as my requirements get more detailed."
  },
  {
    "objectID": "posts/2023-11-18-terraform-cluster.html#runtimes",
    "href": "posts/2023-11-18-terraform-cluster.html#runtimes",
    "title": "Databricks cluster policies in terraform",
    "section": "Runtimes",
    "text": "Runtimes\nOne of the first things we want our cluster policy to enforce is using a recent version of the Databricks Runtime (DBR). Depending on the environment we might further restrict this to LTS releases only. Using a series of data blocks I find all the relevant releases. Note that this will change as new releases come out, so we’ll want to schedule running this to ensure we’re always enforcing the latest runtimes. As an example, this block finds the latest LTS release that supports the ML runtime and has GPU drivers installed:\ndata \"databricks_spark_version\" \"latest_ml_gpu_lts\" {\n  latest            = true\n  long_term_support = true\n  ml                = true\n  gpu               = true\n}\nAnother bonus on enforcing runtime policies is it provides an easier way to restrict GPU compute without having to find a list of instance types with GPUs. Since you can’t provision a runtime onto a VM with a GPU unless it includes GPU drivers we can limit access to GPU easily with this.\nIn terms of which runtimes are enabled I made the assumption that we would want consistency across policies in terms of enabled runtimes. That is, the code does not allow for you to enable GPUs on single node compute policies but disable them on multi node.\nYou’ll see a bit further down that offering multiple runtime limitations across policies within a workspace would be fairly straightforward but introduce a lot of boilerplate code, at least the way I’ve implemented it. Again, I don’t really see this being a requirement. Specific runtimes are enabled or disabled with the module booleans lts_dbr, ml_dbr, and gpu_dbr. So if lts_dbr is true then only LTS runtimes are enabled, if it’s false users are allowed to choose LTS or the most recent runtime. It’s similar for ml_dbr for ML runtimes and gpu_dbr for ML runtimes with GPU enabled (there is no non-ML GPU enabled runtime)\nSetting the actual array of allowed runtimes feels kind of hacky, terraform doesn’t seem to support if else blocks, or other cleaner ways I could think of to do this:\n  no_lts_no_ml_no_gpu_arr = (!var.lts_dbr && !var.ml_dbr && !var.gpu_dbr) ? [data.databricks_spark_version.latest_lts.id, data.databricks_spark_version.latest.id] : null\n  lts_no_ml_no_gpu_arr    = (var.lts_dbr && !var.ml_dbr && !var.gpu_dbr) ? [data.databricks_spark_version.latest_lts.id] : null\n  lts_ml_no_gpu_arr       = (var.lts_dbr && var.ml_dbr && !var.gpu_dbr) ? [data.databricks_spark_version.latest_ml_lts.id, data.databricks_spark_version.latest_lts.id] : null\n  lts_ml_gpu_arr          = (var.lts_dbr && var.ml_dbr && var.gpu_dbr) ? [data.databricks_spark_version.latest_ml_lts.id, data.databricks_spark_version.latest_lts.id, data.databricks_spark_version.latest_ml_gpu_lts.id] : null\n  no_lts_ml_no_gpu_arr    = (!var.lts_dbr && var.ml_dbr && !var.gpu_dbr) ? [data.databricks_spark_version.latest_ml_lts.id, data.databricks_spark_version.latest_ml.id, data.databricks_spark_version.latest_lts.id, data.databricks_spark_version.latest.id, ] : null\n  no_lts_ml_gpu_arr       = (!var.lts_dbr && var.ml_dbr && !var.gpu_dbr) ? [data.databricks_spark_version.latest_ml_lts.id, data.databricks_spark_version.latest_ml.id, data.databricks_spark_version.latest_lts.id, data.databricks_spark_version.latest.id, data.databricks_spark_version.latest_ml_gpu_lts.id, data.databricks_spark_version.latest_ml_gpu.id] : null\n  fallback_spark_vers_arr = [data.databricks_spark_version.latest_lts.id]\n  runtime_version = {\n    \"spark_version\" : {\n      \"type\" : \"allowlist\",\n      \"values\" : coalesce(local.no_lts_no_ml_no_gpu_arr, local.lts_no_ml_no_gpu_arr, local.lts_ml_no_gpu_arr, local.no_lts_ml_no_gpu_arr, local.no_lts_ml_gpu_arr, local.fallback_spark_vers_arr),\n      \"defaultValue\" : data.databricks_spark_version.latest_lts.id\n    }\n  }\nbasically, whichever of those conditionals is true for the combination of runtime booleans that’s the list of runtimes that will be available to users of that policy. I put just the latest LTS runtime as a fallback just to handle errors, it shouldn’t really come up.\nThis is honestly more limiting than I’d strictly prefer for the non-LTS releases. As an example, if DBR 14.0 is the latest LTS runtime, and 14.2 is the latest overall runtime, I’d prefer users be able to provision 14.1 as well. To handle that though I think I’d have to do some array sorting and regex inference to find the position of the LTS release in the non-LTS array and return everything up to and including that index, and frankly I didn’t feel like writing that. Maybe I’ll be more motivated in the future."
  },
  {
    "objectID": "posts/2023-11-18-terraform-cluster.html#cost-management",
    "href": "posts/2023-11-18-terraform-cluster.html#cost-management",
    "title": "Databricks cluster policies in terraform",
    "section": "Cost management",
    "text": "Cost management\nThe next big thing we want to enforce is cost management. One approach would be setting careful limitations on combinations of instance types and number of workers, but databricks also offers a max_dbu parameter which just limits the compute cost. This doesn’t exactly translate to overall cost, as underlying VM costs are not factored in, but they tend to be very closely related to the DBU cost of the instance type, so the simplicity seemed like a worthwhile trade off. Again, I’m assuming that we don’t want to have too many different DBU limits within a given workspace, although I have allowed for interactive and job/DLT compute to have different thresholds. We probably generally want to limit the threshold for exploratory work below what we use to run scheduled jobs. Note that this does introduce a somewhat perverse incentive at the margins to run a larger instance with photon disabled, as enabling photon doubles your DBU cost for any given size of underlying compute.\nThis is accomplished by passing a line into the compute policy definition that looks something like this:\n    { \"dbus_per_hour\" : { \"type\" : \"range\", \"maxValue\" : var.max_dbu_job } },"
  },
  {
    "objectID": "posts/2023-11-18-terraform-cluster.html#single-of-multi-node",
    "href": "posts/2023-11-18-terraform-cluster.html#single-of-multi-node",
    "title": "Databricks cluster policies in terraform",
    "section": "Single of multi node",
    "text": "Single of multi node\nFor interactive clusters I’ve created both single node and multi node cluster policies. In theory we shouldn’t really care which a user selects, as long as they’re below their cost threshold, but for less sophisticated users it might reduce complexity to only allow single node clusters.\n  single_node = {\n    \"spark_conf.spark.databricks.cluster.profile\" : {\n      \"type\" : \"fixed\",\n      \"value\" : \"singleNode\",\n      \"hidden\" : true\n    },\n    \"num_workers\" : {\n      \"type\" : \"fixed\",\n      \"value\" : 0,\n      \"hidden\" : true\n    }\n  }\nThis can either be added to or left out of a policy definition to enforce single node"
  },
  {
    "objectID": "posts/2023-11-18-terraform-cluster.html#auto-termination",
    "href": "posts/2023-11-18-terraform-cluster.html#auto-termination",
    "title": "Databricks cluster policies in terraform",
    "section": "Auto termination",
    "text": "Auto termination\nFor all interactive policies (it’s not relevant to jobs or DLT) I enforce an auto termination of 10 minutes to minimize cluster idling. We could make that a variable if a need comes up, but I’d personally like to keep it low and consistent for now:\n  autotermination = {\n    \"autotermination_minutes\" : {\n      \"type\" : \"fixed\",\n      \"value\" : 10\n      # \"hidden\" : true\n  } }\nI took off the hidden flag for now so users can see that it’s been auto set for them. We can remove that later to reduce the complexity of the cluster creation interface.\nI have heard some feedback from ML users that it’s not reasonable to expect them to be sitting around ready to pounce on long running tasks when they’re prototyping so I’m going to end up modifying this to a range with a higher maximum value that we can configure for ML workspaces."
  },
  {
    "objectID": "posts/2023-11-18-terraform-cluster.html#tags",
    "href": "posts/2023-11-18-terraform-cluster.html#tags",
    "title": "Databricks cluster policies in terraform",
    "section": "Tags",
    "text": "Tags\nFinally, I added some tags, which right now don’t really do much since I don’t know what additional tags we want to add. A lot gets auto applied that might be sufficient, but I wanted to demonstrate the capability:\n  default_tags = {\n    \"custom_tags.lob\" : {\n      \"type\" : \"fixed\",\n      \"value\" : \"${var.lob_name}\",\n      \"hidden\" : true\n    },\n    \"custom_tags.TEST\" : {\n      \"type\" : \"fixed\",\n      \"value\" : \"testfromterraform\"\n    }\n  }"
  },
  {
    "objectID": "posts/2023-11-18-terraform-cluster.html#actual-cluster-policies",
    "href": "posts/2023-11-18-terraform-cluster.html#actual-cluster-policies",
    "title": "Databricks cluster policies in terraform",
    "section": "Actual cluster policies",
    "text": "Actual cluster policies\nPutting it all together we can define cluster policies like so:\nresource \"databricks_cluster_policy\" \"multi-node-personal\" {\n  count = var.create_multi_node_personal_policy ? 1 : 0\n  name  = \"Multi Node Personal Compute\"\n  definition = jsonencode(merge(\n    { \"dbus_per_hour\" : { \"type\" : \"range\", \"maxValue\" : var.max_dbu_interactive } },\n    local.runtime_version,\n    local.autotermination,\n    local.default_tags,\n    local.photon\n  ))\n}"
  },
  {
    "objectID": "posts/2022-12-31-proxmox2.html",
    "href": "posts/2022-12-31-proxmox2.html",
    "title": "Home cluster part 2 - Configuring Proxmox",
    "section": "",
    "text": "In part 1 of this series I laid out the rationale for building a homelab cluster, walked through my hardware selection, and ran through the proxmox installer. In this post I’ll document the further configuration of the cluster and its nodes. The actual implementation will be done in ansible and tracked in my recipes repository, so I will keep it light on code in this post. What I’m going to try and document here is the steps I took, why I took them, and any weird edge cases or other fun learning opportunities I encounter in the process."
  },
  {
    "objectID": "posts/2022-12-31-proxmox2.html#failed-first-attempt",
    "href": "posts/2022-12-31-proxmox2.html#failed-first-attempt",
    "title": "Home cluster part 2 - Configuring Proxmox",
    "section": "Failed first attempt",
    "text": "Failed first attempt\nI made a good amount of progress trying to set this up a certain way, but eventually hit a wall. Some of what I tried originally is still interesting, it just didn’t work. The core issue is that I was dynamically identifying which host had the UPS attached to it based on the output of a locally run command on each host. This worked fine in terms of setting conditionals to only do server setup stuff on the node with the UPS attached, but ran into troubles configuring the clients. The clients need to know the hostname of the server, but I couldn’t figure out any way to dynamically identify that host in an ansible playbook. Registered variables from commands (what I was using to identify which host was connected to the UPS) are host variables only, so the other hosts didn’t have access to it. From the look of it you can’t really make a host variable a global variable based on a condition. There might be a way to concatenate all host variables in a way that would let me get &lt;server host&gt; + '' + '' == &lt;server host&gt; as the output for all hosts, but that felt pretty hacky. Based on this I’m just going to hard code which host is directly connected to the UPS and build my playbook from that.\n\nBorked attempt write up\nOne node in my cluster is connected via USB to my UPS. In the event of a power failure I want it to be alerted via that USB connection, and then pass that alert on to the other nodes via NUT. I’m largely relying on the Arch wiki to set this up, even though proxmox is Debian based, just because that wiki is amazing. I also found this repository which has a role configured for setting up nut. It’s set up in a different way than I want, and also has a lot more abstraction that’s good for portability but bad for interpretability, so I won’t use it directly.\nThe first thing I want to do is install the nut utility on all the systems, as with the previous section this is easily accomplished with the apt module in ansible.\nNext I need to identify which system has the UPS USB cable connected to it, as this one will be the NUT server, and the others will be NUT clients. Realistically this is not going to change and I could just hard code it, but I thought it would be fun to figure out how to automate it.\nThe nut package comes with a nut-scanner utility which can be used to identify compatible devices. I can register the output of that command in ansible and then set a conditional to only perform certain operations if the output of the command listed a USB device. To test this before I actually applied anything with conditionals I used the debug module to output which host had the UPS attached. I won’t keep that debug in my final playbook, so I’ll reproduce that part here:\n- name: Check if USB for UPS is connected to this host\n  ansible.builtin.shell: nut-scanner -U -P\n  register: ups_driver\n\n- name: Show me which host has the UPS connected\n  ansible.builtin.debug:\n    msg: System {{ inventory_hostname }} has UPS driver {{ ups_driver }}\n  when: ups_driver.stdout.find('usbhid-ups') != -1\nNext is to configure the driver on connected system. To do this I copy over a ups.conf file based on the output of the nut-scanner command. After copying over the template I test it by sshing into the machine and running upsdrvctl start. Since that looked good I enable the nut-driver service with ansible’s systemd module.\nAfter that it’s time to set up the nut server for clients (both the local machine and the other nodes in the cluster) to connect to. Following the Arch wiki I created a upsd.users file with user configuration for the clients and then tried to enable and start the nut server. I didn’t get an error from ansible for this, but when I tried to check the server I got nothing, and checking the state of the service I saw that it was dead. The relevant lines in the service status seemed to be:\nupsd disabled, please adjust the configuration to your needs\nThen set MODE to a suitable value in /etc/nut/nut.conf to enable it\nTaking a look at that file I see this:\n##############################################################################\n# The MODE determines which part of the NUT is to be started, and which\n# configuration files must be modified.\n#\n# This file try to standardize the various files being found in the field, like\n# /etc/default/nut on Debian based systems, /etc/sysconfig/ups on RedHat based\n# systems, ... Distribution's init script should source this file to see which\n# component(s) has to be started.\n#\n# The values of MODE can be:\n# - none: NUT is not configured, or use the Integrated Power Management, or use\n#   some external system to startup NUT components. So nothing is to be started.\n# - standalone: This mode address a local only configuration, with 1 UPS\n#   protecting the local system. This implies to start the 3 NUT layers (driver,\n#   upsd and upsmon) and the matching configuration files. This mode can also\n#   address UPS redundancy.\n# - netserver: same as for the standalone configuration, but also need\n#   some more network access controls (firewall, tcp-wrappers) and possibly a\n#   specific LISTEN directive in upsd.conf.\n#   Since this MODE is opened to the network, a special care should be applied\n#   to security concerns.\n# - netclient: this mode only requires upsmon.\n#\n# IMPORTANT NOTE:\n#  This file is intended to be sourced by shell scripts.\n#  You MUST NOT use spaces around the equal sign!\n\nMODE=none\nSo based on this I think I need ansible to remove the MODE=none line and change it to MODE=netserver on the server. Probably it will have to be MODE=netclient on the clients, but let’s leave that alone for now. I can handle this using the lineinfile module. After doing this and restarting the nut-server service I ran upsc pveups and had the state of the UPS returned, indicating the config was good for the directly connected node. This is where I got stuck, see the write up above"
  },
  {
    "objectID": "posts/2022-12-31-proxmox2.html#working-second-attempt",
    "href": "posts/2022-12-31-proxmox2.html#working-second-attempt",
    "title": "Home cluster part 2 - Configuring Proxmox",
    "section": "Working second attempt",
    "text": "Working second attempt\nDon’t reinvent the wheel folks. I vendored in this role and got everything working pretty much right away. I did have to hard code which host was attached to the UPS, but that’s a small price to pay. Problem solved!"
  },
  {
    "objectID": "posts/2022-11-21-proxmox.html",
    "href": "posts/2022-11-21-proxmox.html",
    "title": "Home cluster part 1 - Intro and Proxmox install",
    "section": "",
    "text": "I’ve been running services on my home network for years. It started with running things bare metal on the same machine I used as my desktop for day to day work. That was a nice easy way to get started, but I was constantly running into conflicting updates, or accidentally breaking something when I tried to get some desktop service working. The next step was getting a dedicated machine solely for hosting services. This worked a lot better since my service requirements changed a lot less frequently than my desktop requirements, but I still ran into conflicting services, or breaking one service when I was testing something out on another. The next step was a dedicated machine, but running all my services in docker containers. That really helped with isolation and was also where I got serious about automating my environment with ansible, which generally meant that even a complete system wipeout only took me down for as long as it took to reinstall the base OS and re-run my ansible playbook.\nNow it’s time for the next step in making my home server environment fancier - Proxmox."
  },
  {
    "objectID": "posts/2022-11-21-proxmox.html#what-i-actually-bought",
    "href": "posts/2022-11-21-proxmox.html#what-i-actually-bought",
    "title": "Home cluster part 1 - Intro and Proxmox install",
    "section": "What I actually bought",
    "text": "What I actually bought\n\nHP ProDesk 400 G3\n\ni5-7500T\n32GB RAM\n512 GB nvme SSD\n1TB WD Blue SATA SSD (purchased new and installed after)\n348.75 CAD total price - 252.75 CAD for the system 96 CAD for the SSD upgrade.\n\nHP EliteDesk 800 G3\n\ni5-7500T\n32GB RAM (purchased new and installed after, shipped with 4GB)\n512 GB nvme SSD (purchased new, came with 240 GB that I repurposed for a portable drive)\n1TB WD Blue SATA SSD (purchased new and installed after)\n456.63 CAD total price - 141.34 for the system, 125.15 for the RAM, 96 for the SSD, 94.14 for the nvme SSD\n\nDell 3060 Micro\n\ni5-8500T\n32GB RAM (purchased new and installed after, shipped with 8GB)\n512 GB nvme SSD (purchased new, came with 240 GB that I repurposed for a portable drive)\n1TB WD Blue SATA SSD (purchased new and installed after)\n503.53 CAD total price - 180 for the system, 133.39 for the RAM, 96 for the SSD, 94.14 for the nvme SSD\n\n\nAs you can see the price varied between the nodes. I got lucky with the HP ProDesk because it was in Canada and came equipped with the RAM and nvme I wanted. Making those upgrades after on the other systems and ordering from the US increased the price. With further patience and luck maybe I could have saved a couple hundred bucks, but honestly I’d already been waiting a long time to get this project kicked off and I kind of think that ProDesk was a bit of a unicorn."
  },
  {
    "objectID": "posts/2021-01-09-traefik-lan.html",
    "href": "posts/2021-01-09-traefik-lan.html",
    "title": "Using traefik for internal services",
    "section": "",
    "text": "Introduction\nI have a small server at home that I use to run various services in docker containers. I don’t expose any of them to the internet, I can tunnel in through my vpn running on my pfsense router. Because of that up until recently it was enough to just publish the ports of my various services and make a bookmark pointing to my server at that port. Recently I wanted to set up a couple services that both expected to be exposed on port 80, the default http port. Just remapping the port would be insufficient, because their clients did not have options to specify a port to connect on. To get around this issue, I decided to suck it up and learn to use a reverse proxy. I looked into both nginx and traefik and settled on traefik. Unfortunately for me, pretty much all the tutorials online expect you to be exposing traefik to the web and have all sorts of stuff about TLS and letsencrypt that don’t matter to me, and don’t really explain how to do the routing if you only want it to work on your internal network. This guide is for me in the future if I ever have to set this up again, and anyone else that’s interested in a similar setup.\nThis guide isn’t going to cover what a reverse proxy is, or many of the details of traefik, there are lots of good guides for that online. It’s specifically going to cover the configuration specific to a LAN only connection.\n\n\nThe key configuration\nMy pfsense box manages DNS and allows me to resolve machines on my network by their name. For instance, my server is named mars and if I run ping mars it will correctly resolve to that machine’s IP. Traefik doesn’t like to play nice with that naming convetion though, at least I couldn’t get it to work with all sorts of combinations of mars and mars.localdomain. Fortunately, I found a forum post that addressed this issue. In my DNS settings I picked a nice sounding domain that I was sure I wouldn’t actually want to connect to (in my case I used ian.ca) and set my DNS to redirect any requests to that domain to go to my server. To do this, from pfsense I went to services -&gt; DNS resolver, and added two lines to the “custom options” section near the bottom\nlocal-zone: \"ian.ca\" redirect\nlocal-data: \"ian.ca A &lt;my server IP&gt;\"\n\n\nSetting up Traefik\nMost of the rest of the traefik setup could be borrowed from other basic guides. I configure traefik and all my other docker containers in an ansible role. You can find that on my GitHub. A couple small gotchas that I ran into were making sure Traefik was on the same docker network as the containers I wanted it to route, and to remember the difference between exposing ports (available within the docker network) and publishing ports (mapping them to a port on the host).\n\n\nConclusion\nThis was one of those setups that is straightforward in retrospect, but I had to spend a lot of time googling and banging my head against the wall before I could get the correct combination of configurations to do what I want. Hopefully this is useful to others, or at least me next time I try and do this."
  },
  {
    "objectID": "posts/2023-06-04-ansible-ad-users.html",
    "href": "posts/2023-06-04-ansible-ad-users.html",
    "title": "Finding all AD group users with ansible",
    "section": "",
    "text": "Introduction\nThis is a write up summarizing the process I went through to retrieve information about members of Active Directory groups from a Linux VM using ansible. My specific intent was to use it as part of a playbook to configure rootless docker, but it would be applicable in any other situation where you need to get the members of a number of AD groups. The hardest part of it by far is getting the output of earlier tasks into a format that’s suitable for later steps. I’ve got a reasonable clean approach documented below, after trying some extremely ugly alternate approaches earlier. I’m sure there’s some even fancier way to do this that will make my approach look silly, and if you know it I’d love for you to fill me in.\n\n\nPre-requisites\nIn order to do this I need the user ansible is running as to be authenticated against Active Directory. I don’t have elevated privileges on the AD I tested this on, so I think any normal user account should be sufficient. For this example I have my username and password stored as variables ad_user and ad_password.\nI’ve also got a host variable configured for each host I’m doing this on that maps to a list of AD groups I want members of for that host, called domain_groups in this playbook.\nHaving set this up I need to make sure the host machine has a pre-requisite module available, and that the user I’m running as has a kerberos ticket issued for my user:\n- name: Install ldap pre-requisites\n  become: true\n  ansible.builtin.apt:\n    pkg:\n      - python3-ldap\n\n- name: Issue a kerberos ticket to authenticate to AD\n  ansible.builtin.shell: |\n    echo \"{{ ad_password }}\" | kinit -l 1h {{ ad_user }}@example.com\n  changed_when: false\nI’ve changed the actual domain to example.com and you’ll need to modify that to your domain of course.\n\n\nGet the users\n- name: Return all users in the groups associated with the machine using LDAP search\n  community.general.ldap_search:\n    dn: \"cn={{ item }},cn=Users,dc=EXAMPLE,dc=COM\"\n    sasl_class: \"gssapi\"\n    server_uri: \"ldap://example.com\"\n    attrs:\n      - member\n  register: intermediate_calc_group_members\n  with_items: \"{{ domain_groups }}\"\nThis first part does the actual data retrieval, everything that follows is just cleanup. For reference, the JSON I get out of this looks something like:\n{\n            \"ansible_loop_var\": \"item\",\n            \"changed\": false,\n            \"failed\": false,\n            \"invocation\": {\n                \"module_args\": {\n                    \"attrs\": [\n                        \"member\"\n                    ],\n                    \"bind_dn\": null,\n                    \"bind_pw\": \"\",\n                    \"dn\": \"cn=group1,cn=Users,dc=EXAMPLE,dc=COM\",\n                    \"filter\": \"(objectClass=*)\",\n                    \"referrals_chasing\": \"anonymous\",\n                    \"sasl_class\": \"gssapi\",\n                    \"schema\": false,\n                    \"scope\": \"base\",\n                    \"server_uri\": \"ldap://example.com\",\n                    \"start_tls\": false,\n                    \"validate_certs\": true\n                }\n            },\n            \"item\": \"group1\",\n            \"results\": [\n                {\n                    \"dn\": \"cn=group1,cn=Users,dc=EXAMPLE,dc=COM\",\n                    \"member\": [\n                        \"CN=example_user,OU=Synced to Azure,OU=Example Client,OU=Example Corporate,dc=EXAMPLE,dc=COM\",\n                        \"CN=example_user,OU=Synced to Azure,OU=Example Client,OU=Example Corporate,dc=EXAMPLE,dc=COM\",\n                        \"CN=example_user,OU=Synced to Azure,OU=Example Client,OU=Example Corporate,dc=EXAMPLE,dc=COM\",\n                        \"CN=example_user,OU=Synced to Azure,OU=Example Client,OU=Example Corporate,dc=EXAMPLE,dc=COM\",\n                        \"CN=example_user,OU=Synced to Azure,OU=Example Client,OU=Example Corporate,dc=EXAMPLE,dc=COM\",\n                        \"CN=example_user,OU=Synced to Azure,OU=Example Client,OU=Example Corporate,dc=EXAMPLE,dc=COM\",\n                        \"CN=example_user,OU=Synced to Azure,OU=Example Client,OU=Example Corporate,dc=EXAMPLE,dc=COM\",\n                        \"CN=example_user,OU=Synced to Azure,OU=Example Client,OU=Example Corporate,dc=EXAMPLE,dc=COM\"\n                    ]\n                }\n            ]\n        },\n        {\n            \"ansible_loop_var\": \"item\",\n            \"changed\": false,\n            \"failed\": false,\n            \"invocation\": {\n                \"module_args\": {\n                    \"attrs\": [\n                        \"member\"\n                    ],\n                    \"bind_dn\": null,\n                    \"bind_pw\": \"\",\n                    \"dn\": \"cn=group2,cn=Users,dc=EXAMPLE,dc=COM\",\n                    \"filter\": \"(objectClass=*)\",\n                    \"referrals_chasing\": \"anonymous\",\n                    \"sasl_class\": \"gssapi\",\n                    \"schema\": false,\n                    \"scope\": \"base\",\n                    \"server_uri\": \"ldap://example.com\",\n                    \"start_tls\": false,\n                    \"validate_certs\": true\n                }\n            },\n            \"item\": \"group2\",\n            \"results\": [\n                {\n                    \"dn\": \"cn=group2,cn=Users,dc=EXAMPLE,dc=COM\",\n                    \"member\": [\n                        \"CN=example_user,OU=Example Clients,OU=Example Corporate,dc=EXAMPLE,dc=COM\",\n                        \"CN=example_user,OU=Example Clients,OU=Example Corporate,dc=EXAMPLE,dc=COM\",\n                        \"CN=example_user,OU=Example Clients,OU=Example Corporate,dc=EXAMPLE,dc=COM\",\n                        \"CN=example_user,OU=Example Clients,OU=Example Corporate,dc=EXAMPLE,dc=COM\",\n                        \"CN=example_user,OU=Example Clients,OU=Example Corporate,dc=EXAMPLE,dc=COM\",\n                        \"CN=example_user,CN=Users,dc=EXAMPLE,dc=COM\",\n                        \"CN=example_user,OU=Example Clients,OU=Example Corporate,dc=EXAMPLE,dc=COM\",\n                        \"CN=example_user,OU=Example Clients,OU=Example Corporate,dc=EXAMPLE,dc=COM\"\n                    ]\n                }\n            ]\n        }\nIn the example above I’ve replaced all the actual user names with example_user but you can see that the information I want to assemble (the usernames and which group each of them is in) is surrounded by a lot of extraneous data and text.\n- name: Get group and member list\n  set_fact:\n    intermediate_calc_users: &gt;-\n      {%- set result = [] -%}\n      {%- for play_dict in intermediate_calc_group_members.results -%}\n        {%- for user in play_dict['results'][0]['member'] -%}\n          {%- set clean_user = user | regex_search('^CN=(\\\\w\\\\d+),.+', '\\\\1') | first | lower -%}\n          {{\n            result.append({'group': play_dict['item'], 'id': clean_user, 'user': clean_user + \"@EXAMPLE.COM\"})\n          }}\n        {%- endfor -%}\n      {%- endfor -%}\n      {{ result | to_json | from_json }}\nSome parts of this are witchcraft to me. I don’t really know why I have to pipe my result to json and then back from json. It’s doing something to clean up my variables in such a way that subsequent steps can understand it, but as for why I’m not really sure. I got a lot of the structure of this variable construction from this post.\nThe regex I’m using in this particular case is based on the fact that all the user IDs I’m working with are in the format of one letter followed by several numbers. If your user IDs are more heterogeneous you’ll have to mess with the regext to get just the username out of that part of the output that looks something like:\n\"CN=example_user,OU=Example Clients,OU=Example Corporate,dc=EXAMPLE,dc=COM\"\nAt the end of this step I have a list of dictionaries with one entry per user with keys for their AD group, just their username, and their username with the domain appended.\nNote that this step will fail if you pull an AD group that only has one member, because the member item in the dictionary will go from being a list to a string. I didn’t specifically have to deal with that in my use case, but it would be more robust to do something like putting play_dict['results'][0]['member'] in a list and then flattening that list so you always got a list back.\n- name: Register getent results so I can retrieve UIDs\n  ansible.builtin.getent:\n    database: passwd\n    key: \"{{ item.user }}\"\n  with_items: \"{{ intermediate_calc_users }}\"\n  register: intermediate_calc_getent\nFor my particular use case in addition to the usernames I also needed the UIDs of each user, so in this step I use ansible’s built in getent module and the dictionary I created above in the last step to return the entry in /etc/passwd for each user, which will include their UID.\n- name: Cleanup getent results\n  set_fact:\n    intermediate_calc_getent_clean: &gt;-\n      {%- set result = [] -%}\n      {%- for play_dict in intermediate_calc_getent.results -%}\n        {%- set getent_passwd = play_dict['ansible_facts']['getent_passwd'] -%}\n        {%- set key = getent_passwd.keys() | first -%}\n        {{ result.append({'key': key,'value': getent_passwd[key][1]}) }}\n      {%- endfor -%}\n      {{ result | items2dict | to_json | from_json }}\nAgain, this step is a bit of black magic, just messing around with the output of the last step (ansible’s debug is your friend for this) and fiddling with it until I have a list of dictionaries with one item where the key is the username and the value is their UID.\n- name: Create final list of dicts for all users\n  set_fact:\n    users_dict: &gt;-\n      {%- set result = [] -%}\n      {%- for user in intermediate_calc_users -%}\n        {{ result.append({'group': user['group'], 'user': user['user'], 'user': user['user'], 'uid': intermediate_calc_getent_clean[user['user']]})}}\n      {%- endfor -%}\n      {{ result | to_json | from_json }}\nNow all I have to do is combine those two dictionaries together into one. This part is pretty self explanatory except for that to_json | from_json bit at the bottom.\n\n\nConclusion\nIf you work with Linux systems where users are managed by AD (or probably other LDAP providers, but I’m using AD in this example) then this is a handy trick to get a fact in your playbook with basic information about those users."
  },
  {
    "objectID": "posts/2022-08-01-dunning.html",
    "href": "posts/2022-08-01-dunning.html",
    "title": "Dunning-Kruger is autocorrelation",
    "section": "",
    "text": "A while back I read this interesting post that explained how the Dunning-Kruger effect is an example of autocorrelation. Before proceeding with the rest of this post, I’d highly recommend reading that one. I liked their example of using simulated data to show that the supposed effect can be observed even in completely uncorrelated variables. It got me wondering what it would look like if we had other relationships between predicted and actual performance, and so this post is an opportunity to explore that."
  },
  {
    "objectID": "posts/2022-08-01-dunning.html#introduction",
    "href": "posts/2022-08-01-dunning.html#introduction",
    "title": "Dunning-Kruger is autocorrelation",
    "section": "",
    "text": "A while back I read this interesting post that explained how the Dunning-Kruger effect is an example of autocorrelation. Before proceeding with the rest of this post, I’d highly recommend reading that one. I liked their example of using simulated data to show that the supposed effect can be observed even in completely uncorrelated variables. It got me wondering what it would look like if we had other relationships between predicted and actual performance, and so this post is an opportunity to explore that."
  },
  {
    "objectID": "posts/2022-08-01-dunning.html#setup-all-the-code-is-here",
    "href": "posts/2022-08-01-dunning.html#setup-all-the-code-is-here",
    "title": "Dunning-Kruger is autocorrelation",
    "section": "Setup (all the code is here)",
    "text": "Setup (all the code is here)\nIf you don’t want to read code and just want to see the charts feel free to skip to the next section.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.style.use(\"dark_background\")\n\n\nN_SAMPLES = 1_000\n\n\nclass DunningKruger:\n    def __init__(self, actual_dist: np.ndarray, predicted_dist: np.ndarray) -&gt; None:\n        \"\"\"Build dataframe of actual percentile vs predicted.\"\"\"\n        self.df = (\n            pd.DataFrame({\"Actual Distribution\": actual_dist, \"Predicted Distribution\":predicted_dist})\n            # Handle prediction errors that produce impossible percentiles\n            .clip(lower=0.0, upper=100.0)\n            .assign(predict_error=lambda df: df[\"Predicted Distribution\"] - df[\"Actual Distribution\"])\n            .rename(columns={\"predict_error\": \"Prediction Error\"})\n        )\n    \n    @property\n    def x_vs_y_chart(self) -&gt; sns.axisgrid.FacetGrid:\n        \"\"\"Original style chart of actual vs predicted.\"\"\"\n        g = sns.lmplot(x=\"Actual Distribution\", y=\"Predicted Distribution\", data=self.df, scatter_kws={\"alpha\": 0.25})\n        g.ax.axline((0,0), (100,100), linewidth=2, color=\"white\", linestyle=\"--\", alpha=0.5)\n        plt.show()\n\n    @property\n    def x_vs_error_chart(self) -&gt; sns.axisgrid.FacetGrid:\n        \"\"\"Actual vs prediction error (implicit reference in DK paper).\"\"\"\n        g = sns.lmplot(x=\"Actual Distribution\", y=\"Prediction Error\", data=self.df, scatter_kws={\"alpha\": 0.25});\n        g.ax.axline((0,0), (100,0), linewidth=2, color=\"white\", linestyle=\"--\", alpha=0.5)\n        plt.show()"
  },
  {
    "objectID": "posts/2022-08-01-dunning.html#scenarios",
    "href": "posts/2022-08-01-dunning.html#scenarios",
    "title": "Dunning-Kruger is autocorrelation",
    "section": "Scenarios",
    "text": "Scenarios\nIn the following sections I’ll simulate some actual vs predicted distributions and see what they look like when you compare them to the Dunning Kruger graph. I’m not going to plot quantile vs percentile as in the original chart, as I think that just obscures what’s actually going on. The percentile vs percentile view shows the same overall pattern if you fit a regression line through it, which is easy enough to do.\n\nPurely random\nThis was the example in the original blog post that convinced me of the argument. I’ll start by reproducing it here. In this example predicted percentile performance is completely random and independent of actual percentile.\n\nactual_dist = np.random.uniform(low=0.0, high=100.0, size=N_SAMPLES)\npredicted_dist = np.random.uniform(low=0.0, high=100.0, size=N_SAMPLES)\n\n\ndk = DunningKruger(actual_dist, predicted_dist)\n\n\ndk.x_vs_y_chart\n\n\n\n\nThe basic format of this x_vs_y_chart will be the same in all scenarios, so I’ll just explain how to interpret it in this first example. The x axis represents someone’s actual place in the percentile distribution, the y axis is their predicted place in that same distribution. Each point in the scatter plot represents a single observation of someone’s actual place in the distribution and their corresponding prediction of where they are in the distribution. The white dotted line moving up at a 45 degree angle represents perfect prediction, where all observations would be if everyone could exactly predict their place in the distribution. The blue-green line is a regression line of the relationship between someone’s actual place in the distribution and their prediction. The fact that it’s pretty much flat suggests no relationship, which is what we’d expect since that’s how the data was generated\n\ndk.x_vs_error_chart;\n\n\n\n\nAs with the previous chart, the basic format of this x_vs_error_chart will be the same in all scenarios, so I’ll just explain how to interpret it in this first example. The x axis again represents someone’s actual place in the percentile distribution, but now the y axis is the difference between their prediction and actual place. Each point in the scatter plot represents a single observation of someone’s actual place in the distribution and their corresponding prediction error. The horizontal white dotted line moving up at a zero again represents perfect prediction, where all observations would be if everyone could exactly predict their place in the distribution. The blue-green line is a regression line of the relationship between someone’s actual place in the distribution and their prediction error. Note that in this randomly generated scenario we’re seeing something that looks like Dunning Kruger, with people on the low endof the distribution tending to overestimate their place, and people on the high end tending to underestimate.\n\n\nUnbiased prediction error\nIn this scenario everyone is equally good at predicting their percentile placement with only a small, unbiased error.\n\nactual_dist = np.random.uniform(low=0.0, high=100.0, size=N_SAMPLES)\nerror = np.random.normal(loc=0.0, scale=10.0, size=N_SAMPLES)\npredicted_dist = actual_dist + error\n\n\ndk = DunningKruger(actual_dist, predicted_dist)\n\n\ndk.x_vs_y_chart;\n\n\n\n\n\ndk.x_vs_error_chart;\n\n\n\n\nEven here we can see a slight appearance of “overconfidence” for lower percentiles and “underconfidence” for higher percentiles simply because prediction errors are truncated at the top and bottom (If I’m actually in the lowest percentile the only errors I can possibly make are to overestimate my performance).\n\n\nActual Dunning Kruger\nLet’s see what this would actually look like if we simply had overconfidence in the bottom half of the distribution. I could add in underconfidence among the top half as well, but the message of the original paper focused on the overconfidence so let’s do the same here.\n\nactual_dist = np.random.uniform(low=0.0, high=100.0, size=N_SAMPLES)\nbase_error = np.random.normal(loc=0.0, scale=10.0, size=N_SAMPLES)\nbelow_median_indicator = actual_dist &lt; 50.0\n# The further below the median you are the greater you might overestimate your position\nerror_scaler = (50.0 - actual_dist) * below_median_indicator\n# Scale up how overconfident this makes you\noverconfidence_error = error_scaler * np.random.uniform(low=0.0, high=2.0, size=N_SAMPLES)\npredicted_dist = actual_dist + error + overconfidence_error\n\n\ndk = DunningKruger(actual_dist, predicted_dist)\n\n\ndk.x_vs_y_chart;\n\n\n\n\n\ndk.x_vs_error_chart;"
  },
  {
    "objectID": "posts/2022-08-01-dunning.html#conclusion",
    "href": "posts/2022-08-01-dunning.html#conclusion",
    "title": "Dunning-Kruger is autocorrelation",
    "section": "Conclusion",
    "text": "Conclusion\nHonestly I was pretty convinced by the argument in the original post, so nothing earth shattering came out of this exercise for me. But it was interesting to see what the charts looked like under different situations. And it gave me an excuse to write some code and build some charts, which I haven’t had in a while.\nIf you’d like to try out other scenarios, you can click the binder link near the top of this post to open it in an interactive notebook. If you want to try other distributions for your actual/predicted/error distributions, check out the numpy docs on random sampling."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "My blog about things I find interesting, mostly data, python, statistics, and Linux."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ian's blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDatabricks workspace config in terraform\n\n\n\n\n\nExpanding the limited examples I could find online\n\n\n\n\n\n\nNov 19, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nDatabricks cluster policies in terraform\n\n\n\n\n\nExpanding the limited examples I could find online\n\n\n\n\n\n\nNov 18, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\nChecking out XCP-NG\n\n\n\n\n\nHypervisor hopping is the new distro hopping\n\n\n\n\n\n\nNov 12, 2023\n\n\n24 min\n\n\n\n\n\n\n\n\nAutomating my network\n\n\n\n\n\nOverengineering at its finest\n\n\n\n\n\n\nJul 6, 2023\n\n\n16 min\n\n\n\n\n\n\n\n\nFinding all AD group users with ansible\n\n\n\n\n\nLDAP search is your friend, but you’ll have to do some string parsing\n\n\n\n\n\n\nJun 4, 2023\n\n\n14 min\n\n\n\n\n\n\n\n\nInstall Microsoft ODBC drivers with ansible\n\n\n\n\n\nThere’s a couple small tricks\n\n\n\n\n\n\nJun 4, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\nConfiguring Rootless docker with ansible\n\n\n\n\n\nRootless docker is nice and more secure, but it’s a hassle to set up\n\n\n\n\n\n\nJun 4, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\nConfiguring autofs for CIFS mounts with ansible\n\n\n\n\n\nA user isolated way to mount file shares on a shared Linux host\n\n\n\n\n\n\nJun 4, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\nA couple notes on working with Nvidia cards\n\n\n\n\n\nI figured some stuff out at work and wanted to save it here\n\n\n\n\n\n\nJun 4, 2023\n\n\n4 min\n\n\n\n\n\n\n\n\nRedesigning my network\n\n\n\n\n\nNetworking is hard.\n\n\n\n\n\n\nApr 9, 2023\n\n\n54 min\n\n\n\n\n\n\n\n\nSetting up my first managed switch\n\n\n\n\n\nI picked up a cheap HP procurve 2810, let’s get it working\n\n\n\n\n\n\nApr 8, 2023\n\n\n18 min\n\n\n\n\n\n\n\n\nNotes on Kubernetes the hard way\n\n\n\n\n\nI may have bit off a bit more than I could chew\n\n\n\n\n\n\nFeb 24, 2023\n\n\n30 min\n\n\n\n\n\n\n\n\nHome Cluster Part 4 - Setup CEPH\n\n\n\n\n\nWhat’s HA services without HA storage?\n\n\n\n\n\n\nFeb 5, 2023\n\n\n29 min\n\n\n\n\n\n\n\n\nMigrating to quarto\n\n\n\n\n\nFastpages is dead, long live quarto\n\n\n\n\n\n\nFeb 3, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\nHome cluster part 3 - Setup VM templates on proxmox\n\n\n\n\n\nHomelab cluster adventures continue\n\n\n\n\n\n\nJan 21, 2023\n\n\n44 min\n\n\n\n\n\n\n\n\nHome cluster part 2 - Configuring Proxmox\n\n\n\n\n\nHomelab cluster adventures continue\n\n\n\n\n\n\nDec 31, 2022\n\n\n13 min\n\n\n\n\n\n\n\n\nBuilding my own devcontainers\n\n\n\n\n\nSure I could just use the Microsoft built ones, but where’s the fun in that?\n\n\n\n\n\n\nDec 30, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\nSetting up rootless docker\n\n\n\n\n\nHow can multiple users share a host for docker without a security nightmare?\n\n\n\n\n\n\nDec 30, 2022\n\n\n12 min\n\n\n\n\n\n\n\n\nHome cluster part 1 - Intro and Proxmox install\n\n\n\n\n\nFiguring out how to make my own little homelab cluster\n\n\n\n\n\n\nNov 21, 2022\n\n\n10 min\n\n\n\n\n\n\n\n\nDunning-Kruger is autocorrelation\n\n\n\n\n\nRead a cool post, wanted to play around with the idea some more\n\n\n\n\n\n\nAug 1, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\nModeling out fixed closed vs capped variable mortgages\n\n\n\n\n\nI’m buying a house, let’s use python to help.\n\n\n\n\n\n\nMar 17, 2022\n\n\n9 min\n\n\n\n\n\n\n\n\nBuilding a where to live app\n\n\n\n\n\nAn excuse to teach myself some cool tools and figure out the best place to live\n\n\n\n\n\n\nDec 30, 2021\n\n\n14 min\n\n\n\n\n\n\n\n\nMy Portfolio\n\n\n\n\n\nThe bigger posts and projects I’ve worked on that I want to share.\n\n\n\n\n\n\nApr 10, 2021\n\n\n8 min\n\n\n\n\n\n\n\n\nAlberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue\n\n\n\n\n\nI wanted to practice my python and econometrics, found a data error that invalidated the paper’s findings as well.\n\n\n\n\n\n\nFeb 26, 2021\n\n\n10 min\n\n\n\n\n\n\n\n\nUsing traefik for internal services\n\n\n\n\n\nHow to make a reverse proxy work within your LAN\n\n\n\n\n\n\nJan 9, 2021\n\n\n3 min\n\n\n\n\n\n\n\n\nAutomating provisioning Arch continued - TLDR\n\n\n\n\n\nSummarizing how I provision my machines with just the steps to reproduce the process.\n\n\n\n\n\n\nNov 26, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\nAutomating provisioning Arch continued - dotfiles\n\n\n\n\n\nHow I do all the user level configuration for my system, including setting up my python environment.\n\n\n\n\n\n\nNov 25, 2020\n\n\n10 min\n\n\n\n\n\n\n\n\nAutomating provisioning Arch continued - Ansible\n\n\n\n\n\nUsing ansible to install programs and do other system level configuration\n\n\n\n\n\n\nNov 21, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\nAutomating provisioning Arch - OS installation\n\n\n\n\n\nMy bash script for getting a minimal working Arch install.\n\n\n\n\n\n\nOct 14, 2020\n\n\n28 min\n\n\n\n\n\n\n\n\nNotes on docker-compose\n\n\n\n\n\nTiny snippets of things I forget or often have to look up about docker compose.\n\n\n\n\n\n\nOct 6, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\nConnecting a Harmony remote to a raspberry pi\n\n\n\n\n\nHandy tip if you’re using raspberry pi OS as a media front end and have a harmony remote, or just want to remember how to pair bluetooth from the command line.\n\n\n\n\n\n\nJul 20, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\nPython packaging\n\n\n\n\n\nHelping myself understand python packaging by working up from a single file to an actual library.\n\n\n\n\n\n\nJul 9, 2020\n\n\n28 min\n\n\n\n\n\n\n\n\nTrouble with reproducible conda environments\n\n\n\n\n\nSometimes my conda environments aren’t reproducible and I can’t figure out why. This documents what I’ve tried so far.\n\n\n\n\n\n\nMay 17, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\nHow to work with conda environments in shell scripts and Makefiles\n\n\n\n\n\nconda environments are a little tricky to incorporate into automation. This post outlines a bit of how they work, and how to integrate them in scripts and makefiles.\n\n\n\n\n\n\nMay 13, 2020\n\n\n8 min\n\n\n\n\n\n\n\n\nA basic SAMBA share for home networks\n\n\n\n\n\nSimple (but not super secure) SAMBA shares for the home user\n\n\n\n\n\n\nMay 9, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nBuilding pfsense\n\n\n\n\n\nDocumenting everything I did to configure my pfsense router.\n\n\n\n\n\n\nMay 6, 2020\n\n\n19 min\n\n\n\n\n\n\n\n\nManaging SSH keys for a home network\n\n\n\n\n\nHow to generate certificate authorities so your clients and hosts can all talk to each other easily.\n\n\n\n\n\n\nMay 3, 2020\n\n\n10 min\n\n\n\n\n\n\n\n\nSetting up for data science in python on Windows\n\n\n\n\n\nMy opinionated setup to do data work in python on Windows, without admin rights.\n\n\n\n\n\n\nFeb 15, 2020\n\n\n18 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-02-05-proxmox-ceph.html",
    "href": "posts/2023-02-05-proxmox-ceph.html",
    "title": "Home Cluster Part 4 - Setup CEPH",
    "section": "",
    "text": "This is the fourth post ( part 1, part 2, part 3 ) in my home cluster with proxmox series. In this post we’re going to add distributed storage to the cluster using ceph. As with the other posts in this series, this is not a how-to guide from an established practitioner, but a journal I’m writing as I try and do something new.\nCeph in many ways is overkill for what I’m doing here. It’s designed to support absolutely massive distributed storage at huge scale and throughput while maintaining data integrity. To accomplish that it’s very complicated and their hardware recommendations reflect that. On the other hand, it’s integrated with proxmox and I’ve seen it run on even lower spec gear than I’m using. In this post my goal is to get a ceph cluster working that uses the 3 1TB SSDs I have in my nodes for file sharing. I’m not going to do any performance testing or tuning, and other than deploying an image to one just to confirm it works I probably won’t even use it in this section. The thing I actually want this storage for is to be my persistent storage in kubernetes, backed by rook, but that will come later once I actually have kubernetes set up.\nAs with most things with computers I won’t be starting from scratch. I’ve found a repository of ansible roles for setting up a proxmox cluster that includes ceph configuration and is very similar to my overall setup. I’ll work through vendoring this code into my recipes repository through this post.\n\n\nI ran into lots of problems getting this working. This post is definitely less of a guide and more a diary of the struggles I had getting ceph working. There may be some value to another reader if they find themselves having a similar challenge to me, but mostly this was just my scratchpad as I worked through getting things set up."
  },
  {
    "objectID": "posts/2023-02-05-proxmox-ceph.html#note",
    "href": "posts/2023-02-05-proxmox-ceph.html#note",
    "title": "Home Cluster Part 4 - Setup CEPH",
    "section": "",
    "text": "I ran into lots of problems getting this working. This post is definitely less of a guide and more a diary of the struggles I had getting ceph working. There may be some value to another reader if they find themselves having a similar challenge to me, but mostly this was just my scratchpad as I worked through getting things set up."
  },
  {
    "objectID": "posts/2023-02-05-proxmox-ceph.html#setting-up-the-inventory",
    "href": "posts/2023-02-05-proxmox-ceph.html#setting-up-the-inventory",
    "title": "Home Cluster Part 4 - Setup CEPH",
    "section": "Setting up the inventory",
    "text": "Setting up the inventory\nThe first section of the repo that I’ll incorporate is the inventory folder. This contains descriptions of the hosts, as well as what groups they belong to for roles. The inventory folder in this repo also contains group_vars and host_vars, which I keep in their own folders in my repo.\nLooking at the actual inventory there are a bunch of groups created for various ceph roles like mds, mgr, and osd. However, in the example case and in my case all nodes will fulfill all roles, so this is only necessary for expansion or comprehensibility of what tasks are doing what when a role is run. There is one differentiator for ceph_master, which only targets the first node to handle tasks that are managed at the proxmox cluster level. In my previous setup I’ve just had a pve group for the cluster and manually set pve1 as the host for things that take place at the cluster level. If I end up growing my cluster a lot and want to split things out I’ll have to refactor, but for now for simplicity I’m going to stick with just using the pve group. Based on this I don’t need any actual changes to my inventory. Looking at host_vars there are host specific variables identifying the separate NIC and IP address the nodes are using for the ceph network. Having a separate network for ceph is a recommendation that I am not following at this point so I don’t need to worry about that. They also have a host var specifying which storage drive should be attached to the ceph cluster. For me that’s /dev/sda on all of my nodes. I’ll have to refactor that out if I add another node that deviates from that, but for now I’m going to minimize the complexity in terms of number of files I have to reference and leave that alone. Looking at the group vars under ceph there’s an entry for the pool name, and for the list of nodes. Again, both of those I can just set as defaults for now and refactor later if I have to expand. So based on initial reading I’m going to leave this folder alone."
  },
  {
    "objectID": "posts/2023-02-05-proxmox-ceph.html#checking-the-library-folder",
    "href": "posts/2023-02-05-proxmox-ceph.html#checking-the-library-folder",
    "title": "Home Cluster Part 4 - Setup CEPH",
    "section": "Checking the library folder",
    "text": "Checking the library folder\nThe library folder contains a script for managing proxmox VMs with the qm command. That’s interesting, but not relevant to what I’m trying to do with ceph so I won’t worry about it here."
  },
  {
    "objectID": "posts/2023-02-05-proxmox-ceph.html#roles",
    "href": "posts/2023-02-05-proxmox-ceph.html#roles",
    "title": "Home Cluster Part 4 - Setup CEPH",
    "section": "Roles",
    "text": "Roles\nHere is going to be the bread and butter of this process. There are a number of roles in this folder helpfully prepended with ceph_ that I’ll want to take a look at.\nIn terms of order of reviewing these files I’m going to look at the site.yml file that’s at the base of the repository to understand what order they’re called in. That should make the most sense.\n\nceph_node\nThe first role is ceph_node which runs on all the nodes. There are two steps here, the first with the name “Install ceph packages”, and the second “Configure ceph network”, which I’ll ignore. There’s also a handler in this role, but it’s only to restart the network after configuring the second network, so I don’t need that. The first task looks like this:\n- name: Install ceph packages\n  shell: yes | pveceph install\n  args:\n    creates: /etc/apt/sources.list.d/ceph.list\nThere are a few things I have not seen before here that I’d like to understand before I blindly copy paste. The first is the yes command. This post explains what it is and why I’d use it. It’s basically for entering y into the user input of everything the command it’s piped to installs. The other thing I haven’t seen before is args. While args appears to be a generic keyword its use in this case is pretty well documented in the docs for shell. In this case it’s being used to say that running this command will create that file, so if it exists the file doesn’t need to be run, ensuring idempotency. Pretty handy!\nWhile I’m sure this would just work, I do want to know a bit about what I’m hitting y to by running this playbook, so let’s ssh into one of my nodes and manually run the command and save the output.\nroot@pve1:~# ls /etc/apt/sources.list.d | grep ceph\ndownload_proxmox_com_debian_ceph_quincy.list\nPrior to running the command I can confirm I do not have that file present.\nRunning pveceph install prompts an apt install command, the y is to confirm that I want to install a ton of ceph related packages. There are no other prompts so this seems safe to run.\n\n\nceph_master\nThe next role is described as creating the ceph cluster and only needs to be run on one node. This is also a small task and it looks like this:\n- name: Check ceph status\n  shell: pveceph status 2&gt;&1 | grep -v \"not initialized\"\n  register: pveceph_status\n  ignore_errors: true\n  changed_when: false\n\n- name: Create ceph network\n  command: pveceph init --network 10.10.10.0/24\n  when: pveceph_status.rc == 1\nI’ll have to modify the network part to match my own setup, but otherwise this looks straightforward. Just for curiosity, let’s see what the first command looks like. As a reminder to myself, the 2&gt;&1 redirects stderr to stdout.\nroot@pve1:~# pveceph status 2&gt;&1\npveceph configuration not initialized\nLooking at the pveceph docs it looks like I can just drop the --network argument if I’m not specifying a separate one, so this will be a very small task. Note from me in the future: you need the network flag.\n\n\nceph_mon\nNext up we create monitors. This is also a simple looking role:\n- name: Check for ceph-mon\n  command: pgrep ceph-mon\n  register: ceph_mon_status\n  ignore_errors: true\n  changed_when: false\n\n- name: Create ceph-mon\n  shell: pveceph createmon\n  when: ceph_mon_status.rc == 1\npgrep looks for running processes, so that’s how we check if the monitor is already up and running. If it’s not, we create a monitor. The only arguements for this command are to assign an address or ID, neither of which I want to explicitly do, so I can leave this as is.\n\n\nceph_mgr\nAfter the monitor we create a manager. The setup is basically the same as the monitor and the command it runs has even fewer arguments than the monitor so I won’t spell it out here.\n\n\nceph_osd\nNow we have to create an osd which is the first place we’ll have to touch an actual disk. Having this step not be idempotent would be really bad as it could lead to wiping disks. The task looks like this:\n- name: Check for existing ceph_osd\n  command: pgrep ceph-osd\n  register: ceph_osd_pid\n  changed_when: false\n  ignore_errors: true\n\n- name: Read first 5KB of ceph device to determine state\n  shell: dd if={{ ceph_device }} bs=5K count=1 | sha256sum\n  when: \"ceph_osd_pid.rc != 0\"\n  register: ceph_device_first_5KB_sha256\n  changed_when: false\n\n- name: Determine if should initialize ceph_osd\n  when: \"ceph_osd_pid.rc != 0 and ceph_device_first_5KB_sha256.stdout == 'a11937f356a9b0ba592c82f5290bac8016cb33a3f9bc68d3490147c158ebb10d  -'\"\n  set_fact:\n    ceph_device_initialize: true\n  \n- name: Initialize ceph_osd device\n  when: ceph_device_initialize == True\n  command: pveceph createosd {{ ceph_device }}\nThere’s also a default variable for ceph_device_initialize that’s set to False. It only gets updated to true if that third step’s condition is met. I’m a little confused and worried about this role to be honest. The first step is fine, we’re just checking if the osd process is running. The next one is apparently making some assumption about what the hash of the first 5KB of my disk should look like if it doesn’t already have an osd installed. I don’t know how this would work and searching didn’t turn anything up. Let’s test though and check what it returns on my drives:\nroot@pve1:~# dd if=/dev/sda bs=5K count=1 | sha256sum\n1+0 records in\n1+0 records out\n5120 bytes (5.1 kB, 5.0 KiB) copied, 0.00509153 s, 1.0 MB/s\na11937f356a9b0ba592c82f5290bac8016cb33a3f9bc68d3490147c158ebb10d  -\n\nroot@pve2:~# dd if=/dev/sda bs=5K count=1 | sha256sum\n1+0 records in\n1+0 records out\n5120 bytes (5.1 kB, 5.0 KiB) copied, 0.00511535 s, 1.0 MB/s\na11937f356a9b0ba592c82f5290bac8016cb33a3f9bc68d3490147c158ebb10d  -\n\nroot@pve3:~# dd if=/dev/sda bs=5K count=1 | sha256sum\n1+0 records in\n1+0 records out\n5120 bytes (5.1 kB, 5.0 KiB) copied, 0.00503435 s, 1.0 MB/s\na11937f356a9b0ba592c82f5290bac8016cb33a3f9bc68d3490147c158ebb10d  -\nJust to make sure I wasn’t losing it, I tried it on another device that wasn’t blank and got a different hash. This is why I love the internet, there is absolutely no way I would have figured that out on my own. I don’t know how it works and that makes me a little nervous, but at this point I’m convinced that it will work. I’ll add in a default variable for my ceph device of /dev/sda and should be good to go.\n\n\nceph_pool\nNow that I’ve got my OSDs, it’s time to create a pool. This role also has a defaults file, with currently just one variable to specify the minimum number of nodes that must be up for pool creation (set to 3 which works for me). I’ll have to add in another default to mine for the pool name, as the original repo sets that in group vars. Beyond that let’s focus on the task:\n- name: Check ceph status\n  command: pveceph status\n  register: pveceph_status\n  ignore_errors: true\n  changed_when: false\n\n- name: Check ceph pools\n  shell: pveceph pool ls | grep -e \"^{{ ceph_pool }} \"\n  register: ceph_pool_status\n  changed_when: false\n  ignore_errors: true\n\n- name: Create ceph pool\n  when: ceph_pool_status.rc &gt; 0 and (pveceph_status.stdout | from_json).osdmap.osdmap.num_up_osds &gt;= minimum_num_osds_for_pool\n  command: pveceph pool create {{ ceph_pool }}\n\n- name: Check ceph-vm storage\n  command: pvesm list ceph-vm\n  changed_when: false\n  ignore_errors: true\n  register: ceph_vm_status\n\n- name: Create ceph VM storage (ceph-vm)\n  when: ceph_vm_status.rc &gt; 0\n  command: pvesm add rbd ceph-vm -nodes {{ ceph_nodes }} -pool {{ ceph_pool }} -content images\n\n- name: Check ceph-ct storage\n  command: pvesm list ceph-ct\n  changed_when: false\n  ignore_errors: true\n  register: ceph_ct_status\n\n- name: Create ceph container storage (ceph-ct)\n  when: ceph_ct_status.rc &gt; 0\n  command: pvesm add rbd ceph-ct -nodes {{ ceph_nodes }} -pool {{ ceph_pool }} -content rootdir\nThe first step pulls up a detailed description of the ceph pool status. In the third step we’ll parse it to check that we have the minimum number of OSDs up. The next one is pretty straightforward, make sure the pool we want to create doesn’t already exist. Next, assuming we have at least the minimum number of OSDs and our pool hasn’t been created, create it. This one is using all the defaults of the command since we don’t pass any arguments. Briefly, they are:\n\nnot to configure VM and CT storage for the pool (that appears to happen later)\nset the application as rbd (we will configure ceph fs later on).\nSome other stuff about scaling and erasure coding that I don’t understand and hopefully won’t need for now. Full docs here, search for pveceph pool create &lt;name&gt; [OPTIONS]\n\nThe next four parts configure proxmox to use ceph as a storage location for VMs and containers. I actually don’t want to do that, my VMs will live on my nvme drives, but it won’t hurt to have as an option I guess, and at least I can test if I can do stuff on the pool with this enabled so I’ll leave it but not spend much time working out how it works. I will have to add a variable for ceph_nodes to my defaults that maps to a comma separated list of my nodes.\n\n\nceph_mds\nAfter this we’re doing some necessary pre-configuration for enabling ceph-fs. Specifically the ceph metadata server. This is another very short task that checks if the service is running and starts it if not with a oneliner, so I won’t reproduce it here.\n\n\nceph_fs\nLast one. Ceph fs, from what little I’ve read of it would be nice to have as it will enable sharing storage across pods (docs). This task has very similar structure to the earlier ones as well so I won’t write it up in detail here."
  },
  {
    "objectID": "posts/2023-02-05-proxmox-ceph.html#adding-them-to-the-playbook",
    "href": "posts/2023-02-05-proxmox-ceph.html#adding-them-to-the-playbook",
    "title": "Home Cluster Part 4 - Setup CEPH",
    "section": "Adding them to the playbook",
    "text": "Adding them to the playbook\nHaving created the roles, I now need to make sure they’re done in the correct order in my playbook. As mentioned above I can base that on the order they’re listed in site.yml in the base repository I’ve been working off."
  },
  {
    "objectID": "posts/2023-02-05-proxmox-ceph.html#clean-up-the-install",
    "href": "posts/2023-02-05-proxmox-ceph.html#clean-up-the-install",
    "title": "Home Cluster Part 4 - Setup CEPH",
    "section": "Clean up the install",
    "text": "Clean up the install\nAs discussed in the last section I’ll run pveceph purge --crash --logs on all three nodes (that might be overkill but let’s be safe).\nroot@pve1:~# pveceph purge --crash --logs\nUnable to purge Ceph!\n\nTo continue:\n- remove pools, this will !!DESTROY DATA!!\n- remove active OSD on pve1\n- remove active MDS on pve1\n- remove other MONs, pve1 is not the last MON\nOk, I can’t purge to start, I’ll have to back my way out.\n\nremove cephfs\nThe list above only talks about pools, but I’ve got a cephfs on top of that to remove first. The pveceph docs have a section on destroying a cephfs. Let’s follow that.\numount /mnt/pve/cephfs\npveceph stop --service mds.cephfs # Run this on all nodes\nThat didn’t seem to actually stop the MDSs, so I went into the UI and destroyed them all. Based on the guide, after that I should be able to remove it with pveceph fs destroy cephfs --remove-storages --remove-pools but I get storage 'cephfs' is not disabled, make sure to disable and unmount the storage first. A little more searching gets me ceph fs rm cephfs --yes-i-really-mean-it which runs ok and upon completion I don’t see any entries for cephfs anymore, so I think that’s good.\n\n\nremove my other pool\nI think I’m going to do the rest of this through the UI. It’s not the sort of thing I need to automate, and the UI seem to be cleaner and easier. Ok, my pools are gone, including some related to cephfs that didn’t seem to clear out with the old command. My nodes are still showing the pools as storage locations, but with a ? by them. I think that will go away once I purge the config for ceph, so let’s not worry about it for now.\n\n\nremove OSDs\nFrom the UI, for each OSD in my cluster I first take it out, stop it, then destroy it.\n\n\nremove MDS\nLooks like that was taken care of when I removed cephfs. No action\n\n\nremove managers and monitors\nAgain from the UI I destroy each manager, and then destroy all but one monitor.\n\n\ntry purging again\nHmmm, I’m still getting told to remove pools and mons. Not sure what’s up with that. Ahh, pveceph pool ls tells me I still have a .mgr pool. I didn’t realize that counted. Ok, that’s cleared out. I’ve still got this monitor listed under one of my nodes but with status unknown and I can’t seem to destroy it from the UI. Going into the ceph docs I can see there are some docs on removing mons from an unhealthy cluster. The ghost monitor is running on my third node so I ssh into it and I can see the monitor service is indeed running there. I’m able to stop the service on that node with systemctl stop ceph-mon.target. This still doesn’t let me run purge though. If I run it I get told that my monitor isn’t the last one running, but also if I try and remove that monitor I get told it’s the last one. That’s… confusing. Ok, let’s go back to that third node, disable the monitor service and reboot it the node. Still nothing. Running ceph mon dump on any node only shows the monitor I know is running on my first node. Looking at /etc/pve/ceph.conf I only see the one monitor. Ok, bit of googling and I’m back to this thread which reminds me to check /var/lib/ceph/mon on the node with the unknown status monitor. Sure enough, there’s still a folder there and after I delete it I don’t see that entry anymore. Let’s try purging again.\nThat seems to have worked. If I go to the ceph page in the UI I’m told that it’s not configured. I can still see the storage pools on my nodes though. I wonder if that’s just in /etc/pve/storage.cfg like my NFS share configs are. Yup! Ok, after deleting that I no longer see them as storage in the UI. I think I’m good. One last thing to do is to go into each node through the UI and wipe the SSDs."
  },
  {
    "objectID": "posts/2022-12-30-devcontainers.html",
    "href": "posts/2022-12-30-devcontainers.html",
    "title": "Building my own devcontainers",
    "section": "",
    "text": "Introduction\nMicrosoft has created the devcontainer standard for packaging up your development environment into a docker image and I love it.\nEven though I have a fairly automated environment setup at home, it’s still a hassle whenever I want to start a new project or pick up an old one to make sure I have all the dependencies in place. It’s even trickier if I’m trying to help another person contribute to a project of mine. Devcontainers solve both these issues. Microsoft publishes a number of out of the box images and templates in their GitHub devcontainers project.\nThese work quite well, but I’m picky and want things set up in a certain way. For instance, I want rcm installed for dotfile management, and starship prompt in any environment I work in. On top of that, for python development I like the hypermodern suite of tools to be installed. It would be relatively easy to make a dockerfile that has these features installed and put it in every project, but I want to overengineer things. This isn’t entirely just me liking to hack on things. The build time for my python environment is actually quite long thanks to compiling several different versions of python, so while a Dockerfile would work, it would be annoying to maintain and take quite a while to build.\nIn light of this, I decided to make my own copy of the images repository that Microsoft uses to build their devcontainers and make my own. This post chronicles some of the challenges I had doing that.\n\n\nFiguring out the code\nTo start I just copied the entire images repository and poked around. It’s a beast of a repo (at least compared to the personal or small organization projects I’m used to working on) so it took quite a while just to get a sense of what was there. At a high level there’s a .github folder which contains the CI/CD workflows, a build folder that contains node scripts that build the images, and a src folder that contains the devcontainer specs. I started by deleting all but the base-ubuntu image from src so I could focus on getting one container built without extensive build times. After that I tried to get the build script working locally. Fortunately there are pretty good README files included in each section of the codebase, so I could get a general sense of what was going on.\nThe next two difficult parts that went together were figuring out how to navigate and understand the node codebase, since I’ve never written node or any javascript before, and figuring out what I’d need to modify to get things working in my repository. Some things were relatively straightforward, like the GitHub actions were calling for secrets like REPOSITORY_NAME and REPOSITORY_SECRET that I’d have to swap out for my image registry name and credentials. Once I got past that surface level understanding though, it got trickier. One fairly easy example was that the original GitHub action wanted to be run on some custom Microsoft devcontainer-image-builder-ubuntu VM that I didn’t have access to. It seemed to work fine if I changed that to ubuntu-latest, I just had to notice the issue and change it. Other things were more embedded. Microsoft is publishing their images to Azure container registry whereas I want to use Docker hub. Again, some of this was as simple as switching out az login with docker login in the scripts, but some of it was a little more complicated. Part of the node code queries the registry to see what images are there and what tags they have to make sure published image tags aren’t overwritten accidentally. This is a great feature, but it relied heavily on calling the acr command prompt to retrieve that info. I had to find those sections in the code, figure out what sort of data they’d be returning, figure out what request to send to the docker hub API to get similar data, and then modify the node code to parse it the same way. Since I’d never worked with the docker hub API, or node, or seen the actual output of the acr commands I was trying to reproduce, this took some trial and error.\nAn additional challenge was separating out the nice features of the Microsoft code base from the stuff that I didn’t want and that just made things more complicated. The two main things in the latter category were the secondary registry logic and the stubs repository logic. In both cases, Microsoft is publishing lots of extra stuff besides the built devcontainer image, either because they have two repositories to publish to (I think this relates to them moving the devcontainer spec outside of VS Code into its own project) or they want to publish stub files that other developers can extend for their own purposes. Neither applies to me, but since that logic is embedded in the GitHub actions and the node code that publishes regular images I had to find and strip out all that logic before I could publish my own images.\n\n\nBuilding my own devcontainers\nPrior to going to all the trouble of setting up this build infrastructure, I’d already spent quite a bit of time building devcontainer images, primarily for python. In light of that, once I got the build infrastructure going it wasn’t a huge leap to get my own devcontainers building. There were some growing pains though. The Microsoft image builder builds multiple variants of images, namely basing them off different Ubuntu releases or architectures (x86/64 vs arm). I definitely ran into situations where things seemed to be building fine but then I realized some combo of release and architecture was failing and stopping the whole pipeline from completing. There are ways to test those things locally in the repository, but I didn’t have any comprehensive workflow set up so it was easy to miss things. Some stuff I just didn’t bother fixing and removed the troublesome build. For instance, there were a fair number of issues building images based on Ubuntu 18.04LTS (one of the default variants from Microsoft) and I just decided there was no point spending time fixing issues with a release that was about to be EOL from Ubuntu and just dropped it. Similarly, my Infrastructure as Code image didn’t want to install Terraform on the arm build. I’m not currently planning to run that on an arm system so I just dropped it, maybe I’ll put it back later if I want to run it off a raspberry pi but for now it’s not worth the effort.\n\n\nConclusion\nThis was quite likely more effort than it was objectively worth compared to just building an image and pushing it manually with some tags using the devcontainer cli at least for my personal projects. I did learn a fair bit going through the exercise though, and since I also intend to adopt devcontainers at work (for myself and other people writing at least python code) knowing how to build images in a more automated and versioned manner will be useful.\nMy repository is here, the original Microsoft one is here. My repo is definitely a bit of a mess with ugly commits just testing out CI/CD outcomes and a lot of failed releases since I’d never used GitHub directly to release software before, but that’s all part of the learning experience."
  },
  {
    "objectID": "posts/2020-11-21-ansible.html",
    "href": "posts/2020-11-21-ansible.html",
    "title": "Automating provisioning Arch continued - Ansible",
    "section": "",
    "text": "This is part 2 of a 4 part series describing how I provision my systems. Links to each part are below:\n\npart 1 - The base OS install\npart 2 - Software install and system configuration with Ansible\npart 3 - User level and python environment config with dotfiles and mkrc\npart 4 - The tldr that wraps up how to do the whole thing from start to finish]\n\n\nIntroduction\nMy previous post described how to automate a base installation of Arch. This follow up post will give an overview of the next step of configuration.\nAfter getting a base system setup there is still a ton of administrative tasks to do, like creating a user account and installing software. I accomplished this using Ansible. As with the previous post, I borrowed heavily from Brennan Fee for the configuration. My copy is here. This post won’t be as in depth as the previous one, as the ansible syntax is a lot more directly readable, so in most cases it should be enough to look at the code and maybe consult the ansible docs to figure out what’s going on. The sections below will outline a few of the parts that were a little tricky.\n\n\nHashed passwords\nAnsible lets you create a user and include the hash of their password, which means you can have the data available publicly without a security concern. In order to generate a hash of a password refer to this section of the ansible FAQ\n\n\ngit clone\nI had a tricky time with this task. I wanted to clone some repositories I controlled using ssh and save them in my home directory. After a lot of googling I determined that trying to become my user and do the clone directly wouldn’t work because ansible wouldn’t know which key to use (I have a separate key for GitHub than for my local network). This task splits it up by cloning into the ansible user directory and then using the copy task to move them over to my home directory and set the correct permissions. A little hacky, but it worked.\n\n\ndconf\nYou can use ansible to configure your GNOME desktop with the dconf module. The trickiest part of that is figuring out what key you have to change. This blog has the solution I used.\n\ndconf dump / &gt; before.txt\nmake changes in settings or tweak tool\ndconf dump / &gt; after.txt\ndiff before.txt after.txt\nFigure out what changed and create a dconf task for it.\n\n\n\nOther resources\nBeyond the links previously mentioned I want to highlight a tutorial series from Jeff Geerling which was excellent and informative. He also wrote a book on ansible that I haven’t read yet but imagine is quite good, given the quality of his video guide, and the fact that I found posts from him a few times when I was googling how to do something.\n\n\nConclusion\nAnsible is a pretty rad way to reproducibly get your desktop environment set up just the way you like it. It’s a bit overkill given what it’s actually designed for, but it’s a handy skill to learn and it saves rebuilding your environment from scratch."
  },
  {
    "objectID": "posts/2023-02-24-k8s-the-hard-way.html",
    "href": "posts/2023-02-24-k8s-the-hard-way.html",
    "title": "Notes on Kubernetes the hard way",
    "section": "",
    "text": "In this post I’ll be recording notes related to working through kubernetes the hard way by Kelsey Hightower. I’ve gone through a few kubernetes tutorials before, and messed around with minikube a bit, but that’s it. Before I try and get a “proper” k8s cluster going on my proxmox setup I’m going to try and work through this guide in the hope that it will improve my understanding of the setup."
  },
  {
    "objectID": "posts/2023-02-24-k8s-the-hard-way.html#sidebar-to-load-in-secrets",
    "href": "posts/2023-02-24-k8s-the-hard-way.html#sidebar-to-load-in-secrets",
    "title": "Notes on Kubernetes the hard way",
    "section": "Sidebar to load in secrets",
    "text": "Sidebar to load in secrets\nTerraform needs credentials to control my proxmox cluster, and I clearly don’t want to have those in git. In my earlier post I mentioned that I’d try using vault or bitwarden to manage secrets at some point. I’m going to save vault for now, but I have added the bitwarden cli to my devcontainer, so I should be able to use it to securely retrieve credentials into a project. I created a secure note in bitwarden labeled pve_terraform_cred, now to load that into my workspace.\nThis actually turned out to be pretty straightforward, which was a nice surprise:\nif [ ! -f pve_creds.env ]; then\n    echo \"Credentials file doesn't already exist, loading from Bitwarden.\"\n    echo \"Logging into bitwarden\"\n    bw login\n    echo \"Getting the terraform creds\"\n    bw get notes pve_terraform_cred &gt; pve_creds.env\nfi\nNow when I start working in terraform I just have to run source pve_creds.env in my terminal to have the environment variables for my username and password available. I spent a bit of time trying to get the script itself to set the environment variables, but child processes can’t modify the environment of their parents so I’m stuck there."
  },
  {
    "objectID": "posts/2023-02-24-k8s-the-hard-way.html#basic-setup",
    "href": "posts/2023-02-24-k8s-the-hard-way.html#basic-setup",
    "title": "Notes on Kubernetes the hard way",
    "section": "Basic setup",
    "text": "Basic setup\nI covered this in the templates post, but briefly let’s go over setting up the provider and a connection to my cluster. In my main.tf file I have the following code:\nterraform {\n  required_providers {\n    proxmox = {\n      source  = \"telmate/proxmox\"\n      version = \"2.9.11\"\n    }\n  }\n}\nprovider \"proxmox\" {\n  pm_tls_insecure = true\n  pm_api_url      = \"https://pve1.local.ipreston.net:8006/api2/json\"\n}\nAnd that’s it for setting up the connection. I have environment variables set by my script above with the username and password to my cluster so I don’t need to provide anything in the code. A quick terraform init followed by terraform plan shows that I’m all set up."
  },
  {
    "objectID": "posts/2023-02-24-k8s-the-hard-way.html#provision-vms",
    "href": "posts/2023-02-24-k8s-the-hard-way.html#provision-vms",
    "title": "Notes on Kubernetes the hard way",
    "section": "Provision VMs",
    "text": "Provision VMs\nTo follow the guide I will need to create a total of 6 VMs, 3 each of controllers and workers. The configuration of all of these nodes should be largely identical, with the exception of hostname, IP address, and which proxmox node they’re loaded on. The most straightforward way to do this in terraform would be to just set up one VM, and then copy-paste its config 5 more times with slight modifications. I could do a slightly fancier version with for each loops, but that will still have a lot of common config mixed in with the parts that are looping and will be tricky to read and update. It’s a bit overkill for something like this, but the point here is learning so I’m going to create a module that hard codes all the common aspects of the VMs I’m going to create and only leaves the parts that will change across nodes and controlllers/workers as variables.\n\nCreate a module\nIn the terraform folder I’ll make a modules/ubuntu-vm subdirectory and in that I’ll place two files. First we have variables.tf:\nvariable \"node\" {\n  description = \"Proxmox node number to deploy to\"\n  type        = number\n}\n\nvariable \"type\" {\n  description = \"A controller or worker node\"\n  type        = string\n}\n\nvariable \"ip\" {\n  description = \"The static IP for the VM\"\n  type        = string\n}\nThis is just defining the variables that I’ll need to pass into this module to create a resource. As described above, I want everything else about these nodes to be the same, so this is all I need for variables.\nThen I have a main.tf:\nterraform {\n  required_providers {\n    proxmox = {\n      source  = \"telmate/proxmox\"\n      version = \"2.9.11\"\n    }\n  }\n}\n\nresource \"proxmox_vm_qemu\" \"ubuntu-vm\" {\n  name        = \"ubuntu-${var.type}-${var.node}\"\n  target_node = \"pve${var.node}\"\n  onboot      = true\n  oncreate    = true\n  clone       = \"ubuntujammytemplate\"\n  full_clone  = true\n  agent       = 1\n  os_type     = \"cloud-init\"\n  cores       = 4\n  cpu         = \"host\"\n  memory      = 8192\n  bootdisk    = \"scsi0\"\n  disk {\n    slot     = 0\n    size     = \"100G\"\n    type     = \"scsi\"\n    storage  = \"local-zfs\"\n    iothread = 1\n  }\n  network {\n    model  = \"virtio\"\n    bridge = \"vmbr0\"\n  }\n  ipconfig0 = \"ip=${var.ip}/24,gw=192.168.85.1\"\n}\nHaving to put the required provider up here was a little confusing at first, since I had it defined in the base terraform module, but after some errors and troubleshooting I learned that I have to specify the required provider in every module that uses it. Note that I don’t have the provider block that explicitly points to the actual proxmox instance I want to apply this to, that only lives in the base module. The rest of this block is a standard terraform proxmox VM resource where I’ve hard coded in all the parameters I want to be consistent across nodes, and plugged in variables for the parts that are going to change.\n\n\nFun with loops\nThe other tricky part of this is that I would really like to do nested for each loops, which I guess isn’t a native concept in terraform. In python to create the map of values that I want I’d do something like:\nvms = [\n    {\n        \"nodetype\": nodetype,\n        \"pvenode\": i + 1,\n        \"vm_ip\": f\"192.168.85.{70 + base_octet + i}\"\n    }\n    for nodetype, base_octet in [(\"controller\", 0), (\"worker\", 3)]\n    for i in range(3)\n]\nI can’t nest a for each in terraform, so I have to do some nested for loops in variables to create a map that I can then use for each on. This blog has basically the same issue so I should be able to follow its logic to produce what I want. After some fiddling around I get the following:\nlocals {\n  nodetypes = {\n    \"controller\" = 0\n    \"worker\"     = 3\n  }\n  vm_attrs_list = flatten([\n    for nodetype, baseoctet in local.nodetypes : [\n      for i in range(3) : {\n        name = \"${nodetype}${i}\"\n        node = \"${i + 1}\",\n        type = \"${nodetype}\",\n        ip   = \"192.168.85.${70 + baseoctet + i}\"\n      }\n    ]\n  ])\n  vm_attrs_map = {\n    for obj in local.vm_attrs_list : \"${obj.name}\" =&gt; obj\n  }\n\n}\nWhich is certainly a lot more verbose than python, but whatever. I can check it out before trying to apply it to a resource by using terraform console:\n❯ terraform console\n&gt; local.vm_attrs_map\n{\n  \"controller0\" = {\n    \"ip\" = \"192.168.85.70\"\n    \"name\" = \"controller0\"\n    \"node\" = 1\n    \"type\" = \"controller\"\n  }\n  \"controller1\" = {\n    \"ip\" = \"192.168.85.71\"\n    \"name\" = \"controller1\"\n    \"node\" = 2\n    \"type\" = \"controller\"\n  }\n  \"controller2\" = {\n    \"ip\" = \"192.168.85.72\"\n    \"name\" = \"controller2\"\n    \"node\" = 3\n    \"type\" = \"controller\"\n  }\n  \"worker0\" = {\n    \"ip\" = \"192.168.85.73\"\n    \"name\" = \"worker0\"\n    \"node\" = 1\n    \"type\" = \"worker\"\n  }\n  \"worker1\" = {\n    \"ip\" = \"192.168.85.74\"\n    \"name\" = \"worker1\"\n    \"node\" = 2\n    \"type\" = \"worker\"\n  }\n  \"worker2\" = {\n    \"ip\" = \"192.168.85.75\"\n    \"name\" = \"worker2\"\n    \"node\" = 3\n    \"type\" = \"worker\"\n  }\n}"
  },
  {
    "objectID": "posts/2023-02-24-k8s-the-hard-way.html#put-it-all-together",
    "href": "posts/2023-02-24-k8s-the-hard-way.html#put-it-all-together",
    "title": "Notes on Kubernetes the hard way",
    "section": "Put it all together",
    "text": "Put it all together\nNow that I’ve got my module created and my map to loop over I can finish up in main.tf in the root of this project:\nmodule \"ubuntu_vm\" {\n  source   = \"./modules/ubuntu-vm\"\n  for_each = local.vm_attrs_map\n  node     = each.value.node\n  type     = each.value.type\n  ip       = each.value.ip\n}\nNice and easy! I run terraform init again so that the module I created is loaded, then terraform plan to make sure I’m actually getting the 6 nodes I expect. Everything looks good so I run terraform apply… and wait an hour and a half for it to not actually deploy any nodes. When I initially tested terraform back when I was doing templates I did notice that it took a lot longer to deploy via terraform than via the menu, but that was minutes, not hours. Time to figure out what’s going on here.\nAs a fun aside to remember for later, as part of troubleshooting I tried updating the proxmox provider from the 2.9.11 version I was using to the 2.9.13 release and it just straight up doesn’t work. Everything installs ok but then I get:\n❯ terraform plan\n╷\n│ Error: Plugin did not respond\n│ \n│   with provider[\"registry.terraform.io/telmate/proxmox\"],\n│   on main.tf line 9, in provider \"proxmox\":\n│    9: provider \"proxmox\" {\n│ \n│ The plugin encountered an error, and failed to respond to the plugin.(*GRPCProvider).ConfigureProvider call. The plugin\n│ logs may contain more details.\n╵\nWhen I revert back to the old release I can at least run terraform plan. There are quite a few threads about how the proxmox provider for terraform is kind of buggy and I’m starting to wonder if ansible would be a better way to go. I like the ability of terraform to tear down infrastructure with terraform destroy but I’m not sure it’s worth all this other hassle. I’ll keep messing with it for a bit though.\nI found an open issue on the proxmox terraform provider about slow provisioning. There’s also this one about issues deploying multiple VMs. Both are still open but there were some suggested config changes, along with a recommendation to run in debug. Let’s try that with TF_LOG=DEBUG terraform apply --auto-approve. This dumped a giant stream of output, most of which I will not reproduce. One thing that caught my eye was that it couldn’t find the template VM I wanted to use. Looking back at my code I realized that I had missed the dashes in the template name. That’s definitely on me, although I’m going to put some blame on the provider for just hanging forever instead of returning an error.\nAfter fixing the template the playbook applied and I had 6 VMs up and running, two on each node. It took a couple minutes to apply, but that’s not bad at all. Problem solved?\nAlmost. The newly deployed VMs are up and running, and I can ssh into them at their IPs, but they don’t have qemu guest agents running so I can’t see their IPs from the proxmox UI, or load up a terminal session from there. This isn’t the end of the world, but I’d like to fix it if I can. I think the problem is that I had agent turned off in the proxmox config as part of troubleshooting the slow deploy. Let’s see if I can fix that. This will also give me a chance to confirm that terraform destroy works. The destroy worked no problem. Setting agent = 1 back in the template config worked fine in terms of creating the VM (no slowdown in deploy), but I still couldn’t load them from the proxmox UI. I created a manual clone of the same template to see if I could figure out the issue there. This one did show me the configured IP, but still wouldn’t let me open a console. After some more troubleshooting I realized this was because some changes I’d made to my proxmox ssh host keys were blocking me from bringing up terminal sessions on any hosts other than the one I was connecting to the UI through. Again, that’s totally my bad, although I could have gone for some better error messages."
  },
  {
    "objectID": "posts/2023-02-24-k8s-the-hard-way.html#provisioning-a-ca-and-generating-tls-certificates",
    "href": "posts/2023-02-24-k8s-the-hard-way.html#provisioning-a-ca-and-generating-tls-certificates",
    "title": "Notes on Kubernetes the hard way",
    "section": "Provisioning a CA and Generating TLS certificates",
    "text": "Provisioning a CA and Generating TLS certificates\nAnsible playbook\nOn to chapter 4 in the guide!\nI have to do some minor modification of the scripts outlined in the doc since I’m not using GCP. I think for these future configs I’m going to use ansible, since that’s how I’d like to actually manage hosts in the future.\nThe first big headache I ran into was getting cfssl installed to generate the certs. It didn’t have a .deb available that I could find so I had to install go and then install the package there, as well as figuring out how to make the path to the binary it installed available to my user (learned a couple things about GOPATH in the process).\nOther than that creating all the keys and copying them onto the hosts was pretty straightforward ansible. I’ll have to wait until later to see if anything broke, but for now it seems good."
  },
  {
    "objectID": "posts/2023-02-24-k8s-the-hard-way.html#generating-kubernetes-configuration-files-for-authentication",
    "href": "posts/2023-02-24-k8s-the-hard-way.html#generating-kubernetes-configuration-files-for-authentication",
    "title": "Notes on Kubernetes the hard way",
    "section": "Generating Kubernetes configuration files for authentication",
    "text": "Generating Kubernetes configuration files for authentication\nAnsible playbook\nOn to the next thing! This section uses kubectl, which I fortunately already have available in my devcontainer, so no config required there. I’ll keep going with my pattern of using ansible to manage the scripting. No issues with any of these steps, at least not at this point. I might have to come back to some of it for troubleshooting."
  },
  {
    "objectID": "posts/2023-02-24-k8s-the-hard-way.html#generating-the-data-encryption-config-and-key",
    "href": "posts/2023-02-24-k8s-the-hard-way.html#generating-the-data-encryption-config-and-key",
    "title": "Notes on Kubernetes the hard way",
    "section": "Generating the data encryption config and key",
    "text": "Generating the data encryption config and key\nAnsible playbook\nSame as the above. I did a slightly different workflow for the ansible playbook. Since this called for generating a random number as part of the config, rather than doing something fancy like registering the output of a command to generate the random number and then inserting that into a template I just wrapped the whole thing in a shell command."
  },
  {
    "objectID": "posts/2023-02-24-k8s-the-hard-way.html#rbac-for-the-kubelet-authorization",
    "href": "posts/2023-02-24-k8s-the-hard-way.html#rbac-for-the-kubelet-authorization",
    "title": "Notes on Kubernetes the hard way",
    "section": "RBAC for the kubelet authorization",
    "text": "RBAC for the kubelet authorization\nThese commands I only have to run on one node, and I’m not sure how to easily make them idempotent with ansible. They’re making changes on my cluster, not creating files (at least that I know of), so I don’t know how to tell ansible not to re-run the commands. In theory running them multiple times shouldn’t really matter, so I’ll just do it manually anyway."
  },
  {
    "objectID": "posts/2023-02-24-k8s-the-hard-way.html#front-end-load-balancer",
    "href": "posts/2023-02-24-k8s-the-hard-way.html#front-end-load-balancer",
    "title": "Notes on Kubernetes the hard way",
    "section": "Front end load balancer",
    "text": "Front end load balancer\nAgain, I don’t actually have a load balancer (maybe that will be what I do in my next post). So I’ll skip this part."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html",
    "href": "posts/2020-11-25-dotfiles.html",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "",
    "text": "This is part 3 of a 4 part series describing how I provision my systems. Links to each part are below:"
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#setup.sh",
    "href": "posts/2020-11-25-dotfiles.html#setup.sh",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "setup.sh",
    "text": "setup.sh\nThis file either links in or generates the config file for RCM (rcrc). This is the file that identifies which tags are applicable for the machine so it has to be configured properly before the rest of the dotfiles can be brought up. The base implementation checks what Operating System you’re running and adds tags for that. At work I have it generate additional tags for which user is running it so I can create user specific tagged files (for things like email addresses).\nAfter running this script, if we didn’t link in an already existing .rcrc file then you’ll have a host specific one generated, but it won’t be saved in the repository, it will just be a reglar file. If as prompted you run mkrc -o ~/.rcrc it will add a host specific rcrc file to the repository."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#base-rcrc",
    "href": "posts/2020-11-25-dotfiles.html#base-rcrc",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "base-rcrc",
    "text": "base-rcrc\nThis file is used by setup.sh to generate the host specific ~/.rcrc. The script adds tags to this file based on the operating system you’re running. You can add additional tags if you’d like."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#other-root-files",
    "href": "posts/2020-11-25-dotfiles.html#other-root-files",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "other root files",
    "text": "other root files\nThe other files in the root of the repository are generic repository management files. README.md will show on the base of the page on GitHub and should point back to this blog post for more details. I picked GPL V3 for the license somewhat arbitrarily. I think I used the GitHub license picker helper for it. .gitignore and .gitattributes handle files for git to ignore and enforce consistent line break characters. .editorconfig tells a variety of text editors things like whether to use tabs or spaces for indentation."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#completions",
    "href": "posts/2020-11-25-dotfiles.html#completions",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "completions",
    "text": "completions\nThese scripts let you tab complete commands for certain applications. At the time of this writing I have completions for git, pipx and poetry installed."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#distrosmanjaroaliases",
    "href": "posts/2020-11-25-dotfiles.html#distrosmanjaroaliases",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "distros/manjaro/aliases",
    "text": "distros/manjaro/aliases\nI don’t actually use manjaro, but I wanted to keep this in as an example for myself of how to set distribution specific functionality."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#linux",
    "href": "posts/2020-11-25-dotfiles.html#linux",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "linux",
    "text": "linux\nThis has a few commands to set start or open to run xdg-open in linux. Makes the syntax compatible against platforms. That would be for opening a file in a gui rather than with a command line app."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#macos",
    "href": "posts/2020-11-25-dotfiles.html#macos",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "macos",
    "text": "macos\nI don’t have any mac machines to test this stuff out on right now. It’s got a few files that presumably help make behaviour consistent on macs."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#windows-wsl",
    "href": "posts/2020-11-25-dotfiles.html#windows-wsl",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "windows-wsl",
    "text": "windows-wsl\nSimilar to the mac and linux entries above. Lets you use the same commands regardless of your specific platform."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#nerdfonts",
    "href": "posts/2020-11-25-dotfiles.html#nerdfonts",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "nerdfonts",
    "text": "nerdfonts\nThis maps a bunch of nerd fonts to environment variables so they can be included in shell scripts. It lets you do things like put a check mark in your command prompt. Very important stuff."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#shared",
    "href": "posts/2020-11-25-dotfiles.html#shared",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "shared",
    "text": "shared\nThis is where the bulk of the content is in the bash directory. All of these files are cross platform and should work the same on linux, mac or WSL.\n\naliases\nBasically these are all the command shortcuts. For example alias grep=\"grep --color\" means you can just type grep but get nicely coloured results.\n\n\nexports\nThis is where environment variables are set. For example EDITOR=vim is set here.\n\n\nfunctions\nThis is where user defined functions/tools live. For example extract is defined here to call the appropriate underlying app to extract a file based on its extension.\n\n\noptions\nSets a bunch of shell options. Things like turning on vi mode for the command line.\n\n\nother\nA catch all. Code to set up conda, manage the file path, and actually set the appearance of my command prompt all live here.\n\n\nthird party\nA place to dump cool code snippets you found on the internet that you want to be able to manage in your shell."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#bash_logout",
    "href": "posts/2020-11-25-dotfiles.html#bash_logout",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "bash_logout",
    "text": "bash_logout\nClear the screen when you log out. I’m not sure if I actually need this, doesn’t seem to hurt"
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#bash_profile",
    "href": "posts/2020-11-25-dotfiles.html#bash_profile",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "bash_profile",
    "text": "bash_profile\nI’m sure in theory there’s a difference between this file and .bashrc but in practice they seem to be the same. Just map this one to load ~/.bashrc so whichever one your terminal expects you get the same result."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#bashrc",
    "href": "posts/2020-11-25-dotfiles.html#bashrc",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "bashrc",
    "text": "bashrc\nbashrc configures your shell on login. Brennan has a nice modular design that I’m going to emulate. Basically nothing goes in bashrc itself, rather it walks through all the folders in the previously described bash folder and adds them in (at least those relevant to your Operating System). A snippet of what that looks like is below.\n# We want to walk \"outside\" in... which is to say run all options files first, then all\n# exports, then all functions, etc.\nfor folder in \"options\" \"exports\" \"functions\" \"third-party\" \"other\" \"aliases\"; do\n  for base in \"shared\" \"$OS_PRIMARY\" \"distros/$OS_SECONDARY\"; do\n    for root in \"$DOTFILES/bash\" \"$DOTFILES_PRIVATE/bash\"; do\n      if [[ -d \"$root/$base/$folder\" ]]; then\n        for file in $root/$base/$folder/*.bash; do\n          # shellcheck source=/dev/null\n          source \"$file\"\n        done\n      fi\n    done\n  done\ndone\nAll the actual functionality lives in the bash folders of the dotfiles repositories and only this file itself needs to be linked in by RCM. Distribution and OS specific functionality can be managed by just placing the script in the appropriate folder. Because of the order of execution the more granular files will overwrite more general settings if there’s a conflict."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#other-files",
    "href": "posts/2020-11-25-dotfiles.html#other-files",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "other files",
    "text": "other files\n\ndircolors: make ls show pretty colours.\ngitignore: files and patterns to ignore in all git repositories\ninputrc: manage basic keyboard mappings for the shell (home to go to the beginning of the line for example)\nprettierrc: configurations for the code formatter prettier. Kind of like black for other languages\ntmux.conf: configuration for tmux. I don’t use tmux enough to have strong opinions about these commands so the commenting is pretty sparse at the time of this writing"
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#config-folder",
    "href": "posts/2020-11-25-dotfiles.html#config-folder",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "config folder",
    "text": "config folder\nPolite applications store their configuration files here rather than your home directory. The Arch Wiki has a good list of polite applications and how to override some of the impolite ones. The folders all correspond to the name of the application they configure (e.g. git) so they layout is pretty self explanatory."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#host--folders",
    "href": "posts/2020-11-25-dotfiles.html#host--folders",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "host-* folders",
    "text": "host-* folders\nHost specific configs. Everything within here will have the same layout as the rcs folder above it, but will have machine specific configs. For my setup that’s just the ~/.rcrc file that sets the tags for everything else on the machine."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#tag--folders",
    "href": "posts/2020-11-25-dotfiles.html#tag--folders",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "tag-* folders",
    "text": "tag-* folders\nThe same idea as hosts, except each host can have multiple tags. In general this is used for OS specific configurations. At work I also add tags for each user on the system for things like configuring e-mail addresses."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#vifm-and-vim",
    "href": "posts/2020-11-25-dotfiles.html#vifm-and-vim",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "vifm and vim",
    "text": "vifm and vim\nThese folder should really be under config. They’re just the settings for vim and vifm. Rude of them to demand their own space in ~."
  },
  {
    "objectID": "posts/2022-12-30-rootless-docker.html",
    "href": "posts/2022-12-30-rootless-docker.html",
    "title": "Setting up rootless docker",
    "section": "",
    "text": "This post covers something that I did for work that I thought might be of more general interest. We have a number of developers that want to build and work in docker containers. Due to security constraints we can’t install WSL and docker desktop on our laptops, so some remote development solution is required. We don’t have kubernetes or anything fancy in our environment, so the initial solution was to just provision an Ubuntu VM and give everyone a login and access to the docker group. Unfortunately there are some major usability and security issues with this approach. Basically, since giving access to docker is essentially giving root access to the machine, developers can see and access each other’s containers and files. From a security perspective this is obviously no good. Even from a usability perspective it means there’s constant risk of inadvertently tripping over another developer’s work, and cleaning up your old images and containers has to be done with a lot more care. To address this issue I decided to look into rootless docker.\nI’m going to try a different approach with this blog. The next section will be a lightly edited transcription of all the things I tried (including failures, dead ends, and dumb mistakes). The following section will be a TLDR summary of what you actually need to do to get this working."
  },
  {
    "objectID": "posts/2022-12-30-rootless-docker.html#host-configuration",
    "href": "posts/2022-12-30-rootless-docker.html#host-configuration",
    "title": "Setting up rootless docker",
    "section": "Host configuration",
    "text": "Host configuration\nOfficial documentation on rootless docker config can be found here\nFor further discussion of the user namespace remapping (which explains why users should be root within the devcontainer and what /etc/subuid and /etc/subgid are doing) see the official docs here\n\nAll testing was done on an Ubuntu VM (specifically 22.04.1 LTS). As most development activity occurs within docker, most of these instructions will hopefully survive a newer Ubuntu release, and could probably even be applied to an entirely different distro if for some reason we wanted to do that. CPU, RAM and disk requirements will largely depend on the size of the team and their activity, but note that docker images tend to take up a fair bit of space, and due to (intentional) isolation of docker runtimes between users there will be no sharing of image layers. Thus 5 users each using a 2GB docker image will take up a total of 10GB of space.\nJoin the machine to the domain\n#Setting up AD Authentication\nsudo apt install -y realmd libnss-sss libpam-sss sssd sssd-tools adcli samba-common-bin oddjob oddjob-mkhomedir packagekit\n#Discovery\nsudo realm discover &lt;domain&gt;\n#Adding to the domain (enter password when prompted)\nsudo realm join -U &lt;privileged domain user&gt;\n#Adding domain user to allow ssh\nrealm permit -g groupname@domainname\nInstall docker enginer as per the official docker docs. Note, do not use the included docker package in the Ubuntu base repository.\nMake sure the system docker daemon is not running and disable it if it is: sudo systemctl disable --now docker.service docker.socket\nInstall autofs and cifs-utils to allow users to mount network shares with their credentials.\nFor each user that will be running docker on this machine, create an entry in /etc/subuid and /etc/subgid (the entry in each file should look the same)\n\neach entry in each file has the format &lt;username&gt;:&lt;baseuid&gt;:65536. &lt;baseuid&gt; starts at 100000 for the first entry and increases by 65536 for each subsequent entry. For example, here’s what /etc/subuid looks like on the machine where this guide was tested:\n\nadmin:100000:65536\ndockertest:165536:65536\ndockertesta:231072:65536\ndockertestb:296608:65536\n&lt;user&gt;:362144:65536\n\nLocal user accounts seem to get their entries auto-generated correctly, but at least in testing, domain joined user accounts had to be manually created.\n\nFor each user that will be running docker in this machine, create an entry in /etc/passwd that specifies their default shell as bash. Otherwise VS code will not be able to figure out the user level docker socket it should attach to.\n\ngetent passwd &lt;user&gt; | sudo tee -a /etc/passwd\n\nCreate a base user folder to mount network shares for each user in /mnt/&lt;user&gt;, make that user the owner of that folder and lock down access to that user (chown &lt;user&gt; and chmod 700 /mnt/&lt;user&gt;).\nFor each user that will be running docker from this machine, create a line in /etc/auto.master in the following format:\n/mnt/&lt;user&gt; /etc/auto.sambashares-&lt;user&gt; --timeout=30 –ghost\nPopulate /etc/auto.sambashares-&lt;user&gt; with a line for each network share that user has to access as follows:\n&lt;localsharename&gt; -fstype=cifs,rw,sec=krb5,uid=${UID},cruid=${UID} :&lt;full share path&gt;\nWhere &lt;localsharename&gt; is the name of the folder under /mnt/&lt;user&gt; that the share will be mounted to, and &lt;full share path&gt; is the path to the SMB file share."
  },
  {
    "objectID": "posts/2022-12-30-rootless-docker.html#user-configuration",
    "href": "posts/2022-12-30-rootless-docker.html#user-configuration",
    "title": "Setting up rootless docker",
    "section": "User configuration",
    "text": "User configuration\nMost configuration of the VM should have been completed by its system administrator, but there are a couple user level tasks you will have to run before you can work with docker.\n\nInstall the rootless docker daemon\ndockerd-rootless-setuptool.sh install\n\n\nSet an environment variable for the Docker socket\nAdd the following line to ~/.bashrc: DOCKER_HOST=unix:///run/user/$(id -u)/docker.sock\n\n\nLog out and back in and test\ndocker run hello-world\n\n\nSet up network shares\nAttaching network shares cannot be done directly by the user. System administrators provision network drives for each user under /mnt/&lt;user&gt;. If the network share you want is not there, contact your system administrator with its information and they will add it."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html",
    "href": "posts/2021-12-30-wheretolive.html",
    "title": "Building a where to live app",
    "section": "",
    "text": "To start, here’s the code. I’ll include more specific links to specific parts of the process in detail below.\nI have two goals with this project:\n\nFigure out a good place to live when I move next\nLearn some data engineering and system administration type skills\n\nFor the first goal, I want to scrape real estate sites in my area and assemble a database of listings. I want to supplement that with open data from the city and other sources. I want all of this data to be collected and updated in an automated and efficient process. Finally, I want to be able to analyze this data in order to find the best place to live based on my personal preferences and requirements.\nThe second goal should come about as a consequence of the first. I’ve done web scraping before, but mostly for one off tasks where I can babysit if my results look weird. To store the data that I scrape I’ll use a database. I’ve done lots of querying of databases, but I haven’t had much opportunity to design one, so this will be a learning experience in the regard. I’ll also need to have an ETL pipeline to manage the scheduling, ingestion, and other tasks between the scraper and the database. Finally, I’ll need some way to serve the recommendations."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#too-much-upfront-validation",
    "href": "posts/2021-12-30-wheretolive.html#too-much-upfront-validation",
    "title": "Building a where to live app",
    "section": "Too much upfront validation",
    "text": "Too much upfront validation\nMy first instinct when ingesting data from a source I didn’t control (the API endpoints for rentfaster.ca and realtor.ca) was that I should do a bunch of cleaning and validation as early as possible, which would allow all of my downstream data processing steps to remain clean. On the plus side I got to learn a bit about how to use fastapi and pydantic. On the much larger down side, this approach meant that if I wanted to modify any of the filtering I was applying, or if there were unanticipated parsing errors (people put the weirdest stuff in the square footage field) there was no possible recovery. In the final implementation I downloaded results in the most raw format I could manage. While the uncompressed data was a little larger than I wanted to be dealing with daily, it compressed down to very manageable sizes. Separating extraction from any sort of filtering or processing was definitely the right call."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#trying-to-learn-this-and-cloud-at-once",
    "href": "posts/2021-12-30-wheretolive.html#trying-to-learn-this-and-cloud-at-once",
    "title": "Building a where to live app",
    "section": "Trying to learn this and cloud at once",
    "text": "Trying to learn this and cloud at once\nSince one of the goals of this project was learning, I fairly early on got the idea in my head that I should try doing this whole process “cloud native” on the “modern data stack”. I’d read a fair bit about these technologies, but hadn’t had the opportunity to implement much in them. In theory, the cool thing about the cloud is that everything is pay as you go, so for a relatively small data project like I had in mind, the costs should have been manageable and the learning curve shouldn’t have been insurmountable. In practice this turned out to be incorrect. First, trying to learn how to solve a specific problem at the same time as learning to use a general technology really compounds the difficulty of both. I did manage to learn a lot about creating and deploying Azure Functions but due to some issue that I still don’t fully understand I also managed to rack up a sizable cloud bill. It had something to do with a queue function getting stuck and reprocessing a message repeatedly rather than failing. I learned a very hard lesson about setting up cost alerts thanks to this. In a future project I’d like to reimplement this or a similar project in the cloud, as it is still a skillset I’d like to develop, but I will definitely do as much locally as I can before migrating to the cloud, rather than trying to prototype something there directly, at least until I get more experience."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#setting-up-my-environment",
    "href": "posts/2021-12-30-wheretolive.html#setting-up-my-environment",
    "title": "Building a where to live app",
    "section": "Setting up my environment",
    "text": "Setting up my environment\nOne of the most important, but also annoying, aspects of any project is configuring and managing your environment. Most of my custom built logic was in python, so I built a poetry project. On top of python there was a lot of adjacent infrastructure to manage. For one thing, even though I wasn’t using the cloud, I still had information I wanted to leverage but keep private (namely addresses and API keys), as well as other services that I needed to have up and running. To coordinate all of this I used ansible. Specifically I kept my secrets using ansible-vault. From the vault I could either use a .env file to load data in with python-dotenv or use them directly in a playbook (for example, to set my database password). You can see the playbook I used here and there’s some related errata at the root of that repository."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#scraping-the-listings",
    "href": "posts/2021-12-30-wheretolive.html#scraping-the-listings",
    "title": "Building a where to live app",
    "section": "Scraping the listings",
    "text": "Scraping the listings\nThere are two listings sources I’m interested in. realtor.ca for sales listings and rentfaster.ca for rental listings. That’s not going to be 100% comprehensive but in my experience it will cover the vast majority of listings.\nThe pattern for the initial scrape of both was very similar. Both sites have an endpoint that you can query to get a result back in JSON. There were a few examples online on GitHub that I was able to base mine on. In each case the endpoint has a limit on the number of results that it will return at one time, so I needed to find a way to iterate through. In the case of rentfaster it was easy, since it returned search results with a page number associated. For a given query I could start at page 1 and increment my page number until I had an empty result set. After each query I dumped the JSON to a raw date stamped folder. For realtor.ca it was a little trickier, as there was no automatic chunking. It did allow a price range though, so I picked a very high price ceiling, and then incremented my price floor to be the highest price seen in the previous result until I got an empty result back.\nThe end result of each of these scrapes was a date stamped folder for each containing zipped JSON files of the raw results from the endpoint. You can find the scraping code for realtor.ca here and for rentfaster here."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#parsing-the-listings",
    "href": "posts/2021-12-30-wheretolive.html#parsing-the-listings",
    "title": "Building a where to live app",
    "section": "Parsing the listings",
    "text": "Parsing the listings\nAfter downloading the raw listings data, the next step was to process and format it into something I’d want to consume. This was pretty tedious, but it’s a critical part of any data project. Lots of validating and transforming of various fields. I won’t go into the details here, but the code for parsing realtor.ca is here and for rentfaster here. As the final stage of parsing any given day I would write a pandas DataFrame out to parquet in a folder along with the compressed raw files. This setup made it easy to read in cleaned up data, while still giving me the flexibility to go back and modify my data cleaning process as necessary on historical results."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#storing-all-the-data",
    "href": "posts/2021-12-30-wheretolive.html#storing-all-the-data",
    "title": "Building a where to live app",
    "section": "Storing all the data",
    "text": "Storing all the data\nI probably could have done basically everything I needed to do for this project in pandas, or at least geopandas, but it didn’t seem like the most elegant solution, and I wanted to learn some stuff. With those two criteria in mind I went with a PostgreSQL using PostGIS to handle the geospatial aspects of the data (location being very important in selecting where to live after all). I deployed the database itself in a docker container using ansible to manage the deployment. I also wrote a small wrapper script to make it easier to connect to the database from python using sqlalchemy. The wrapper code is here."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#ingesting-listings-data-in-postgis",
    "href": "posts/2021-12-30-wheretolive.html#ingesting-listings-data-in-postgis",
    "title": "Building a where to live app",
    "section": "Ingesting listings data in PostGIS",
    "text": "Ingesting listings data in PostGIS\nThe last thing that needed to happen with the listings themselves was getting them into the database. First I created a table for each of rentfaster and realtor.ca in the final format I wanted. Here’s the sql used to create the realtor.ca one for example. With that created I used pandas and sqlalchemy to push the cleaned data into a staging table (no need to predefine this since it’s getting wiped each time and pandas can handle table creation). Once the data was up in staging I would do a few additional calculations, like turning the latitude and longitude records into PostGIS Points before moving the data into the final table. I also would update a materialized view of listing data joined to some other data sets at this point, but I haven’t talked about the other data yet so I’ll cover that later. Here’s an example of the ingestion script."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#adding-in-commute-data",
    "href": "posts/2021-12-30-wheretolive.html#adding-in-commute-data",
    "title": "Building a where to live app",
    "section": "Adding in commute data",
    "text": "Adding in commute data\nOne of the most critical things in terms of choosing where to live is how easy it is to get places from it. This was one of the key pain points that made me think to develop this project in the first place. Plugging a candidate location into google maps and then interating through commute times to various important locations (downtown, work, family) is quite tedious. To make this easier I wanted to compute isochrones for various transit modes and locations. I initially looked at Azure maps for this. They have a built in method for isochrones, which I got working. Unfortunately it wasn’t very granular in terms of the isochrones it produced, and it didn’t support public transit data at all.\nFortunately, I learned about an amazing project called Open Trip Planner that was exactly what I needed. It was definitely more work to set up, but the results were way better than I could get through Azure. Open Trip Planner doesn’t include any maps or transit information out of the box, so I had to set that up. I used this script to grab a map of my region from OpenStreetMap, supplemented it with detailed transit commute information for my city with this script and finally even added in some elevation data so that walking and cycling commute times would be more accurate from This government of Canada page. I couldn’t automate that last part at all as I had to queue up for my data request and then retrieve it from a personalized email link. Oh well.\nOnce I had OpenTripPlanner up and running (again, in a docker container) I was able to use the API it provided to compute isochrones of various time ranges, transit modes, and locations using this script (it still has the Azure maps code in it even though I didn’t end up using that if you’re curious).\nThe output of that API was saved to JSON files, and then ingested into PostGIS using this script.\nFinally, I needed some way to associate this isochrone data with all the listings I was saving. I wanted columns that would easily let me filter on things like “Is this more than a 30 minute walk/transit trip from downtown?”. Between the different transit modes (walk, cycle, transit, drive, plus combinations), time ranges (I did 5 minute intervals between 10 and 60 minutes) and finally locations of interest I had a lot of possible ways to slice the data. While I could have hand written a giant SQL statement that would create them all, that would have been very boring to do, error prone, and also required significant rework if I changed any of my criteria. Instead I did some hacky string manipulation in python to construct the various components of my query and then stuck it together to create a view in PostGIS that associated each listing with all the transportation related attributes I might be interested in. Here’s what that looks like for realtor.ca."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#adding-in-grocery-store-data",
    "href": "posts/2021-12-30-wheretolive.html#adding-in-grocery-store-data",
    "title": "Building a where to live app",
    "section": "Adding in grocery store data",
    "text": "Adding in grocery store data\nWhile commute time to various places is certainly important for location, another factor is nearby amenities. Specifically I was asked if I could include the nearest grocery store. For this I used the FourSquare API. Similar to the initial scraping above, I had some issues with chunking here. The FourSquare API only returns a maximum of 50 results, and there are (a few) more than 50 grocery stores in all of Calgary. One thing the API lets you specify is a NE and SW corner to define a rectangle to search within. I took advantage of that and numpy’s linspace method to chunk the city into many boxes, query for grocery stores in each of them, and combine the result. The scraping code is here. The results are a little messy. There are several locations that FourSquare considers a grocery store that I would disagree with. It hasn’t been enough of an issue to bother with, but between when I save the raw FourSquare results and when I upload the data into PostGIS (here) I could easily (but tediously) add in a step that drops the locations that I don’t want to consider as grocery stores.\nOnce the grocery store data is in the database I create a table that has a row for each listing, its nearest grocery store, and the distance in meters to that grocery store. This is just straight line distance and doesn’t consider commute time, but it’s fast to compute, gives a good idea, and doesn’t make me run every listing and every grocery store through OpenTripPlanner daily. That seemed like a reasonable tradeoff to me."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#adding-flood-zone-data",
    "href": "posts/2021-12-30-wheretolive.html#adding-flood-zone-data",
    "title": "Building a where to live app",
    "section": "Adding flood zone data",
    "text": "Adding flood zone data\nAnother thing I want to consider when choosing where to live is climate resiliency. Calgary experienced a very significant flood less than a decade ago, and I would like to avoid living somewhere likely to be impacted by a similar event in the future. To manage this, I grabbed some flood risk data from the City of Calgary Open Data Portal and ingested it into PostGIS (here). From that I could create a table that checked if any given listing was in the 1 in 20 or 1 in 100 year flood zones as defined by the city (here)."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#combining-the-results",
    "href": "posts/2021-12-30-wheretolive.html#combining-the-results",
    "title": "Building a where to live app",
    "section": "Combining the results",
    "text": "Combining the results\nAt this stage in the write up I have a table with listings and their details, as well as some views that have a foreign key identifying the listing, along with some other specific attributes (closest grocery store, flood zone status, commute details). Creating those views actually takes an appreciable amount of time (not massive, but the commute one for example is a solid 10 seconds). What I want to build off the combination of all these tables is a filtered list of just the listings that match my criteria. Both because I want to be able to iterate on my criteria quickly, and because I’m building similar criteria list for a few other people who are interested in finding a place to live, I don’t want to have to recompute all those queries every time I want to change something or need to find candidates for a new person. To manage this, I created a materialized view of all the data sets joined together (here’s the realtor.ca table for example). After I ingest a new day of listings I can refresh this materialized view, and then have quick access to all my updated criteria for current listings."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#creating-candidate-lists",
    "href": "posts/2021-12-30-wheretolive.html#creating-candidate-lists",
    "title": "Building a where to live app",
    "section": "Creating candidate lists",
    "text": "Creating candidate lists\nThe next piece is filtering down all of the possible listings to just the ones that I might actually want. I did this by making views on top of the wide table described above that applied whatever filter criteria I wanted, along with only returning a subset of the available columns that I’d want to see in advance before investigating a listing further. Here’s the code for making candidate views for realtor.ca for example."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#sharing-the-candidates",
    "href": "posts/2021-12-30-wheretolive.html#sharing-the-candidates",
    "title": "Building a where to live app",
    "section": "Sharing the candidates",
    "text": "Sharing the candidates\nNow to make the candidate listings accessible. To make it easier for me, and possible for others, I export the listings daily to Dropbox. This part of the process was actually delightfully easy. I made some minor modifications to the example code on the Dropbox page and then used pandas to_html method to push up a table of listings. From there I could use regular Dropbox functionality to share personalized folders with people interested in particular listings candidates. If I was trying to do this as an actual application I’d obviously need a more robust solution, but for myself and a couple other people this worked perfect. The basic dropbox export code is here and the actual listings upload code is here."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#scheduling-things",
    "href": "posts/2021-12-30-wheretolive.html#scheduling-things",
    "title": "Building a where to live app",
    "section": "Scheduling things",
    "text": "Scheduling things\nNow that I have all the components of the pipeline set up I need to automate it. I was tempted to go with something cool for this like airflow or dagster but it didn’t seem worth the complexity. I ended up adding a task to my ansible playbook to schedule cron jobs for realtor.ca and rentfaster listings. The script cron runs looks like this."
  },
  {
    "objectID": "posts/2023-06-04-odbc-drivers-ansible.html",
    "href": "posts/2023-06-04-odbc-drivers-ansible.html",
    "title": "Install Microsoft ODBC drivers with ansible",
    "section": "",
    "text": "Introduction\nThis is a quick note on how to set up Microsoft ODBC drivers using ansible. Most of it is quite trivial, but you can run into issues with the Microsoft repository version of dotnet conflicting with the one from the base Ubuntu repository, and this playbook addresses that.\n\n\nHow to do it\n- name: Install odbc pre-requisites\n  ansible.builtin.apt:\n    pkg:\n      - lsb-release\n\n- name: Add microsoft gpg key\n  ansible.builtin.shell: |\n    install -m 0755 -d /etc/apt/keyrings && \\\n    curl -fsSL https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor -o /etc/apt/keyrings/microsoft.gpg && \\\n    chmod a+r /etc/apt/keyrings/microsoft.gpg\n  args:\n    creates: \"/etc/apt/keyrings/microsoft.gpg\"\n\n- name: add Microsoft repository to apt\n  apt_repository:\n    repo: \"deb [arch=amd64,armhf,arm64 signed-by=/etc/apt/keyrings/microsoft.gpg] https://packages.microsoft.com/ubuntu/{{ansible_distribution_version}}/prod {{ansible_distribution_release}} main\"\n    state: present\n\n- name: Prioritize Microsoft repo so you don't end up with dotnet conflicts if you need it later\n  ansible.builtin.copy:\n    src: \"99-microsoft-dotnet.pref\"\n    dest: \"/etc/apt/preferences.d/99-microsoft-dotnet.pref\"\n\n- name: Install odbc drivers\n  ansible.builtin.apt:\n    pkg:\n      - msodbcsql18\n  environment:\n    ACCEPT_EULA: \"Y\"\nThe 99-microsoft-dotnet.pref file is simple and looks like this:\nPackage: *\nPin: origin \"packages.microsoft.com\"\nPin-Priority: 1001\n\n\nConclusion\nI’m not going to do a bunch of exposition on this. If you’re in the very specific circumstance of needing to install Microsoft ODBC drivers with ansible I hope this helps."
  },
  {
    "objectID": "posts/2023-11-12-xcp-ng.html",
    "href": "posts/2023-11-12-xcp-ng.html",
    "title": "Checking out XCP-NG",
    "section": "",
    "text": "I have enjoyed learning proxmox, and haven’t run into any deal breakers with it, but I also like to see if the grass is greener. After watching some videos from Tom at Lawrence Systems I decided xcp-ng might be more to my liking. This post will document what I did to get it set up and my thoughts on using it."
  },
  {
    "objectID": "posts/2023-11-12-xcp-ng.html#figuring-out-how-to-add-vm-storage",
    "href": "posts/2023-11-12-xcp-ng.html#figuring-out-how-to-add-vm-storage",
    "title": "Checking out XCP-NG",
    "section": "Figuring out how to add VM storage",
    "text": "Figuring out how to add VM storage\nThe error that I see in the log is SR_BACKEND_FAILURE_77(, Logical Volume group creation failed, ) The first result takes me to a reddit thread that suggests this is an issue because I’d previously used this drive for zfs, which I had when it was running proxmox. Again from the terminal and following this guide wipefs /dev/nvme0n1* does return some zfs_member tags.\nAfter a little messing around I did a couple runs of wipefs -o &lt;offset of ZFS tag&gt; /dev/&lt;device or partition of device&gt; -f to get rid of the tags, I ran the command with the -n flag first to make sure it was removing the right tag.\nAfter doing that I got the same error. There’s either more tags I need to remove (the original post just dropped everything, I tried to be more surgical), or I need a reboot. Let’s try the reboot first. Reboot didn’t fix it. Let’s try the full wipe on both the device and its partitions. If that breaks something I’ll do a reinstall anyway, not like I’ve put much on this box at this point. That worked! wipefs -a -f /dev/nvme0n1 and wipefs -a -f /dev/nvme01p3 allowed me to create a SR for VM images. Well on my way now. DON’T ACTUALLY JUST DO THIS, SEE BELOW"
  },
  {
    "objectID": "posts/2023-11-12-xcp-ng.html#back-to-adding-a-vm",
    "href": "posts/2023-11-12-xcp-ng.html#back-to-adding-a-vm",
    "title": "Checking out XCP-NG",
    "section": "Back to adding a VM",
    "text": "Back to adding a VM\nThe rest of the setup is pretty straightforward. I’m able to give the VM a disk in the storage location. There’s a bunch of advanced features that I’m skipping for now, but will definitely have to come back to at some point for future builds.\nAfter that the machine auto started and I was able to walk through the graphical installer from the “Console” tab on the VM. I saw some things in the docs about setting up VNC for a better full screen experience, but that’s definitely a step for a later date as well.\nThe installation completed and at the end I had a running Debian install."
  },
  {
    "objectID": "posts/2023-11-12-xcp-ng.html#have-to-reinstall",
    "href": "posts/2023-11-12-xcp-ng.html#have-to-reinstall",
    "title": "Checking out XCP-NG",
    "section": "Have to reinstall",
    "text": "Have to reinstall\nAfter I got the Debian VM set up I figured this was a good time to update the XCP-NG host to a static lease and give it a reboot. Unfortunately in doing that I discovered that my wipefs exploits had removed the boot flags from my drive as well. I’m sure there would be some clever way to carefully stitch those labels back on but I really don’t feel like it. Reinstall XCP-NG, add it back to the XO that’s still running in a container on my workstation. Interestingly my local VM storage repository showed up on a fresh install, albeit without any disks. Maybe that’s how it’s supposed to work by default if you don’t have any flags on your extra partition that blocks it. I recreate the ISO SR but just put it in /media/ this time since that’s what the docs suggest and I don’t have to create a new folder. Still have to make some shared folders on my NAS to support this better eventually. I really don’t like how you add ISOs. I want to be able to add it from the SR page but instead I have to go to “Import” on the sidebar. I’m sure I’ll figure that out eventually but right now I find it quite unintuitive. After getting that going it’s relatively quick to get the VM reinstalled. Let’s try this again."
  },
  {
    "objectID": "posts/2023-11-12-xcp-ng.html#management-tools",
    "href": "posts/2023-11-12-xcp-ng.html#management-tools",
    "title": "Checking out XCP-NG",
    "section": "Management tools",
    "text": "Management tools\nMaybe not the obvious place to start, but I really like it when VM info is integrated into my UI. Looking at this machine I see “Management agent not detected”. For Ubuntu according to the docs I’d install xe-guest-utilities, but I can’t find that in debian. Looking a little further down the docs I see how to mount and install from the guest utilities ISO that comes with my setup. That seems to work fine. After a reboot I can see all my information."
  },
  {
    "objectID": "posts/2023-11-12-xcp-ng.html#paste-to-console",
    "href": "posts/2023-11-12-xcp-ng.html#paste-to-console",
    "title": "Checking out XCP-NG",
    "section": "Paste to console",
    "text": "Paste to console\nAs little as I’m planning to use the virtual console in the web ui, I really do want to figure out how to paste into it. ctrl+c, ctrl+shift+ins etc do not seem to be working, nor is just right clicking for paste. From this post it’s not likely to be fixed in this version. From a look at the roadmap it’s in development (XO 6 that is) but I don’t see a release date and there’s nothing available for preview. I’ll stick with it for now. I think that was also the case for dark mode."
  },
  {
    "objectID": "posts/2023-11-12-xcp-ng.html#check-the-log-file-for-issues",
    "href": "posts/2023-11-12-xcp-ng.html#check-the-log-file-for-issues",
    "title": "Checking out XCP-NG",
    "section": "Check the log file for issues",
    "text": "Check the log file for issues\nChecking the logs it looks like a URL issue:\nAn unexpected error occurred: \"https://registry.yarnpkg.com/css-parse/-/css-parse-2.0.0.tgz: Request failed \\\"502 Bad Gateway\\\nI seem to be able to download that fine from my workstation, I wonder if it was just an intermittent failure? Let’s give it one more run before we get too heavy into troubleshooting:\nWARNING: free disk space in /opt seems to be less than 1GB. Install/update will most likely fail\nDid I just not make a big enough virtual disk for this? Let’s take a look. Oh yeah, root is almost entirely full. I didn’t really look at the default partition sizing but it gave almost all the disk to /home and left basically nothing. Do I want to resize or just reinstall? From a bit of reading I can’t resize active volumes, so I guess we’re doing a reinstall. What a fun learning experience! This is why I like having OS templates and automation for everything."
  },
  {
    "objectID": "posts/2023-11-12-xcp-ng.html#have-to-reinstall-1",
    "href": "posts/2023-11-12-xcp-ng.html#have-to-reinstall-1",
    "title": "Checking out XCP-NG",
    "section": "Have to reinstall",
    "text": "Have to reinstall\nAt least I have the presence of mind to manually give this new VM the same MAC as my old one so the static lease for it will persist. I’m only going to give it 10GB this time, but I’m going to be a lot more sensible about my allocation of storage, basically just one giant partition. After the reinstall I mount the guest utils ISO, reinstall guest utils and reboot. Reinstall git and vim. Clone the repo, copy over the template file, don’t bother editing it since I didn’t have to last time. Run the installer. This time it worked, must have just been a disk space problem last time."
  },
  {
    "objectID": "posts/2023-11-12-xcp-ng.html#test-it-out",
    "href": "posts/2023-11-12-xcp-ng.html#test-it-out",
    "title": "Checking out XCP-NG",
    "section": "Test it out",
    "text": "Test it out\nAfter a reboot I head to the address of my VM and there it is! I have to re-add my host but that works easily enough and now I can see my VM that’s running the orchestration server I’m looking at it through. How meta."
  },
  {
    "objectID": "posts/2023-11-12-xcp-ng.html#host-migration",
    "href": "posts/2023-11-12-xcp-ng.html#host-migration",
    "title": "Checking out XCP-NG",
    "section": "Host migration",
    "text": "Host migration\nI don’t want to set up any of my hosts into a pool. I technically could with quite a few of my machines as they’re all the same gen of HP Prodesk. But I’ve got one newer gen Intel and will be adding an AMD machine in a bit. Anything I test I want to at least theoretically work across these hosts, so doing it without pools is more representative.\nOnce my VM was booted up I made a “hello world” test text file in the home directory and hit the migrate button. I was able to choose the storage and network on the remote host, even though it was on another pool. Over on the tasks tab I could see the migration happening. It took a few minutes since it had to move the whole disk plus RAM over and I only have a gigabit connection at home, but after that time I could see the same VM running exactly as it had been on the new host, so that’s pretty slick!"
  },
  {
    "objectID": "posts/2023-11-12-xcp-ng.html#backups-and-snapshots",
    "href": "posts/2023-11-12-xcp-ng.html#backups-and-snapshots",
    "title": "Checking out XCP-NG",
    "section": "Backups and snapshots",
    "text": "Backups and snapshots\nArguably even more important than migration is a good backup and restore experience. To test this I need to create a “remote” in XO, which is fortunately not associated with a particular pool, so I only have to create it once. On the same shared NFS folder as I put the ISOs I make another for backups. On the XO sidebar I head to settings then Remotes. From there I just need to put in the server and path info and I’ve got a remote created. It even does a little speed test that shows I can write at about 100 MiB/s and read at 1.59 GiB/s. The write seems plausible since it’s a spinning disk setup, the read seems high though. At least I’m connected.\nInitially my first thought was to make a backup of my VM, but when I got there I noticed I could set up backups for XO config and my pool metadata. Since I’m actually doing stuff there too I decided to schedule a daily backup of that first.\nAfter that it’s back to doing some backups of the VM itself. I decided to give it a tag test so that I could use smart backup for my configs. It’s overkill when I’m only doing one VM but I want to get in the habit.\nAs a start I’ll test a snapshot creation. This isn’t a backup since it does it to the same storage that the VM is running on, but it’s still nice to have for rollbacks, so I’d like to learn how it works. I create a snapshot for just the test VM I’m working on and run it. I can see the successful run in the list of backups. Heading over to local storage for the host that’s running my VM I can see the snapshot as well. Next up I need to make a change on my VM and then try rolling back to the snapshot. Heading over to the VM console I create a postsnapshot.txt file and put in some text. Still on the VM page but over at the snapshot tab I hit revert. It prompts me to create a snapshot before I do the revert, which is handy so I accept that. It looks like I can also manually create snapshots from this tab, which is nice. I’m not sure I actually need scheduled snapshots, or if it’s more something I’ll take before I do a tricky operation on a host. Back at the console I’m at the login prompt, since I didn’t do a snapshot with RAM. That’s fine. Logging in I can see my postsnapshot.txt file isn’t there. Let’s revert to the previous one. Again I’m at the login prompt, and when I log back in postsnapshot.txt is there as expected. It’s worth noting that each of these snapshots appear to be full copies, not just deltas of other snapshots. That makes sense, but it something to be aware of, as I could pretty quickly fill up a disk with snapshots if I’m not careful. Let’s delete these two and the associated backup job and try actual backups. One nice little note here is that from the VM page I can connect to the associated backup, that’s a nice little UX feature.\nNow let’s try the more traditional backup. I’m going to do delta backups so that I don’t have to store full snapshots for every backup I want. We still won’t do any scheduling yet, but it looks straightforward to do. I pick my remote as the location for the backup and hit save. Saving seems to take a while so I assume this is triggering a full backup immediately. I’ll just wait a bit to see what happens. Upon completion I’m back at the backup screen. I actually tried to just edit the existing “backup” I’d made of the snapshot job and when I look at the “modes” on the created backup it looks like it’s doing both now. That’s not what I want, let’s see how I can remove the snapshot feature. Ahhh, from the UI it’s hard to tell but clicking on each type of backup enables it and brings up its settings. Snapshots don’t really have settings so it’s not obvious from the window that you’ve enabled them. Let’s save again with snapshots disabled. This again takes a little while, which is a bit surprising, since I’d expect the backup to have been completed already. Looking at the backup page the backup shows as successful, but maybe that’s of the old snapshot type. When I do this for real I’m going to have to be more careful about making new backup jobs and separating them appropriately. Now that I’ve run a backup with the new setting it says by the status that it transferred 2.43GB. That seems realistic for the compressed size of my VM. Let’s make a change and run another backup. Following the pattern from the snapshot I add a postdelta1.txt and save the file. Heading over to the backups page I can see a new job running. After completion it says it transferred 2.43GB as well, which doesn’t seem right. Looking at the job run of the second backup there’s a warning about an “unused VHD”. Heading to that path on my NAS I can see a similarly named vhd file with an incrementally higher file name. The job mentions 20231119T190652Z.vhd and the file name in the NAS is 20231119T190951Z.vhd. Let’s try running one more backup and seeing if we get a delta this time, I’ll add one more file to be safe. This one is successful and transfers only 16MB. That’s a lot more than the size of the text file I created, but still very small in the scheme of things. Looking in the NAS folder I still only have one .vhd file, but it’s been modified at the time of the most recent backup. I guess I didn’t set a retention? I think to do retention I have to set some schedules. I suppose that makes sense. Let’s try and restore this backup. For the easiest option, that might not work since I still have the machine running I just head to “restore” under the backup sidebar in XO. When it comes to restore I can restore to any storage I have available, so presumably I could restore to a different host, but just to start let’s try restoring to the same host as the original machine. According to the run the restore was successful. Back in my VM list I still only see one copy of the test VM, so presumably it was overwritten. Let’s bring up the console and have a look. It appears to still be running and I see both the postdelta files I created. That’s odd. Ahh, I had my VM filter set to running (which it does by default). If I clear that filter I have my running VM as well as a halted VM named archtest (20231119T191644Z) which must be my restored copy. Let’s destroy the running one and start this one up. I stop and remove the old VM, I note that in addition to the fun name the new one also gets a “restored from backup” tag, which is interesting. This restored copy also has my second post delta text file, but I think that’s because I forgot to create a new one after running that extra delta backup, so that’s probably to be expected. Let’s stop and remove this machine and try restoring to a different host. Ok, with no running VMs I head back to the restore page, pick the same restore point to do, but point it to a different local storage on a different host than where I was originally running the VM. This time I tell it to start the VM on restore since I don’t have any alternative VMs for it to contend with. I’m up and running just fine on the new host now. In this case these hosts have the same CPU type and everything, so I don’t know if a cross environment restore would be more difficult, but the basic restore works just fine. Nice! There’s obviously a lot more to do in backups, but this is good for now."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html",
    "href": "posts/2020-10-14-arch-bootstrap.html",
    "title": "Automating provisioning Arch - OS installation",
    "section": "",
    "text": "This is part 1 of a 4 part series describing how I provision my systems. Links to each part are below:\n\npart 1 - The base OS install\npart 2 - Software install and system configuration with Ansible\npart 3 - User level and python environment config with dotfiles and mkrc\npart 4 - The tldr that wraps up how to do the whole thing from start to finish]"
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#edit-2020-10-20",
    "href": "posts/2020-10-14-arch-bootstrap.html#edit-2020-10-20",
    "title": "Automating provisioning Arch - OS installation",
    "section": "edit 2020-10-20",
    "text": "edit 2020-10-20\nAs I use this script to provision machines I’m going to end up making edits to it. I’m not going to edit this post every time I do that. The latest version of the provision script is always available here.\nI’ve also updated the TLDR section a bit based on some experience from the install. That part I will update if I make changes since I use it for reference when building systems."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#actual-introduction",
    "href": "posts/2020-10-14-arch-bootstrap.html#actual-introduction",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Actual Introduction",
    "text": "Actual Introduction\nI’ve installed a lot of operating systems a lot of times. The goal of writing out this post is to force me to really think about and clearly document a reproducible workflow for building my workstation.\nA secondary goal is to get better at bash."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#setting-up-wifi",
    "href": "posts/2020-10-14-arch-bootstrap.html#setting-up-wifi",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Setting up WiFi",
    "text": "Setting up WiFi\nIn the case where I’m doing this on a laptop I’ll likely have to get on WiFi before I can continue.\nsystemctl start iwd.service\niwctl\ndevice list #magically changed my device name to wlan0 here somehow\nstation wlan0 connect &lt;your SSID&gt;  # You can enclose it in quotes if it has spaces\n&lt;enter passphrase&gt;\nexit\ndhcpcd wlan0\nThat should work, try pinging something just to be safe."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#make-sure-partitions-are-set-up",
    "href": "posts/2020-10-14-arch-bootstrap.html#make-sure-partitions-are-set-up",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Make sure partitions are set up",
    "text": "Make sure partitions are set up\nI’m a wuss and don’t trust a script to actually create partitions. lsblk will tell you what disks you have. If you need to create/delete partitions before proceeding use cfdisk /dev/sd&lt;letter&gt; to create them. If it’s a completely blank hard drive and you need to create a boot partition make one at the beginning of the disk with 500M of space in cfdisk and then run mkfs.vfat -F32 /dev/sd&lt;letter&gt;1 to format it. There’s probably a cleaner way to clean out the LVMs this script creates but for now it’s easier for me to just blow them away in cfdisk and create a fresh partition to install over. edit: I got braver. The new script has an option to just wipe the whole disk if you want."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#run-the-script",
    "href": "posts/2020-10-14-arch-bootstrap.html#run-the-script",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Run the script",
    "text": "Run the script\nbash &lt;(curl -fsSL http://bootstrap.ianpreston.ca)"
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#post-install",
    "href": "posts/2020-10-14-arch-bootstrap.html#post-install",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Post install",
    "text": "Post install\n\nGet the Wifi going again. It’s a different command than you use from the installer:\n\nnmcli device wifi connect &lt;SSID&gt; password &lt;password&gt;\n\nSet up ssh keys - plug in the USB\n\nlsblk  # find where the partition with the keys is stored\nmkdir ssh  # make a mount point\nsudo mount /dev/sd&lt;something&gt; ssh\ncp -R ssh ssh_local  # Have to set permissions on keys (stupid NTFS)\ncd ssh_local/CA\nchmod 600 host_ca\nchmod 600 user_ca\ncd ../\nchmod +x setup_host.sh\nchmod +x setup_user.sh\nsudo ./setup_host.sh\n./setup_user.sh\nAfter this point you should be able to run ansible to complete the setup."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#prepping-the-vm",
    "href": "posts/2020-10-14-arch-bootstrap.html#prepping-the-vm",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Prepping the VM",
    "text": "Prepping the VM\nFirst thing to do for any install is download the ISO. I’m going to use Virtualbox as my hypervisor. No particular reason, I’ve just used it in the past and am comfortable with it.\nThen I create a base image to work off of.\n\n\n\nbase_vm\n\n\n\n\n\nvm_disk\n\n\nFire up the new VM, and select the Arch ISO at the prompt:\n\n\n\nvm_iso\n\n\nAfter that we’re at the boot prompt. Now comes the fun task of developing a bootstrap script that will automate the install process.\n\n\n\nvm_prompt"
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#getting-the-bootstrap-script-to-the-machine",
    "href": "posts/2020-10-14-arch-bootstrap.html#getting-the-bootstrap-script-to-the-machine",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Getting the bootstrap script to the machine",
    "text": "Getting the bootstrap script to the machine\nI’ve created a repository on GitHub to host code like this. I think I could probably just install git on the boot machine, clone the whole repo, navigate to the bootstrap script, and run it. That’s no fun though. Let’s see if I can find a more complicated approach just to save a few keystrokes.\nThe full URL to the raw script is at this page. That’s pointing to the branch I’m using while I develop the script. When I’ve got it working I’ll hopefully remember to come back here and point it to the master reference. From there I can go to my domain registrar and add a URL redirect record to point an easy to remember subdomain to that path:\n\n\n\nDNS\n\n\nSo now bootstrap.ianpreston.ca redirects directly to my shell script.\nTo actually get the script onto my machine and run it I’ll use curl. The exact syntax will be\nbash &lt;(curl -fsSL http://bootstrap.ianpreston.ca)\n-f Specifies that the script should fail silently. Otherwise if there’s an http error it will return a 404 page, which I’d then try and run. I’d rather it just not return anything and fail that way.\n-L Specifies that if the server reports that the page has moved then curl will redo the request.\n-sS Means the script should run silently, unless there’s a failure, in which case it will show the output. -S means show error and -s means silent.\nAs I’m writing this the shell script doesn’t really do anything, it just prints something out so I know it worked. Here’s where it’s at at this point:\n#!/usr/bin/env bash\n\n# To install: bash &lt;(curl -fsSL http://bootstrap.ianpreston.ca)\n\necho \"We got this far!\"\nAs a quick aside, I don’t write bash scripts often, so I often forget how exactly to set up the shebang. For bash scripts I’ve seen #!/bin/bash and #!/usr/bin/env bash. It seems like in most circumstances they’re interchangeable. This StackOverflow post suggests that the latter is slightly more flexible/portable so I’m going to try and make a habit of using it in my scripts going forward.\nBack at the VM I test my overly elaborate bootstrapping setup and…\n\n\n\ncurl\n\n\nSweet!"
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#strict-mode",
    "href": "posts/2020-10-14-arch-bootstrap.html#strict-mode",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Strict mode",
    "text": "Strict mode\nThe first couple lines of Brennan’s script include a bunch of things I don’t really understand. Since part of the goal of this is learning more bash I’m going to dissect them before moving on. The lines in question are:\nSOURCED=false && [ \"${0}\" = \"${BASH_SOURCE[0]}\" ] || SOURCED=true\nif ! $SOURCED; then\n  set -eEu\n  shopt -s extdebug\n  trap 's=$?; echo \"$0: Error on line \"$LINENO\": $BASH_COMMAND\"; exit $s' ERR\n  IFS=$'\\n\\t'\nfi\nLet’s break this up into tiny chunks. SOURCED=false is used to set a shell variable to false. Next up && is a list operator which will only run the next command if the previous one succeeded. So if we set SOURCED to false successfully then the next command will be executed.\nPutting something inside square brackets means to evaluate the expression inside and return success or failure based on that. You can use it for if statements, or use it to only execute a subsequent command based on the result of the conditional. Luke Smith has a good video explaining how to avoid if statements by writing code like the line we’re evaluating.\nSo what are we actually evaluating in the square brackets? This StackOverflow post explains the difference between ${0} and ${BASH_SOURCE[0]}. I’ll outline the difference below with an example script called a few different ways:\nI’ve got a script called experiment.sh which I’ll be running to check out these smaller components of the script\n#!/usr/bin/env bash\necho \"0: [$0] vs bash_source: [${BASH_SOURCE[0]}]\"\nHere’s the output of running that script a few different ways:\nroot@archiso ~ # bash ./experiment.sh\n0: [./experiment.sh] vs bash_source: [./experiment.sh]\nroot@archiso ~ # ./experiment.sh\n0: [./experiment.sh] vs bash_source: [./experiment.sh]\nroot@archiso ~ # . ./experiment.sh\n0: [./experiment.sh] vs bash_source: []\nroot@archiso ~ # source ./experiment.sh\n0: [./experiment.sh] vs bash_source: []\nIf I execute the script in a subshell, as with the first two examples, then they are equivalent and the expression will evaluate to true. If I source the script - that is I tell it to execute the commands in my current shell, then they will not be equivalent.\nBack to the script the shell variable name makes sense now and I can put this all together. Set the shell variable SOURCED to false, check whether the script is being sourced or not, and if it is, update the SOURCED shell variable to true (Since the || operator says to only execute the subsequent command if the previous did not return true).\nInside this block we have some commands that, as described above, will only run if the script is being run from its own subshell. The first command, set -eEu uses the set builtin to change the value of some shell options. -e forces the script to exit if almost any command in the script fails (the link provided above for set includes more details). -E let’s errors that occur within functions be trapped by the shells they inherit from. That’s confusing because it’s how I’d normally expect error handling to work, not a special case. This post explains what’s going on. Finally, -u treats unset variables and parameters as an error. Again, this is what I’d expect a sane language to do by default. I think the normal behavior is to just return an empty string in bash though. Gross.\nNext up is shopt -s extdebug. The shopt builtin lets us set additional shell options. The details of extdebug are in the previous link, but basically it allows better error tracing within function calls.\nThe next line is a bit of error handling as well. trap catches certain signals and runs a command in response. The basic syntax is trap &lt;command to run when caught&gt; &lt;signals to catch&gt;. Looking at trap 's=$?; echo \"$0: Error on line \"$LINENO\": $BASH_COMMAND\" exit $s' ERR that means if/when an ERR signal occurs in the script we’ll set the shell variable s to the exit status of the last task (that’s what $? is), print the filename of the shell script ($0), the line number of the error and it’s command, along with its exit status. Because of the options set above there’s no need to explicitly tell the trap to exit.\nThe final line in the block changes the internal field separator from the default of &lt;space&gt;&lt;tab&gt;&lt;newline&gt; to &lt;newline&gt;&lt;tab&gt;. The link above explains in general what that’s for. We’ll have to get a bit farther along in the script to see why it’s being used here."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#text-formatting",
    "href": "posts/2020-10-14-arch-bootstrap.html#text-formatting",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Text formatting",
    "text": "Text formatting\nThat part was dense. The next few lines are easier:\n# Text modifiers\nBold=\"\\033[1m\"\nReset=\"\\033[0m\"\n\n# Colors\nRed=\"\\033[31m\"\nGreen=\"\\033[32m\"\nYellow=\"\\033[33m\"\nText enclosed within $Bold and $Reset will be bolded. Similarly, enclosing within one of the colours and $Reset will set the text to that colour."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#setup-paths",
    "href": "posts/2020-10-14-arch-bootstrap.html#setup-paths",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Setup paths",
    "text": "Setup paths\nThe next section sets up a log file:\nWORKING_DIR=$(pwd)\nLOG=\"${WORKING_DIR}/arch-install.log\"\n[[ -f ${LOG} ]] && rm -f \"${LOG}\"\necho \"Start log...\" &gt;&gt;\"${LOG}\"\npwd stands for “print working directory”. When you enclose a command in $() it means to take the result of the command. To quickly illustrate, EXAMPLE=pwd would set EXAMPLE to “pwd”, whereas EXAMPLE=$(pwd) would set EXAMPLE to something like /root. The first two lines therefor set the LOG variable to point to a file in the current directory named arch-install.log.\nThe next line checks if the logfile exists, and deletes it if it does.\nThe final line writes “Start log…” into the logfile. &gt;&gt; redirects the output of the previous command to the end of the file on the right hand side. Since we know this is a brand new file (because of the line above) this will be the first line of the logfile."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#flags-and-variables",
    "href": "posts/2020-10-14-arch-bootstrap.html#flags-and-variables",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Flags and variables",
    "text": "Flags and variables\nThe next section sets up some system based flags\nSYS_ARCH=$(uname -m) # Architecture (x86_64)\nUEFI=0\nKEYMAP=\"us\"\nWIFI=0\nuname returns system information, and the -m flag specifies to return the machine hardware. As the comment above describes this will likely return x86_64. Later in the script we’ll check if the system is UEFI or BIOS."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#user-provided-variables",
    "href": "posts/2020-10-14-arch-bootstrap.html#user-provided-variables",
    "title": "Automating provisioning Arch - OS installation",
    "section": "User provided variables",
    "text": "User provided variables\nHere we just provide some defaults to variables that the user will set later in the script.\n# User provided variables\nHOST_NAME=\"computer\"\nKERNEL_VERSION=\"default\"\nMAIN_DISK=\"/dev/sda\"\nROOT_PWD=\"\"\nANSIBLE_PWD=\"\""
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#common-helper-functions",
    "href": "posts/2020-10-14-arch-bootstrap.html#common-helper-functions",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Common helper functions",
    "text": "Common helper functions\nThe next section has a series of small functions that will be used throughout the larger script. Let’s see what they do. The first one is:\nprint_line() {\n  printf \"%$(tput cols)s\\n\" | tr ' ' '-' |& tee -a \"${LOG}\"\n}\nThe function name gives a pretty solid hint what it does. printf Allows you to print a combination of strings and variables along with specified formatting for the variables. There’s some good docs here. tput provides information about the current terminal, in this case cols says the number of columns that make up a row in the terminal. In my VM tput cols returns 100 so the printf would resolve to printf \"%100s\\n\" which means we’ll print spaces across the width of the terminal. |& means to take the output (both from standard error and standard output, as opposed to just | which only return standard output) of the previous command and pass it as an input to the following command. tr in turn replaces the first string with the second, so we turn spaces into dashes, creating a line of dashes across the screen. Finally, that line of dashes is piped to tee -a which sends its input both to standard output and a file. The -a flag means to append the output to the file rather than overwriting it. All of that to say this function prints a line of dashes across your screen and into the logfile we defined above.\nNext up we have blank_line, which based on the explanation above is pretty self explanatory.\nblank_line() {\n  echo -e \"\\n\" |& tee -a \"${LOG}\"\n}\nNext up is print_title\nprint_title() {\n  clear\n  print_line\n  echo -e \"# ${Bold}$1${Reset}\" |& tee -a \"${LOG}\"\n  print_line\n  echo \"\" |& tee -a \"${LOG}\"\n}\nclear clears the screen. Everything else has been explained except $1 which is just the first argument passed to the function. This means calling print_title \"This is the title\" would clear the screen, print a line of dashes to the screen and log file, print This is the title to the screen and log file, another line, and then start at the beginning of a new line for whatever text follows.\nAfter that is print_title_info\nprint_title_info() {\n  T_COLS=$(tput cols)\n  echo -e \"${Bold}$1${Reset}\\n\" | fold -sw $((T_COLS - 18)) | sed 's/^/\\t/' |& tee -a \"${LOG}\"\n}\necho just prints some text, the -e flag tells it to interpret escaped characters, so \\t will show a tab rather than the literal \\t. That gets piped to fold, which wraps the text at 18 characters less than the width of the terminal. -s tells it to wrap at the last whitespace before the column limit (so don’t wrap in the middle of a word) and w is how you specify the column width to wrap on. After that we pipe the output to sed which I frankly find intimidating. This one’s not so bad though. 's/&lt;pattern&gt;/&lt;other pattern&gt;/' just performs a find replace of &lt;pattern&gt; for &lt;other pattern&gt; in each line of text that’s passed in. The patterns can be regular expressions, which I also find intimidating to work with, but this one is just saying to replace the beginning of the line (that’s what ^) means with a tab. Not so terrible.\nThe next several functions repeat the general concepts above, just with different formatting (red font for errors for example) so I won’t reproduce them here.\nNext up is pause_function\npause_function() {\n  print_line\n  read -re -sn 1 -p \"Press enter to continue...\"\n}\nIn the original code the read line was in an if block that was based on a variable that wasn’t set anywhere in the script. I assume that was planned to build fully unattended builds eventually, but I took it out for now at least. read receives input from the user. -re specifies not to allow backslashes to escape characters and to use Readline to obtain the line in an interactive shell. -s tells read not to echo input to the terminal, n 1 tells it how many characters of input to wait for. -p tells it what text to prompt with.\nNext up is arch-chroot\narch_chroot() {\n  arch-chroot /mnt /bin/bash -c \"${1}\" |& tee -a \"${LOG}\"\n}\nThis is cool, when I’ve tried to build my own version of this in the past I broke things up into a pre and post chroot because I couldn’t figure out how to get my script to change contexts. This one does it by just sending the commands one at a time into the chrooted environment. Neat!\nThe final helper is is_package_installed\nis_package_installed() {\n  #check if a package is already installed\n  for PKG in $1; do\n    pacman -Q \"$PKG\" &&gt;/dev/null && return 0\n  done\n  return 1\n}\npacman -Q searches for a package matching the subsequent argument. If it finds it it will return its full name and version. If it can’t it will return an error. So this function will return 0 if any packages are found, and 1 if none of them are."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#verification-functions",
    "href": "posts/2020-10-14-arch-bootstrap.html#verification-functions",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Verification functions",
    "text": "Verification functions\nThese are also helper functions, but they’re specifically designed to make sure the script is being run from the correct environment.\nFirst up is check_root\ncheck_root() {\n  print_info \"Checking root permissions...\"\n\n  if [[ \"$(id -u)\" != \"0\" ]]; then\n    error_msg \"ERROR! You must execute the script as the 'root' user.\"\n  fi\n}\nid -u returns the user id. Since Root is always user 0 on a system we know this isn’t being run as root and the script will fail.\nNext up is check_archlinux\ncheck_archlinux() {\n  if [[ ! -e /etc/arch-release ]]; then\n    error_msg \"ERROR! You must execute the script on Arch Linux.\"\n  fi\n}\n-e &lt;file&gt; checks if a file exists, so if /etc/arch-release does not exist (it’s an empty file on the USB boot system) then we’re not on Arch and had better exit.\nNext up is check_boot_system\ncheck_boot_system() {\n  if [[ \"$(cat /sys/class/dmi/id/sys_vendor)\" == 'Apple Inc.' ]] || [[ \"$(cat /sys/class/dmi/id/sys_vendor)\" == 'Apple Computer, Inc.' ]]; then\n    modprobe -r -q efivars || true # if MAC\n  else\n    modprobe -q efivarfs # all others\n  fi\n\n  if [[ -d \"/sys/firmware/efi/\" ]]; then\n    # Mount efivarfs if it is not already mounted\n    # shellcheck disable=SC2143\n    if [[ -z $(mount | grep /sys/firmware/efi/efivars) ]]; then\n      mount -t efivarfs efivarfs /sys/firmware/efi/efivars\n    fi\n    UEFI=1\n  else\n    UEFI=0\n  fi\n}\nThe purpose of this section is to verify the boot mode. Pretty much any system I can imagine installing on these days will be UEFI, but it doesn’t hurt to check. I’m not totally sure what the first little bit is doing, and I don’t have a mac to test. The -r flag is to remove a module from the kernel, rather than add it like the normal command. modprobe -q efivarfs will add the efivarfs module to the kernel, and fail quietly if it can’t find that module (that’s the -q flag). As described in the install guide, if you have a /sys/firmware/efi/ directory, which is what the first block of the second if statement is checking, then your system is EFI. The next part describes what it’s going to do (mount efivarfs if it’s not already mounted), but let’s dig into how it does that. -z returns true if the length of an evaluated string is zero. mount without any arguments returns all mountpoints in the system. We pipe that into grep which will return /sys/firmwar/efi/efivars if it’s mounted and an empty string if not, which accomplishes the goal. The last part of the script sets the variable UEFI to identify if the system is EFI or BIOS.\nNext up is check_wifi\ncheck_wifi() {\n  has_wifi=($(ls /sys/class/net | grep wlan))\n  if [ -n \"$has_wifi\" ]; then\n    WIFI=1\n  fi\n}\nAs per the Arch Wikie /sys/class/net lists all network devices. So if I list that directory and match on wlan then I know there’s a wireless device. I’ll use this to determine whether or not to install wireless tools when loading software."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#prompts-user-interaction",
    "href": "posts/2020-10-14-arch-bootstrap.html#prompts-user-interaction",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Prompts / User interaction",
    "text": "Prompts / User interaction\nThe first prompt asks for a hostname for the system. It had some code for auto naming that I trimmed out. The rest of it is pretty self explanatory:\nask_for_hostname() {\n  print_title \"Hostname\"\n  print_title_info \"Pick a hostname for this machine.  Press enter to have a random hostname selected.\"\n  read -rp \"Hostname [ex: archlinux]: \" HOST_NAME\n  if [[ $HOST_NAME == \"\" ]]; then\n    HOST_NAME=\"arch-$((1 + RANDOM % 1000)).tts.lan\"\n  fi\n}\nThe read command takes inputs, the -r flag prevents special characters from being included, and -p displays the text that follows as a prompt without a newline before taking the input.\nThe last block says if the input is empty to give a hostname like arch-[random number 1-1000].tts.lan.\nNext up we’re determine which hard disk to install on. Note that this isn’t handling partitioning or anything yet.\nask_for_main_disk() {\n  print_info \"Determining main disk...\"\n  devices_list=($(lsblk --nodeps --noheading --list --exclude 1,11,7 | awk '{print \"/dev/\" $1}'))\n\n  if [[ ${#devices_list[@]} == 1 ]]; then\n    device=${devices_list[0]}\n  else\n    print_title \"Main Disk Selection\"\n    print_title_info \"Select which disk to use for the main installation (where root and boot will go).\"\n    lsblk --nodeps --list --exclude 1,11,7 --output \"name,size,type\"\n    blank_line\n    PS3=\"Enter your option: \"\n    echo -e \"Select main drive:\\n\"\n    select device in \"${devices_list[@]}\"; do\n      if contains_element \"${device}\" \"${devices_list[@]}\"; then\n        break\n      else\n        invalid_option\n      fi\n    done\n  fi\n  MAIN_DISK=$device\n}\nThe first line calls lsblk to list all available block devices. --nodeps tells it not to show holder devices, so for example if I have an sda device with two partitions - sda1 and sda2 it will only show sda, which is what we want since we’re just picking the disk itself at this stage. --noheading drops column headers, which we want since we’re just going to parse this list. --list produces the output as a list, which we want in order to make a list of potential disks to select. --exclude 1,11,17 tells it not to list RAM or optical drive devices. The output of that list is piped into awk so that /dev/ can be prepended to it.\nThe next line says that if the array has only one entry (there’s only one disk available) then we just use that device. If we have more than one device the script prints out a list of them using the same command used to build the list of available disks but showing column headers and including some details to help select the correct disk.\nPS3 sets the prompt used by the select command. The select statement has you pick a device and loops if you haven’t selected one of the options in the list until you do. Finally we set MAIN_DISK to the device we want to install on.\nThe original script has some similar functions to pick a second disk, but I’m not going to be using that option so I’m omitting it.\nThere are a few other selection scripts (to set a root password and kernel version for example), but there’s nothing new in terms of BASH in them so I’ll omit them from this post."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#installationconfiguration-options",
    "href": "posts/2020-10-14-arch-bootstrap.html#installationconfiguration-options",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Installation/configuration options",
    "text": "Installation/configuration options\nNow we get to functions that actually help with the installation and configuration of the system.\nFirst is configure_mirrorlist\nconfigure_mirrorlist() {\n  print_info \"Configuring repository mirrorlist\"\n\n  pacman -Syy |& tee -a \"${LOG}\"\n\n  # Install reflector\n  pacman -S --noconfirm reflector |& tee -a \"${LOG}\"\n\n  print_status \"    Backing up the original mirrorlist...\"\n  rm -f \"/etc/pacman.d/mirrorlist.orig\" |& tee -a \"${LOG}\"\n  mv -i \"/etc/pacman.d/mirrorlist\" \"/etc/pacman.d/mirrorlist.orig\" |& tee -a \"${LOG}\"\n\n  print_status \"    Rotating the new list into place...\"\n  # Run reflector\n  /usr/bin/reflector --score 100 --fastest 20 --age 12 --sort rate --protocol https --save /etc/pacman.d/mirrorlist |& tee -a \"${LOG}\"\n\n  # Allow global read access (required for non-root yaourt execution)\n  chmod +r /etc/pacman.d/mirrorlist |& tee -a \"${LOG}\"\n\n  # Update one more time\n  pacman -Syy |& tee -a \"${LOG}\"\n}\npacman -Syy says to sync all available packages from the master repository. The -S flag is for sync, and yy forces a refresh even if the list appears to be up to date.\nNext the script installs reflector in order to update and optimize the list of mirrors that will be used for downloading packages.\nThe rest of the script is pretty well commented and straightforward.\n\nPartitioning\nThe script I’m templating off of is designed to wipe an entire disk. I generally dual boot Windows so I definitely don’t want that option. In light of that I had to tweak this section a fair bit. Rather than wiping the whole disk and creating new partitions like the template script, I want to identify an existing boot and linux partition and install to them. See the TLDR section for setting up a fresh disk. If you’ve already installed Arch on the disk you’re targeting you should also clear out the partition with the LVMs on it and start with a blank slate. That’s not done by the script, check the TLDR section for how to manage that.\nfind_install_partition() {\n  print_title \"Installation partition selection\"\n  print_title_info \"Select the partition to install Arch. This should be an already existing boot partition. If you don't see what you expect here STOP and run cfdisk or something to figure it out.\"\n  partition_list=($(lsblk $MAIN_DISK --noheading --list --output NAME | awk '{print \"/dev/\" $1}' | grep \"[0-9]$\"))\n  blank_line\n  PS3=\"Enter your option\":\n  lsblk $MAIN_DISK --output NAME,FSTYPE,LABEL,SIZE\n  echo -e \"select a partition\"\n  select partition in \"${partition_list[@]}\"; do\n    if contains_element \"$partition\" \"${partition_list[@]}\"; then\n      break\n    else\n      invalid_option\n    fi\n  done\n  INSTALL_PARTITION=$partition\n}\nThis works similarly to the main disk selection, except I formatted the output slightly differently. The one completely new command I added was the last | grep \"[0-9]$\". That filters the output to only show entries that end with a number ($ means end of line). I couldn’t figure out a way to have lsblk not list the block device (the opposite of what we wanted in the main disk selection) so I filter it out. As an example if I have a disk /dev/sda with partitions sda1, sda2, sda3 before the grep I’d get:\n/dev/sda\n/dev/sda1\n/dev/sda2\n/dev/sda3\nI really don’t want to accidentally try and make a partition on the whole device, so I filter that out so the list is just:\n/dev/sda1\n/dev/sda2\n/dev/sda3\nThere’s a practically identical function called find_boot_partition that does the same thing but identifies the boot partition for installation.\nThe next step is to create a physical and logical volume for the operating system using LVM\nsetup_lvm() {\n  print_info \"Setting up LVM\"\n\n  pvcreate $INSTALL_PARTITION\n  vgcreate \"vg_main\" $INSTALL_PARTITION\n\n  lvcreate -l 5%VG \"vg_main\" -n lv_var\n  lvcreate -l 45%VG \"vg_main\" -n lv_root\n  lvcreate -l 40%VG \"vg_main\" -n lv_home\n}\nThis one’s actually pretty readable. We create a physical volume on the install partition identified in the previous section, create a virtual group on it, and then create logical volumes within that.\nNext we format and mount the partitions:\nformat_partitions() {\n  print_info \"Formatting partitions\"\n\n  mkfs.ext4 \"/dev/mapper/vg_main-lv_var\"\n  mkfs.ext4 \"/dev/mapper/vg_main-lv_root\"\n  mkfs.ext4 \"/dev/mapper/vg_main-lv_home\"\n}\n\nmount_partitions() {\n  print_info \"Mounting partitions\"\n\n  # First load the root\n  mount -t ext4 -o defaults,rw,relatime,errors=remount-ro /dev/mapper/vg_main-lv_root /mnt\n\n  # Create the paths for the other mounts\n  mkdir -p \"/mnt/boot/efi\"\n  mkdir -p \"/mnt/var\"\n  mkdir -p \"/mnt/home\"\n\n  if [[ $UEFI == 1 ]]; then\n    mount -t vfat -o defaults,rw,noatime,utf8,errors=remount-ro \"${MAIN_DISK}1\" \"/mnt/boot/efi\"\n  fi\n\n  # Mount others\n  mount -t ext4 -o defaults,rw,noatime /dev/mapper/vg_main-lv_var /mnt/var\n  mount -t ext4 -o defaults,rw,noatime /dev/mapper/vg_main-lv_home /mnt/home\n}\nAgain, most of this is pretty readable. The one that I didn’t know about was the noatime option. In the original script it was set to relatime, but after reading this post it seems like I want noatime to improve the life of my SSD."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#installation",
    "href": "posts/2020-10-14-arch-bootstrap.html#installation",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Installation",
    "text": "Installation\nThe install_base_system function doesn’t really introduce any new bash stuff, which was my main goal in writing out how this all worked line by line. I’ll present it below without further comment.\ninstall_base_system() {\n  print_info \"Installing base system\"\n\n  pacman -S --noconfirm archlinux-keyring |& tee -a \"${LOG}\"\n\n  # Install kernel\n  case \"$KERNEL_VERSION\" in\n  \"lts\")\n    pacstrap /mnt base base-devel linux-lts linux-lts-headers linux-firmware |& tee -a \"${LOG}\"\n    [[ $? -ne 0 ]] && error_msg \"Installing base system to /mnt failed. Check error messages above.\"\n    ;;\n  \"hard\")\n    pacstrap /mnt base base-devel linux-hardened linux-hardened-headers linux-firmware |& tee -a \"${LOG}\"\n    [[ $? -ne 0 ]] && error_msg \"Installing base system to /mnt failed. Check error messages above.\"\n    ;;\n  *)\n    pacstrap /mnt base base-devel linux linux-headers linux-firmware |& tee -a \"${LOG}\"\n    [[ $? -ne 0 ]] && error_msg \"Installing base system to /mnt failed. Check error messages above.\"\n    ;;\n  esac\n\n  # Install file system tools\n  pacstrap /mnt lvm2 dosfstools mtools gptfdisk |& tee -a \"${LOG}\"\n  [[ $? -ne 0 ]] && error_msg \"Installing base system to /mnt failed. Check error messages above. Part 4.\"\n\n  # Install networking tools\n  pacstrap /mnt dialog networkmanager networkmanager-openvpn |& tee -a \"${LOG}\"\n  [[ $? -ne 0 ]] && error_msg \"Installing base system to /mnt failed. Check error messages above. Part 5.\"\n\n  if [[ $WIFI == 1 ]]; then\n    pacstrap /mnt iwd |& tee -a \"${LOG}\"\n    [[ $? -ne 0 ]] && error_msg \"Installing base system to /mnt failed. Check error messages above. Wifi\"\n  fi\n\n  # Remaining misc tools\n  pacstrap /mnt reflector git gvim openssh ansible terminus-font systemd-swap |& tee -a \"${LOG}\"\n  [[ $? -ne 0 ]] && error_msg \"Installing base system to /mnt failed. Check error messages above. Part 6.\"\n\n  # Add the ssh group\n  arch_chroot \"groupadd ssh\"\n\n  # Set the NetworkManager & ssh services to be enabled\n  arch_chroot \"systemctl enable NetworkManager.service\"\n  arch_chroot \"systemctl enable wpa_supplicant.service\"\n  arch_chroot \"systemctl enable sshd.service\"\n}\nNext up we have some user account setup and configuration for ansible, which will be used for the rest of the configuration of the machine. The original script added some public keys to the ansible user’s authorized keys file. I’d like to add some automation to handle my key management approach but the VM I’m working in makes USB passthrough a hassle. (I switched to Hyper-V part way through making this guide as Virtualbox and WSL2 were fighting on my machine).\nThere’s a script for updating the root user account, but it has a subset of what’s in the ansible account, so let’s just look at that one:\nsetup_ansible_account() {\n  print_info \"Setting up Ansible account\"\n\n  arch_chroot \"useradd -m -G wheel -s /bin/bash ansible\"\n\n  arch_chroot \"echo -n 'ansible:$ANSIBLE_PWD' | chpasswd -c SHA512\"\n\n  arch_chroot \"chfn ansible -f Ansible\"\n\n  mkdir -p /mnt/home/ansible/.ssh\n  chmod 0700 /mnt/home/ansible/.ssh\n  arch_chroot \"chown -R ansible:ansible /home/ansible/.ssh\"\n\n  # Add user to the ssh\n  arch_chroot \"usermod -a -G ssh ansible\"\n}\nuseradd does about what you’d expect. -m creates a home directory for that user if it doesn’t exist. -G is followed by a list of groups you want the user to be a part of. In this case we want ansible to be part of wheel so it can perform actions with elevated privileges. chpasswd is a pretty cool way to set a user password from a script without user interaction, man pages here. chfn is used to change user info, in this case to give the ansible user the first name Ansible."
  },
  {
    "objectID": "posts/2020-10-06-docker-compose.html",
    "href": "posts/2020-10-06-docker-compose.html",
    "title": "Notes on docker-compose",
    "section": "",
    "text": "Introduction\nThis is going to be a grab bag of docker-compose tips and snippets for things that I do commonly enough that I want to write them down but not commonly enough that I remember the syntax offhand.\n\n\nDifferent file names\nBy default docker-compose wants the compose file to be docker-compose.yml in the same directory as the command is being run. Generally you want to stick with that, but I did come up with a situation where I wanted to give them different names. You can do docker-compose -f &lt;file name&gt; &lt;rest of your commands&gt; to get around this. Note that the -f &lt;file name&gt; has to be at the start, you can’t just put it in like any other flag.\n\n\n.env files\nOne of the reasons I was trying to have different file names was because I wanted a bunch of different compose files to share a .env file. My new solution is to have a .env file in the root directory of where I keep my compose files and then use symbolic links to make a link to that master file in each directory. Depending on how I created the link though I would run into a too many levels of symbolic links error. A clean way to solve this is to navigate to the subdirectory and run ln -s ../.env .env. Whatever you put after -s is literally what’s included in the link, so as long as there’s a .env file in the parent folder this will work, regardless of where in your file system you move these directories.\n\n\nConclusion\nThat’s it for now. I’ll come back and update this document if anything else comes up."
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html",
    "href": "posts/2020-05-06-pfsense.html",
    "title": "Building pfsense",
    "section": "",
    "text": "This guide is mostly for me. I’m trying to get better at documenting all the tech stuff I do on my home system, both to ensure I have a decent understanding of what I’m doing, and also to ensure I can reproduce it if I need to rebuild at some point in the future. This guide is heavily reliant on the guidance from Mark Furneaux in his YouTube series on the topic. The first video in that playlist provides a good overview of what pfsense is and why you might want to install it. If you watch that video and think you might like to install pfsense, but don’t want to watch about 9 hours of YouTube to figure out how to set it up, this might come in handy for you as well.\n\n\n\nAt a high level this will cover installing and configuring a pfsense box along with a Unifi wireless access point. Hardware selection is not covered here. The table of contents describes the sections of pfsense settings and applications that will be set up over the course of this guide."
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#who-this-is-for",
    "href": "posts/2020-05-06-pfsense.html#who-this-is-for",
    "title": "Building pfsense",
    "section": "",
    "text": "This guide is mostly for me. I’m trying to get better at documenting all the tech stuff I do on my home system, both to ensure I have a decent understanding of what I’m doing, and also to ensure I can reproduce it if I need to rebuild at some point in the future. This guide is heavily reliant on the guidance from Mark Furneaux in his YouTube series on the topic. The first video in that playlist provides a good overview of what pfsense is and why you might want to install it. If you watch that video and think you might like to install pfsense, but don’t want to watch about 9 hours of YouTube to figure out how to set it up, this might come in handy for you as well."
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#what-this-will-cover",
    "href": "posts/2020-05-06-pfsense.html#what-this-will-cover",
    "title": "Building pfsense",
    "section": "",
    "text": "At a high level this will cover installing and configuring a pfsense box along with a Unifi wireless access point. Hardware selection is not covered here. The table of contents describes the sections of pfsense settings and applications that will be set up over the course of this guide."
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#addendum",
    "href": "posts/2020-05-06-pfsense.html#addendum",
    "title": "Building pfsense",
    "section": "Addendum",
    "text": "Addendum\nI have an old Kobo reader. After switching to the Unifi AP I couldn’t get it on the WiFi. An even older Kindle would connect, and my newer Kobo connected after manually entering the SSID, but no luck with the older one. Eventually I found this reddit thread with the solution. From the Unifi control panel I went to settings -&gt; wireless networks -&gt; &lt;my SSID&gt; -&gt; advanced and unchecked “enable minimum data rate control” for the 2G network. After applying that and trying to connect a few more times I made it online with the Kobo."
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#system-general-setup",
    "href": "posts/2020-05-06-pfsense.html#system-general-setup",
    "title": "Building pfsense",
    "section": "System / General Setup",
    "text": "System / General Setup\nOnly two minor changes here:"
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#system-advanced-admin-access",
    "href": "posts/2020-05-06-pfsense.html#system-advanced-admin-access",
    "title": "Building pfsense",
    "section": "System / Advanced / Admin Access",
    "text": "System / Advanced / Admin Access\nTurn off https since we don’t have a certificate authority (maybe I’ll figure that part out later)\n\n\n\nadmin\n\n\nChange browser tab text so I know what page each tab is on.\n\n\n\ntext\n\n\nEnable SSH and change the default port to 2222 (security through obscurity). I’m going to set up keys later, for now let it work with a password but come back and update this later.\n\n\n\nssh\n\n\nFun piece of trivia, right as I enabled this my web front-end froze. The actual router was still functioning, but I couldn’t get any web admin. Part of it was that I had to switch from https to http in the URL, but even after that I would hit the login, it would accept my password, and just take me back to the login page. I found this reddit thread which had the same issue. For me I could connect in using a private browsing window, which led me to try restarting my browser, which fixed it. Computers…"
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#system-advanced-firewall-nat",
    "href": "posts/2020-05-06-pfsense.html#system-advanced-firewall-nat",
    "title": "Building pfsense",
    "section": "System / Advanced / Firewall & NAT",
    "text": "System / Advanced / Firewall & NAT\nSet Firewall optimization to conservative. I have plenty of CPU and RAM for the size of my network, so why not give idle connections a little longer to hang out? Apparently this can improve VOIP performance.\n\n\n\nfirewall"
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#system-advanced-networking",
    "href": "posts/2020-05-06-pfsense.html#system-advanced-networking",
    "title": "Building pfsense",
    "section": "System / Advanced / Networking",
    "text": "System / Advanced / Networking\nFor now I’m just going to disable IPv6. I don’t think my ISP supports it and I don’t see the need for it on my LAN. Maybe I’ll revisit that later, but for now turning it off seems like the safer approach.\n\n\n\nipv6\n\n\nSet everything possible to work on hardware. Given that I have Intel NICs in my router I think I can safely run all of these things. I ran iperf3 between two wired connections as well as speedtest before enabling the settings and then again after. Note that you have to reboot after changing these settings. After the reboot performance was essentially unchanged so I took that as a good sign and kept the settings.\n\n\n\nhardware"
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#system-advanced-miscellaneous",
    "href": "posts/2020-05-06-pfsense.html#system-advanced-miscellaneous",
    "title": "Building pfsense",
    "section": "System / Advanced / Miscellaneous",
    "text": "System / Advanced / Miscellaneous\nI don’t expect a lot of super heavy CPU needs on this system, so I might as well save some power and heat. I’ll put it on Adaptive to start, but I’ll check out Hiadaptive if some services seem to chug.\n\n\n\nhiadaptive\n\n\nMy CPU supports AES-NI and I’ll want that enabled for better VPN performance later.\n\n\n\naesni"
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#system-advanced-notifications",
    "href": "posts/2020-05-06-pfsense.html#system-advanced-notifications",
    "title": "Building pfsense",
    "section": "System / Advanced / Notifications",
    "text": "System / Advanced / Notifications\nI want email notifications if something gets borked on my router. I don’t want to use my actual gmail address to send these notifications though, as I’d like to keep the security on it a lot more locked down. I created a new gmail account just for the router, and then in my account settings (for the router email do not do this for your actual email) I set “less secure apps” to on so that I could enable sending emails with the following settings:\n\n\n\nnotifications\n\n\nAfter all that I hit “test SMTP settings” and received an email informing me it worked. Hurray!"
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#interfaces-wan-and-lan",
    "href": "posts/2020-05-06-pfsense.html#interfaces-wan-and-lan",
    "title": "Building pfsense",
    "section": "Interfaces / WAN (and LAN)",
    "text": "Interfaces / WAN (and LAN)\nI’m going to disable IPv6 here since as I discussed I don’t want to use it.\nFor the LAN I first have to disable the DHCPv6 service:\n\n\n\ndhcp6\n\n\nAnd then on each respective interface’s page I can disable IPv6\n\n\n\ndhcp6"
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#dashboard",
    "href": "posts/2020-05-06-pfsense.html#dashboard",
    "title": "Building pfsense",
    "section": "Dashboard",
    "text": "Dashboard\nLet’s add some widgets! Everyone loves widgets.\nAdd S.M.A.R.T status so I can see if my hard drive is failing. Services Status to see what’s running. Interface statistics to see if I’m getting any errors, and finally traffic graphs because who doesn’t like a nice live graph?"
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#static-mappings",
    "href": "posts/2020-05-06-pfsense.html#static-mappings",
    "title": "Building pfsense",
    "section": "Static Mappings",
    "text": "Static Mappings\nFor most of the devices on my network I’d like to give them easy to type and remember names, even if they don’t have the functionality to specify a hostname (or even if they do just to be safe). For one thing this makes it easy to connect to devices, but even for things I don’t want to connect to (like an e-reader) it’s nice to give them an obvious name. That way if I’m looking at the DHCP leases on my network it will be easier to notice a new device. To do this there are two steps. The first is adding a DHCP static mapping for each device, and the second is enabling DNS to resolve those names (skipping ahead a bit since DNS is next). While you can do static mappings from the DHCP services page, it’s actually easier to do from the DHCP status page. Beside each device that’s connected there’s an “add static mask icon” which you can click. After that you just have to fill in an IP, the hostname you want, and an optional description:\n\n\n\nstatic mask\n\n\nSince I’m going to set up most of my network this way I’m also going to narrow the range for regular DHCP leases to be handed out back on the DHCP Server service page:\n\n\n\nstatic mask\n\n\nAfter this I’ll have to trigger a release/renew cycle for all devices on the network. The easiest way to do this is probably just reboot the router.\nTo set a proper hostname on a device here are the instructions relevant to me:\nOn Linux: edit /etc/hostname to be whatever you want the hostname to be.\nOn Android: Who knows? I can’t seem to make this give a proper name. Fortunately I can rely on the name from pfsense.\nOn Windows: Hit start, type “pc name” and select the entry that comes up, click “Rename this PC” and change it to whatever you want.\nThe last step is to set the DNS resolver to resolve these names. In services -&gt; DNS resolver I check these two boxes:\n\n\n\nresolver\n\n\nI think I only need the second one since I gave every client on my network a static lease, but the other one seems nice to have as well, so I’ll enable it."
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#basic-setup",
    "href": "posts/2020-05-06-pfsense.html#basic-setup",
    "title": "Building pfsense",
    "section": "Basic Setup",
    "text": "Basic Setup\n\nUnder VPN -&gt; OpenVPN head to the wizard tab.\nLeave authentication backend as Local User Access\nFill in details for the CA. I left most things the default\nSame deal for the server cert, most options should persist from the CA\nSet up general server information\n\nI set hardware crypto on.\nTunnel network has to be an IP range not used by your LAN, and it shouldn’t be a common one. Since my LAN uses 172.17.1.0/24 I went with 172.17.2.0/24\nI left redirect gateway off because I’m just trying to set up LAN access remotely, I don’t need all my traffic tunneled through here if I’m remote\nLocal network points to my LAN so I can access it, so as described above that’s 172.17.1.0/24\nI set concurrent connections to 10. Probably more than I’ll need but why not?\nI enabled Inter-Client Communication\nI set the DNS default domain to localdomain\nI set DNS Server 1 to 172.17.1.1. These two options should allow me to access my servers by hostname even remotely\nI set the NTP server to 172.17.1.1 since I went to all the trouble of configuring it\nI don’t think I need NetBIOS or WINS so I left that blank\n\nCheck boxes for both firewall rules\nWizard complete, now I want to be able to export settings to clients, so over to System -&gt; package manager and install openvpn-client-export\n\n\nDNS resolution\nI’m not sure why I had to set this, but in order to get my client machines to be able to resolve hostnames of servers I had to go into Services -&gt; DNS resolver -&gt; General settings and check “Register connected OpenVPN clients in the DNS resolver”. The way they describe it really sounds like it should allow me to resolve client names, but it fixed my issue so whatever."
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#create-users",
    "href": "posts/2020-05-06-pfsense.html#create-users",
    "title": "Building pfsense",
    "section": "Create Users",
    "text": "Create Users\nSystem -&gt; User Manager -&gt; Add User. For users I’m going with the &lt;person&gt;_&lt;device&gt;_vpn naming convention. So for example my phone’s certificate will be ian_phone_vpn. I don’t add the user to admins, and I create and store a nice secure password with LastPass. Make a certificate for the user, all the default’s should be fine, just fill in an appropriately descriptive name."
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#export-keys-to-devices",
    "href": "posts/2020-05-06-pfsense.html#export-keys-to-devices",
    "title": "Building pfsense",
    "section": "Export keys to devices",
    "text": "Export keys to devices\nBack to VPN -&gt; OpenVPN -&gt; Client Export Utility. Since I have dynamic DNS configured I changed over Host Name Resolution to my DynamicDNS name rather than my IP which is the default.\nI didn’t mess with any of the defaults. I have the OpenVPN Connect app on my phone so I scrolled down to the user I just created for my phone and clicked “OpenVPN Connect (iOS/Android). Downloaded the file, transferred it to my phone, imported it in the app and was good to go."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html",
    "href": "posts/2023-01-21-proxmox3.html",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "",
    "text": "This is the third post ( part 1, part 2) documenting my adventures setting up a home cluster. In this one I will try a few different methods of getting VMs installed on proxmox. As with the previous posts, this is not intended to be a how to guide from an expert. I haven’t used proxmox before working on this project, so I’m mostly doing this to document what I do for future reference, or maybe provide others with the perspective of what it’s like to work on proxmox as a relative beginner.\nAll the code I reference in this post is in my recipes repository. Specifically, the ansible role to create templates is here and the terraform code is here"
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html#create-the-vm",
    "href": "posts/2023-01-21-proxmox3.html#create-the-vm",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "Create the VM",
    "text": "Create the VM\nThe most obvious way to install a VM is through the UI. I know I won’t want to take this approach indefinitely as it involves manual work and isn’t reproducible (at least not easily), but it seems like the right place to start, both to ensure I don’t have any unforeseen issues with my setup, and also to provide a baseline for comparison when I try other methods later.\nSelecting one of my nodes from the web interface I click “Create VM”. In the first tab I pick the node to install to and give it a name, we’ll do ubuntu-test for this. I could also assign it to a resource pool if I had any of those created but I don’t so I won’t. The other thing I can assign is a VM ID, which is the unique numeric identifier proxmox uses internally. At this point I’m fine to let proxmox manage that though so I’ll leave it on the default.\nChecking the advanced options I can also configure the VM to start at boot so it will come back up if I reboot my cluster. I can also configure the order it starts/stops. The start at boot setting seems like it would be handy for production services, but I’m just testing so I’ll leave it for now.\nOn the next tab I can configure the OS. I’ve already configured my NAS (set up in part 2) to hold things like ISO images for installing and uploaded an Ubuntu 22.10 server image, so I’ll select that. The guest OS settings are already correctly set on Linux with a modern kernel so I’m all good there.\nNext up is the System tab. The first option is Graphic Card. There’s a ton of options under this one, but at this point I don’t have any intention of installing anything that will care so I’ll leave it at default. Maybe at some point I’ll have a system with a GPU that I want to pass through, or will need a Windows server, but not right now. I also have to pick a machine option. Based on the docs as long as I don’t want to do PCIe passthrough I can stick with the default, so I will for now. Next I pick a SCSI controller. Again, referring to the docs the VirtIO SCSI Single option that it had selected by default seems perfect for me. There’s also a checkbox for Qemu Agent. Reading the docs this seems like a handy thing to have, so I’ll turn it on (looks like mostly it’s for cleaner shutdown and pausing during disk backups). The last thing on this tab is whether to enable TPM. Since I’m not making a Windows image I don’t need this, so I’ll leave it unchecked.\nFollowing that we’re on to Disks. I can create multiple disks I’m sure, but for now let’s just set up one. First I make sure that the backing storage is my local-zfs storage, which is the NVME drive on the host, rather than my NAS. I haven’t configured the SSD in these hosts yet, I’m planning to set up ceph on them but that’s for a future post. The other basic thing to set on this page is disk size. I’m not planning to keep this image around, so I’ll stick with the default 32GB for now. The Bus/Device field defaults to the SCSI interface I set up on the last tab so that seems fine. There’s an option for cache mode as well. Right now I’m not really sure what that does, but from the docs the default of no cache seems like it will work for me, so I’ll leave it. Taking a look at the docs it seems like I want to have the Discard option checked so I’ll do that. From the docs IO Thread only seems like it really matters if I have multiple disks attached, but I don’t see the harm of turning it on so let’s do that. I’ll check SSD emulation since the underlying disk really is an SSD and the guest OS might as well think so too. I’ll uncheck the backup option on this one, since I’m planning to just destroy this VM shortly after I create it and I don’t need backups hanging around. I want to be able to try replicating this VM to different hosts, and I’d want this disk to be included, so I’ll leave skip replication unchecked. The last thing I have to pick is the Async IO option. From reading this it seems like the default io_uring will work for me, I’m not deep enough on how this sort of thing works to have strong opinions or requirements so I’ll go with the default.\nNow we come to CPU. For sockets I’ll leave it at 1, since all my hosts have only 1 physical socket. For cores my hosts have either 4 or 6 cores, so there’s definitely no point going over 6. Since this is just a test machine let’s just give it 2. For CPU type I’m going to leave it on the Default (kvm64). From the docs on CPU type if I set the CPU to host it will exactly match the CPU flags and type of my host system, but I might have migration issues across hosts, since they’re not all the exact same CPU. The default will allow easier migration, but might disable some specific CPU flags that would be beneficial. For now I’ll stick with the easy option. There’s some other stuff for CPU limits and extra CPU flags here that I’m also going to leave alone for now.\nNow we’re on to memory. Each of these hosts has 32GB of memory, so I don’t really have to be cheap here, at least while I’m testing. Under advanced I can set a memory floor and enable/disable ballooning. From the docs, I want to have ballooning enabled, even if I have the memory floor and ceiling set the same, so that my host can see how much memory the VM is actually using. If I was running a bunch of VMs with dynamic memory requirements I could see overallocating the max across them and setting the floor for each. In this case I’m just going to leave it at the default 2GB since I’m not going to actually run anything on this VM.\nAlmost done, next up is network. I’ve only got one bridge right now so I’ll leave that selected. I’m not currently doing any VLAN stuff in my network so I’ll leave the VLAN tag entry blank. For model I’ll stick with VirtIO as the docs say that’s the one to pick for best performance as long as your host supports it. The firewall option is checked by default. I haven’t looked into the proxmox firewall at all at this point, but let’s leave that on for now. I can also do some rate limiting and other fancy network config here but I’m going to leave those on the default for now.\nThe only thing to do now is confirm my settings and actually create the VM. I’ll check start after created so it fires up right away."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html#configure-the-vm",
    "href": "posts/2023-01-21-proxmox3.html#configure-the-vm",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "Configure the VM",
    "text": "Configure the VM\nAfter waiting a little bit I can see my newly created VM listed under the node I set it up on. Clicking into that VM and selecting the Console section I can see that I’m in the Ubuntu server installation wizard. Since this isn’t a post about installing Ubuntu server I’ll work through the menus without writing everything down. Going through the install worked fine until it came time to reboot and it failed to unmount the virtual CD-ROM that had the installation ISO. I went to the hardware tab on the VM in the proxmox interface, removed the CD-ROM and rebooted. After the reboot the VM came up fine, and I was able to ssh into it from my desktop."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html#thoughts-on-manual-templates",
    "href": "posts/2023-01-21-proxmox3.html#thoughts-on-manual-templates",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "Thoughts on manual templates",
    "text": "Thoughts on manual templates\nThis wasn’t too bad. There are a few tweaks I’d want to apply, like adding the guest agent into the machine, but overall template creation is pretty easy. I could see wanting to update my templates semi regularly when new versions of base OSs come out though, and I’d like to understand more of the theory behind how this actually works, since a lot of what I did was pretty much copy paste. To do that, I’ll clear these out and look into some other template creation options."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html#cleaning-up",
    "href": "posts/2023-01-21-proxmox3.html#cleaning-up",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "Cleaning up",
    "text": "Cleaning up\nI don’t want lingering stuff from this experiment hanging out on my nodes, so let’s go in and see what I have to get rid of. First is the created VMs - I can stop and remove them from the UI easily enough. Same deal for the template VM. I checked the local storage through the UI as well and it looks like any virtual disks I created were removed when I got rid of the VM. The only other thing to remove was that initially downloaded cloud image, so I went into the shell for the node and just ran rm to get rid of that."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html#automate-creating-another-template",
    "href": "posts/2023-01-21-proxmox3.html#automate-creating-another-template",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "Automate creating another template",
    "text": "Automate creating another template\nHaving a working Ubuntu template is pretty handy, but what if I want to branch out? Can I apply this approach to other distros? I’m pretty sure this approach will work fine with another debian based distro, and probably even another fairly standard Linux like CentOS will be fine (although I should test). But what about weird ones? Specifically I want to see if I can get this working on flatcar Linux since I want to try using it for my kubernetes nodes. Let’s walk before we try running though and extend to another version of Ubuntu.\nThe first thing I want to do is tweak how I’m numbering my templates. Right now each template gets a variable set for its whole VM ID. I’d like to break that out into chunks. The first digit should just always be 8 (at least for now) to indicate a template and keep it out of the range of actual VMs I’m deploying. The next one I’m thinking should be the node the template is created on, and then the last two digits can be an identifier for the specific template. This actually wasn’t bad at all. The one variable definition gets a little long, but basically I just go from one line in defaults of build_vm_id: \"8000\" to this.\nThis relies on all my proxmox nodes having hostnames of the format pve&lt;Num&gt; but I can work with that. The number of digits in my IDs will change if I get more than 9 nodes or 99 templates too, but I’m not really expecting that to happen, and I don’t even think that would necessarily break anything if it did, so I won’t worry about it for now.\nWith that slight modification to the role complete I set my playbook to call the role twice, modifying the variables from the defaults for just the template name, the template number, and the URL of the cloud image to build from for Ubuntu Jammy and Kinetic.\nAround this time I realized it was going to be a little tedious running the template build script each time I added a template, so I added a handler to the role to execute the template build script whenever it made a change. It took a little bit of tweaking to figure out that I needed the full path of the script I wanted to run, as well as set it as the working directory so I could call the subscript that defines all the build variables. After those changes the handler triggered properly and built my templates whenever the script changed, or I added a new template to build.\nAt this point the general template creation process is working quite nicely for Ubuntu versions, but what about other distros? Let’s give debian a shot. I grabbed the cloudgeneric version of debian bullseye from their official cloud images page and plugged it into my playbook. No problem at all. The template built, I was able to build an image from it just the same as the Ubuntu ones.\nLet’s get a bit braver and branch out to an even more different distro Rocky Linux. This one might come in handy if I want to try out anything enterprisey or just want to see what the Red Hat experience is like. I found their generic cloud images here and plugged the link into my playbook. The template built ok, but trying to run the VM I ran into problems where it got stuck on a line that said Probing EDD (edd=off to disable)... ok and just hung out there. Similar to the weird boot loop I got deploying from my NAS I wasn’t able to shut down the VM from the Web UI and had to go into the terminal on the node and ps aux | grep \"/usr/bin/kvm -id &lt;VM ID&gt;\" to find its PID and kill -9 it before I could remove the VM. I guess I have to do some troubleshooting. A little searching finds that this error is pretty common, although it doesn’t actually relate to the message, but something that’s happening after. There are a few potential kernel configs I might be able to change, but as I’m poking around in the machine I notice something interesting, it’s got way more disk to start than my Ubuntu templates did. I wonder if I’m somehow filling the disk, so I use that command from the previous section and resize the disk on a newly cloned template before starting it up. Disappointingly this did not solve the problem. Another weird thing I noticed during the start up is that CPU usage on the VM is pinned at right around 50%. Since I gave it 2 cores that suggests that one core is working flat out on something. Several of the posts indicated that after about 10 minutes the system would come up. That’s obviously a terrible startup time, but I’d like to give it a while to see if I at least have the same problem. So I go do some reading and let this VM run for a while… and discover that tragically the usually perfect strategy of ignoring a problem and hoping it goes away doesn’t work in this case."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html#examine-the-template-creation-script-and-modify-it",
    "href": "posts/2023-01-21-proxmox3.html#examine-the-template-creation-script-and-modify-it",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "Examine the template creation script and modify it",
    "text": "Examine the template creation script and modify it\nSomething about how I have my VM configured is not playing nice with Rocky Linux. It could just be a very specific thing that I only want to modify for that distro, but I also just copy-pasted most of the other template creation parameters from some guy on the internet. So before I assume that my basic parameters are the best and it’s only Rocky that needs to be modified, let’s examine those options that I’m using and see if I want to modify any of them. Maybe while I’m at it I’ll fix my Rocky issue (or introduce new ones to working distros), but at a minimum I’ll have a better understanding of what’s going on.\nThe first little bit of the script downloads a cloud image, and then uses virt-customize to update packages, install a list of packages (just qemu-guest-agent and cloud-init by default), copy in a build-info file with some metadata about the template build, copy in some ssh related files and have a script to set them up on first boot (note to self, maybe that’s the part that’s breaking in Rocky, I’ve only tested that script in debian and Arch based distros so far). That stuff (except maybe the ssh part) is all straightforward and I understand what it’s trying to do, so let’s skip to the next line:\nqm destroy ${build_vm_id}\nRemove the old template before you build a new one, makes sense.\nqm create ${build_vm_id} --memory ${vm_mem} --cores ${vm_cores} --net0 virtio,bridge=vmbr0 --name ${template_name}\nCreate a new VM (that we’ll turn into a template later) with an ID of build_vm_id, memory and cores set to our variables, and a virtio network adapter, which is what I did in the manual template creation. Finally we give it a name based on the template_name variable. So far so good, but I had a lot more options available when I built a VM manually earlier in this post, anything else I should set? Reading back through my manual config I set basically everything else to defaults so I think I’m good here. Let’s see what’s next.\nqm importdisk ${build_vm_id} ${image_name} ${storage_location}\nOk, this is fine, I’m importing the disk image I downloaded and modified to the VM I created and putting it in the storage location I specify. All seems fine. Maybe I’ll need to revisit this if I take another crack at storing these templates on my NAS, but fine for now.\nqm set ${build_vm_id} --scsihw ${scsihw} --scsi0 ${storage_location}:vm-${build_vm_id}-disk-0\nOk, here’s where I deviate from what I picked in the manual build. In my defaults (based on the script I copied in) I had scsihw set to virtio-scsi-pci, whereas in my manual build I went with virtio-scsi-single. I’m struggling to find the actual difference between these settings, but let’s change it for kicks for now.\nqm set ${build_vm_id} --ide0 ${storage_location}:cloudinit\nAdd the cloud-init drive, seems fine. It’s emulating a CD drive so ide makes sense.\nqm set ${build_vm_id} --nameserver ${nameserver} --ostype l26 --searchdomain ${searchdomain} --ciuser ${cloud_init_user}\nAdd a couple defaults to the cloud-init template and set the ostype to linux (l26). No worries there.\nqm set ${build_vm_id} --boot c --bootdisk scsi0\n--boot c tells it to boot from hard disk (as opposed to CD or network) and we set the bootdisk to the image that’s been mounted to the VM. Seems fine.\nqm set ${build_vm_id} --agent enabled=1\nThis turns on qemu agent, which we want.\nOne thing I noticed from going through this is I had some lines that set multiple options, even though they weren’t necessarily related. So I cleaned that up to be one option per line. Easier to parse and modify that way.\nI took a quick look back at the manual config section and didn’t see anything else that stood out, so I guess I have to get back to fixing Rocky Linux."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html#get-back-to-making-rocky-linux-work-spoiler-i-do-not-succeed",
    "href": "posts/2023-01-21-proxmox3.html#get-back-to-making-rocky-linux-work-spoiler-i-do-not-succeed",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "Get back to making Rocky Linux work (spoiler, I do not succeed)",
    "text": "Get back to making Rocky Linux work (spoiler, I do not succeed)\nOk, that was a fun side quest, but let’s get back to figuring out Rocky. I re-run my template creation playbook, just in case that storage config changed anything. I also found a proxmox forum post where someone was having the same problem with a particular RHEL image, but no solution. That post also said it worked fine with RHEL 7 and the issue was with 6. I’m trying Rocky 9 (I believe they use the same version as RHEL for compatibility) so I don’t know if that’s helpful. This post suggests the output just means my console output is being redirected somewhere else, so I’m not seeing whatever the actual issue is. I guess I should fix that first regardless. One suggested solution there is to change the default tty from serial. An alternative approach there, is to check out the proxmox docs and enable serial out on the VM with qm set &lt;VM ID&gt; -serial0 socket. Let’s add that line to my template and see if I get anything. A little bit of progress in that it doesn’t just tell me I don’t have serial on that machine, but I also only see starting serial terminal on interface serial0 (press Ctrl+O to exit), which isn’t exactly informative. Let’s ditch my ssh script setup on first boot, just to make sure that’s not what’s hanging the template. Removing it from the template script gives me the same issue, so the problem is elsewhere. Just for kicks, let’s try a different Rocky cloud image. I found this blog that’s using the GenericCloud image rather than the GenericCloudBase image I was using. I’m not sure why I picked GenericCloudBase to begin with so let’s swap over and see what happens. Still nothing. Ok, the blog also has a bunch more cloud-init modules installed than I do. Maybe one of them will fix things. Let’s add them to the package list for that template. Still no luck. Ok, back to basics. We have a blog post where someone made a template, and apparently it worked. Let’s try manually working through those instructions and see what happens. Well, first problem their link goes to a pinned daily build of Rocky that’s no longer available on the site. Fine, we’ll do the latest one and hope that there’s not just some daily build issue that’s leading to all of this. So I get the same error following the guide. After a little more digging I find a link in the rocky vault to the exact image that the blog was using. Let’s apply my template to that image. Slightly better luck. It still bootloops, but I can actually see things in the console and by being very speedy I was even able to get a screencap of the kernel panic error it’s throwing.\n\n\n\nNodes\n\n\nFinding out that this was the issue led me to a proxmox forum post where it turns out lots of people are having this issue with Rocky 9 if they set their CPU to the default kvm64. I reset my VM to use host for the CPU. That fixed the boot loop error but led to another error. At this point I decided I didn’t feel like running Rocky Linux very much."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html#try-arch-linux",
    "href": "posts/2023-01-21-proxmox3.html#try-arch-linux",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "Try Arch Linux",
    "text": "Try Arch Linux\nRocky was supposed to be an intermediate difficulty distro to test out my template process. I don’t actually have a use case for it, I just figured it would be more different than debian but less different than flatcar when it comes to testing. I’m really hoping that I just got unlucky and that other distros won’t be so hard. Let’s see if that’s correct. I don’t have an immediate use case for Rocky, but I do like running Arch, it’s what my current server has. Let’s try that. Building the template goes fine, and this one actually boots to a login prompt from the proxmox UI, so we’re on a happier path than with Rocky already. At first it seemed like Arch wasn’t updating my default user. I started up the image and tried to ssh in but got a connection refused error. Trying to ssh in as the default arch user that the image uses got a permission denied error instead. After some testing it turns out that cloud-init just takes longer to complete on first boot in Arch, I think because it has to do more package updating. If I just left the VM running for a bit I was able to ssh right in."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html#try-flatcar-linux",
    "href": "posts/2023-01-21-proxmox3.html#try-flatcar-linux",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "Try flatcar Linux",
    "text": "Try flatcar Linux\nI’ve never used flatcar before, but it sounds interesting and this blog recommended it for self hosting kubernetes so I’d like to have it available in my environment. I found this repository which had its own scripts to create a flatcar template. Most of it looks broadly similar to the approach I’ve been taking, so let’s try it out. I notice that flatcar images come in .img.bz2 format instead of .img or .qcow2 like the other files I’ve downloaded. I may have to add in some logic to the script to extract images in that case. As a first step though I just tried running the whole workflow as is. That got me a template built, but the VM I created off it couldn’t find any bootable media, suggesting the disk creation didn’t work as intended. Probably because I have to extract it first. After adding a little bit of logic to my image building script:\nif [ \"${image_name##*.}\" == \"bz2\" ]; then\n    bzip2 -d ${image_name}\n    image_name=\"${image_name%.*}\"\nfi\nI got the VM to boot. I could auto login as user core over serial, but it looks like none of my other cloud-init config stuff worked. This post is already getting super long though so I’m going to save getting a fully working flatcar image for a separate post and declare victory on my general goal of “be able to make templates automatically”."
  },
  {
    "objectID": "posts/2023-07-06-network-automation.html",
    "href": "posts/2023-07-06-network-automation.html",
    "title": "Automating my network",
    "section": "",
    "text": "Introduction\nIn a previous post I set up a managed switch in my network, but I did it all manually through the menus. Realistically that’s fine, I don’t have a super big or complicated network and I don’t move things around enough to justify the investment in learning how to automate it in terms of time savings. But I like automating things, so let’s see what I can figure out.\n\n\nWhat I’d have liked to do\nIdeally I would handle this through Ansible, since that’s what I use to do most of the rest of my home automation. Unfortunately, my switch is not one of the supported devices in Ansible’s networking stack as near as I can tell. The next best thing would have been to use NAPALM for python based automation, but that’s also not supported. So I have to go one level down the stack and use netmiko. Let’s see how that goes.\n\n\nConnecting to the switch\nIn the previous post I connected using the serial console and then telnet. For netmiko to work I will need SSH. This does not appear to be enabled by default. After checking the manual it looks like enabling this is a command line only operation. From the initial login I’m in the manager level interface and my prompt looks like this: ProCurve Switch 2810-48G# I need to get from there to the Global configuration level by running config so it looks like this ProCurve Switch 2810-48G(config)# and then run crypto key generate ssh to create a host key on the switch, ip ssh to enable ssh, and then show ip ssh to confirm that it worked.\nAfter this I’ll try and connect to the switch and find that it’s got too old a key exchange method to work by default:\nUnable to negotiate with 192.168.10.2 port 22: no matching key exchange method found. Their offer: diffie-hellman-group1\nAfter finding a bunch of other out of date security protocols that my ssh client didn’t support by default (probably a good reason to not have this switch in the enterprise anymore) I was able to get it working with the following ssh config:\nHost switch\n    User admin\n    HostName 192.168.10.2\n    KexAlgorithms +diffie-hellman-group1-sha1\n    PubkeyAcceptedAlgorithms +ssh-rsa\n    HostkeyAlgorithms +ssh-rsa\n    Ciphers +3des-cbc\nWith that set I can now ssh into my switch. Let’s try and actually do something with netmiko.\nThe baby connection test script that I used looks like this:\nimport netmiko\nfrom getpass import getpass\n\ndevice = {\n    \"ip\": \"192.168.10.2\",\n    \"device_type\": \"hp_procurve\",\n    \"username\": \"admin\",\n    \"password\": getpass(\"Enter password for the switch:\\n\"),\n}\n\nwith netmiko.ConnectHandler(**device) as connection:\n    print(connection)\nwhich does print out a signature for a connection object. I don’t have any actual info on the switch itself, but it appears to be working as I was getting a connection error before I configured ssh properly.\nWe can do something a little more interesting that also validates the connection by modifying the last two lines to:\nwith netmiko.ConnectHandler(**device) as conn:\n    sys_info = conn.send_command(\"show system-information\")\n\nprint(sys_info)\nThis indeed prints out the system info, so the connection is working.\n\n\nFiguring out the commands I need\nLast time I worked on this I just did everything with the menu because I was lazy. If I’m going to automate things I will need to use the CLI, so let’s identify the commands I need and what their outputs look like.\n\nshow vlan will list all my VLANs\nshow vlan &lt;vlan#&gt; will list a specific VLAN as well as any ports that do tagged or untagged traffic for that VLAN\nshow mac-address [&lt;port&gt;] show mac addresses seen by the switch, optionally specify for a particular port. Returns them in format ######-######\n\n\n\nSee if I can do some parsing on those before I do actual change based operations\nSo far I haven’t identified the commands necessary to actually modify my setup, but let’s see if I can do some easy parsing on these to begin with.\nI’ll try show vlan to start. With a little bit of string parsing I can get a nice looking output:\ndef get_vlans(conn) -&gt; list[dict[str, str]]:\n    \"\"\"Get VLAN info.\n\n    Returns a list of dictionaries with keys for\n    vlan_num, vlan_name and vlan_status, all as strings.\n    \"\"\"\n    base_output = conn.send_command(\"show vlan\")\n    output_list = [line.strip() for line in base_output.split(\"\\n\") if line.strip()]\n    vlan_list = [line.split() for line in output_list if re.match(r\"\\d+\\ \", line)]\n    vlan_dict = [\n        {\"vlan_num\": line[0], \"vlan_name\": line[1], \"vlan_status\": line[2]}\n        for line in vlan_list\n    ]\n    return vlan_dict\nWhich returns something like:\n[\n  {'vlan_num': '1', 'vlan_name': 'DEFAULT_VLAN', 'vlan_status': 'Port-based'},\n  {'vlan_num': '15', 'vlan_name': 'TRUST', 'vlan_status': 'Port-based'},\n  {'vlan_num': '30', 'vlan_name': 'Guest', 'vlan_status': 'Port-based'},\n  {'vlan_num': '40', 'vlan_name': 'LAB', 'vlan_status': 'Port-based'}\n]\nI can probably do something for showing a particular VLAN:\ndef get_vlan_ports(conn, vlan_num):\n    \"\"\"Get the ports associated with a vlan and their tagged or default status.\"\"\"\n    base_output = conn.send_command(f\"show vlan {vlan_num}\")\n    output_list = [line.strip() for line in base_output.split(\"\\n\") if line.strip()]\n    vlan_list = [line.split() for line in output_list if re.match(r\"\\d+\\ \", line)]\n    vlan_dict = [{\"port\": line[0], \"state\": line[1]} for line in vlan_list]\n    return vlan_dict\nWhich gets me something like:\n[{'port': '3', 'state': 'Tagged'}, {'port': '7', 'state': 'Tagged'}, {'port': '15', 'state': 'Untagged'}]\nFor the MAC address I’m going to make a little helper function to do some string formatting first, as the formatting for MAC addresses from the switch is different than what I see in most other places. I want to be able to just copy paste from anywhere and have them comparable. This is a one liner: re.sub(\"[^0-9]\", \"\", mac) in a function that takes mac as an argument. After that we have a similar pattern except in this case I’m going to return a dictionary where each key is a MAC address and each value is its associated port:\ndef get_mac_ports(conn):\n    \"\"\"Get MAC addresses seen by the switch and their ports.\"\"\"\n    base_output = conn.send_command(\"show mac-address\")\n    output_list = [line.strip() for line in base_output.split(\"\\n\") if line.strip()]\n    mac_list = [\n        line.split() for line in output_list if re.match(r\"[\\da-fA-F]{6}\", line)\n    ]\n    mac_dict = {mac_parser(line[0]): line[1] for line in mac_list}\n    return mac_dict\nWith this if I have a dictionary with keys being the MAC addresses of my devices and values being the names of those devices, I can find what devices are on what ports in an automated way (if they’re on, the switch only shows current connections).\ndef map_devices_to_ports(conn):\n    mac_dict = get_mac_ports(conn)\n    home_ports = {v: mac_dict.get(k) for k, v in home_macs.items()}\n    return home_ports\nSo one last thing in terms of info gathering. I’d like to know the state in terms of VLAN settings for all of my ports, plus the device associated with them if I have that:\ndef vlan_status(conn):\n    \"\"\"Get the VLAN assignment of each port, along with a name if you can.\"\"\"\n    vlans = get_vlans(conn)\n    vlan_nums = [x[\"vlan_num\"] for x in vlans]\n    # vlan_desc = {x[\"vlan_num\"]: f'{x[\"vlan_num\"]}_{x[\"vlan_name\"]}' for x in vlans}\n    all_ports = {\n        str(port): {k: \"\" for k in [\"name\"] + vlan_nums} for port in range(3, 49)\n    }\n    # Assign names to ports I know\n    for k, v in map_devices_to_ports(conn).items():\n        all_ports[v][\"name\"] = k\n    # Associate VLAN tags\n    for vlan in vlan_nums:\n        port_dicts = get_vlan_ports(conn, int(vlan))\n        for port_dict in port_dicts:\n            port = port_dict[\"port\"]\n            state = port_dict[\"state\"]\n            all_ports[port][vlan] = state\n\n    return all_ports\nI had to do a few hacky things because I haven’t thought through my data structures very well, but I’m ok with this, it does the trick. Now for every port I get a name if I know the device as well as the status of ever VLAN in terms of “tagged”, “untagged” or an empty string for not applied. I start at port 3 because I have the first two trunked to my router and I don’t expect to have to change them and because they’re trunk ports I can’t just show ports 1 and 2.\n\n\nDo actual modifications to the switch config\nLet’s experiment with configuring an actual port the way I want it. The way the commands work in the HP console is operations are performed on VLANs based on ports, so something like vlan 30 tagged 1-5 would allow traffic tagged with VLAN 30 on ports 1-5. I think of things more in terms of how I want ports to behave, so my preferred syntax would be something like port 5 v30 tagged v15 untagged to set port 5 to accept tagged traffic on VLAN 30 and mark untagged traffic as being on VLAN 15. There’s probably clever ways to bundle together my current state and desired state and only execute the commands necessary to reconcile them, but let’s do some building block stuff and figure out how to just change a particular VLAN assignment on a particular port to start.\ndef set_port_vlan_state(conn, port: int, vlan: int, state: str):\n    \"\"\"Set VLAN state on a port.\"\"\"\n    command = f\"vlan {vlan} {state} {port}\"\n    x = conn.send_config_set(command)\n    return True\nThis “works” but doesn’t account for a lot of edge cases. For one thing, I can only enable VLANs as either tagged or untagged with this. If I want to disable them I need to add a flag that will add a “no” to the command. However, if I do that, I also need to ensure I’m not ending up in an invalid state, as I have to have at least one VLAN enabled either tagged or untagged on any given port. I think based on this it might make more sense to try and do a comprehensive remapping rather than individual steps.\nTo start I’ll make a constant at the top of the script called DESIRED_STATE in the same format as the output of vlan_status. This should make it easier to reconcile and also lets me copy paste the output of vlan_status to do the initial population.\nLet’s write a little helper function to do basic validation on this DESIRED_STATE. I won’t be able to catch everything that could be wrong here, especially not just misconfiguration, but I can get the basics:\ndef validate_desired_state():\n    \"\"\"Make sure my desired state will actually work.\"\"\"\n    # We'll catch VLANs actually existing later, just make sure we're consistent\n    reference_keys = set(DESIRED_LAYOUT[\"3\"].keys())\n    correct_states = {\"\", \"Untagged\", \"Tagged\"}\n    for k, v in DESIRED_LAYOUT.items():\n        states = set(pv for pk, pv in v.items() if pk != \"name\")\n        if states - correct_states:\n            raise RuntimeError(\n                f\"Unknown VLAN status on port {k}: {states - correct_states}\"\n            )\n        if set(v.keys()) != reference_keys:\n            raise RuntimeError(f\"Keys for port {k} don't match port 3\")\n        untagged_count = len([x for x in v.values() if x == \"Untagged\"])\n        if untagged_count &gt; 1:\n            raise RuntimeError(f\"Port {k} has more than one VLAN set to untagged\")\n        if untagged_count == 0:\n            raise RuntimeError(f\"Port {k} has no VLAN specified for untagged\")\nNow we can do something to compare the current state and the desired state, and return any ports that don’t reconcile:\ndef check_vlan_status(current_state: dict):\n    \"\"\"Is the current state the same as the desired state?\"\"\"\n    # Check names first\n    mismatch_names = dict()\n    for k in current_state.keys():\n        if (\n            current_state[k][\"name\"] != DESIRED_LAYOUT[k][\"name\"]\n            # Allow for devices to just be turned off\n            and current_state[k][\"name\"] != \"\"\n        ):\n            mismatch_names[\n                k\n            ] = f\"Current Name: {current_state[k]['name']}, Desired Name: {DESIRED_LAYOUT[k]['name']}\"\n    if mismatch_names:\n        print(\"Names don't match on some ports\")\n        for k, v in mismatch_names.items():\n            print(f\"Port: {k} {v}\")\n        raise RuntimeError(\"Port name mismatch\")\n    # Make sure we're working with the same VLANs\n    desired_vlans = {\n        key\n        for vlans in DESIRED_LAYOUT.values()\n        for key in vlans.keys()\n        if key != \"name\"\n    }\n    current_vlans = {\n        key for vlans in current_state.values() for key in vlans.keys() if key != \"name\"\n    }\n    if desired_vlans != current_vlans:\n        print(\n            f\"VLANs don't match. Current state: {current_vlans} Desired: {desired_vlans}\"\n        )\n        raise RuntimeError(\"VLAN selection mismatch\")\n    # If names are all good check ports\n    mismatched_ports = dict()\n    for k, v in DESIRED_LAYOUT.items():\n        for vlan in current_vlans:\n            if DESIRED_LAYOUT[k][vlan] != current_state[k][vlan]:\n                mismatched_ports[k] = DESIRED_LAYOUT[k]\n                break\n    return mismatched_ports\nWe do a little more runtime checking to make sure that devices I think are in a particular port aren’t showing up elsewhere. Note that I want to be able to run this with some devices powered down, as I may want to only bring them up after reconfiguring their ports, so I allow for the name identified in the current state to be an empty string. Then we make sure I have the right VLANs in my desired state, so I haven’t created or deleted any from my current state that I think I should have. If all that goes well I go through each port and if I find a mismatch in VLAN config I add the desired state to a mismatched_ports dictionary that I can pass into some reconcilliation function later.\nWhile doing some testing for this I got my switch into a weird state where I got intermitent errors running the script, even on functions that had worked fine before. I gave the switch a reboot to see if I could clear things up and that seemed to work, but it does add to how sketchy this whole setup feels. This is probably going to get filed under “learning activity” rather than “thing I use to manage my environment”. We’ll see though.\nI did get a function that would update the configuration of a port to match what I want from a desired state dictionary:\ndef set_port_vlan_state(conn, port: str, state: dict):\n    \"\"\"Set VLAN state on a port.\"\"\"\n    # Get rid of the name key\n    state.pop(\"name\", None)\n    vlans = set(state.keys())\n    # Should only be one untagged VLAN and we validate that elsewhere.\n    untagged_vlan = [k for k, v in state.items() if v == \"Untagged\"][0]\n    tagged_vlan = [k for k, v in state.items() if v == \"Tagged\"]\n    # Set the untagged VLAN first so we definitely don't end up orphaned.\n    commands = [\n        f\"vlan {untagged_vlan} untagged {port}\",\n    ]\n    # Turn off untagged explicitly for all other VLANs\n    for vlan in vlans - {untagged_vlan}:\n        commands.append(f\"no vlan {vlan} untagged {port}\")\n    # set tagged vlans\n    for vlan in tagged_vlan:\n        commands.append(f\"vlan {vlan} tagged {port}\")\n    # Turn off tags on other VLANs\n    for vlan in vlans - set(t for t in tagged_vlan):\n        commands.append(f\"no vlan {vlan} tagged {port}\")\n    # Now save the desired config\n    commands.append(\"write memory\")\n    conn.send_config_set(commands)\nI still run into hanging the connection to the switch from time to time with it, but maybe that’s not as big a deal given how infrequently I’ll actually be doing this outside of developing the script. The last thing I have to do is put that together with the list of unreconciled ports that I created into one big function:\ndef reconcile(conn):\n    \"\"\"Bring the current state of the switch in line with the desired state.\"\"\"\n    validate_desired_state()\n    current_state = vlan_status(conn)\n    mismatches = check_vlan_status(current_state)\n    if mismatches:\n        for port, state in mismatches.items():\n            set_port_vlan_state(conn, port, state)\nAnd that appears to work!\n\n\nConclusion\nI’m pretty sure this is not what most people are talking about when they say “software defined networking”, and there were many hacky parts to the setup. On the other hand, it’s slightly easier for me to modify my switch setup in the future, I learned a bit more about managing my switch, and I got to practice my python. Overall I’d call that a win."
  },
  {
    "objectID": "posts/2021-04-10-portfolio.html",
    "href": "posts/2021-04-10-portfolio.html",
    "title": "My Portfolio",
    "section": "",
    "text": "Some of the posts on this blog are pretty big guides or documents that I like to reference and refer others to regularly. In addition, I have a few other projects that I haven’t written about, but would still be nice to share. This post will compile all the bigger posts and projects I’ve worked on as one easy to reference article."
  },
  {
    "objectID": "posts/2021-04-10-portfolio.html#homelab-cluster",
    "href": "posts/2021-04-10-portfolio.html#homelab-cluster",
    "title": "My Portfolio",
    "section": "Homelab cluster",
    "text": "Homelab cluster\nI’ve been learning about proxmox, virtualization, and kubernetes in my homelab. The first in the series on configuring proxmox is here. I also took a stab at getting kubernetes the hard way going on my local setup, and got most of the way through it before realizing there were a lot of differences between my home setup and the cloud and that there was some more fundamental learning I’d have to do before that exercise was super helpful. I did learn a decent amount though, some about kubernetes, more about terraform."
  },
  {
    "objectID": "posts/2021-04-10-portfolio.html#mortgage-modeling",
    "href": "posts/2021-04-10-portfolio.html#mortgage-modeling",
    "title": "My Portfolio",
    "section": "Mortgage modeling",
    "text": "Mortgage modeling\nWhen I was in the process of planning to buy my house, I wanted to understand whether it made sense to go with a variable or fixed rate mortgage. As part of that analysis I pulled some historical data in and did some analysis of how that decision would have worked out for me over the last few decades. The post is here."
  },
  {
    "objectID": "posts/2021-04-10-portfolio.html#where-to-live-app",
    "href": "posts/2021-04-10-portfolio.html#where-to-live-app",
    "title": "My Portfolio",
    "section": "Where to live app",
    "text": "Where to live app\nThis post documents the process I went through to build a where to live app. It scrapes listings from rental and sales sites daily, and then combines them with other data sets like commute times, grocery store locations, and flood risk to produce personalized lists of candidate listings. The blog describes the implementation in more detail, along with some important lessons I learned and links to the code. The blog post is here."
  },
  {
    "objectID": "posts/2021-04-10-portfolio.html#paper-reproduction---albertas-fiscal-responses-to-fluctuations-in-non-renewable-resource-revenue-in-python",
    "href": "posts/2021-04-10-portfolio.html#paper-reproduction---albertas-fiscal-responses-to-fluctuations-in-non-renewable-resource-revenue-in-python",
    "title": "My Portfolio",
    "section": "Paper Reproduction - “Alberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue” in python",
    "text": "Paper Reproduction - “Alberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue” in python\nThis is a paper reproduction I did of a paper that was published by the University of Calgary’s school of public policy. In the course of reproducing the paper I actually found a data error in the original paper. After correcting the error the conclusions of the paper do not appear to be supported. An interesting exercise in coding, and reproducibility in science. The blog post is here."
  },
  {
    "objectID": "posts/2021-04-10-portfolio.html#automating-provisioning-arch",
    "href": "posts/2021-04-10-portfolio.html#automating-provisioning-arch",
    "title": "My Portfolio",
    "section": "Automating provisioning Arch",
    "text": "Automating provisioning Arch\nI use Arch Linux btw. This post links to a 3 part post series I did about automating the setup of my workstations and servers. It goes from a detailed breakdown of the bash script that’s used to get a bare bones arch install, through to using ansible to do all the system configuration, software install, and server setup (mostly a bunch of docker containers), followed by setting up a user profile using rcm."
  },
  {
    "objectID": "posts/2021-04-10-portfolio.html#python-packaging-guide",
    "href": "posts/2021-04-10-portfolio.html#python-packaging-guide",
    "title": "My Portfolio",
    "section": "Python packaging guide",
    "text": "Python packaging guide\nThere are lots of great guides on how to build a python package. I’m personally a big fan of hypermodern python and use its accompanying cookiecutter whenever I’m setting up a new project. But there’s not much out there that walks you through all the phases between just having a script that you can run up to building a full blown package. Also, most guides don’t cover conda, and I find the conda docs are really tuned towards people who are bundling C code or something else with their package, not just making a pure python package. To bridge this gap, and to help cement my own understanding of packaging, I wrote this guide."
  },
  {
    "objectID": "posts/2021-04-10-portfolio.html#setting-up-a-data-science-environment-in-windows",
    "href": "posts/2021-04-10-portfolio.html#setting-up-a-data-science-environment-in-windows",
    "title": "My Portfolio",
    "section": "Setting up a data science environment in Windows",
    "text": "Setting up a data science environment in Windows\nThis guide is for everyone out there that only has a locked down Windows machine, but still wants to work with data in python. Almost all the guides I could find online assumed you had a Mac or Linux machine, and even the Windows ones often assumed you had administrative rights. This guide gets you set up with conda, git, and vs code, all without elevated privilege. The only thing I haven’t managed to do in a Windows environment is build a python package. I’m sure it’s doable, but this guide is for those writing scripts/notebooks."
  },
  {
    "objectID": "posts/2023-06-04-nvidia-notes.html",
    "href": "posts/2023-06-04-nvidia-notes.html",
    "title": "A couple notes on working with Nvidia cards",
    "section": "",
    "text": "Introduction\nWhile helping the infrastructure team at work get Nvidia drivers working on a VM virtualized on top of esxi and then getting that GPU to be available within rootless docker containers I learned a couple things that I want to note down here.\n\n\nInstalling nvidia drivers\nSo after I went and wrote a nice playbook to do this, I realized that nvidia maintains their own here, so in the future I would definitely just use this. I’m sure it would work better. One note that I will add. In my experience, installing the CUDA version of the drivers is not worth it. I was able to do GPU accelerated ML workloads without it, and installing them caused me nothing but pain and suffering. Maybe it would go smoother with the official Nvidia role, but I would suggest trying without unless you really know for sure you need them.\nFor posterity, here’s how I installed Nvidia drivers:\n# https://fabianlee.org/2021/05/19/ansible-installing-linux-headers-matching-kernel-for-ubuntu/\n- name: Install dependencies\n  ansible.builtin.apt:\n    pkg:\n      - linux-headers-generic\n      - curl\n\n- name: Get distribution name in the weird format nvidia wants it\n  ansible.builtin.shell: |\n    . /etc/os-release;echo $ID$VERSION_ID\n  register: os_release\n\n- name: Add nvidia gpg key\n  ansible.builtin.shell: |\n    curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | gpg --dearmor -o /etc/apt/keyrings/nvidia-container-toolkit-keyring.gpg\n  args:\n    creates: \"/etc/apt/keyrings/nvidia-container-toolkit-keyring.gpg\"\n  \n- name: Add nvidia container toolkit repository to apt\n  ansible.builtin.shell: |\n    curl -s -L https://nvidia.github.io/libnvidia-container/{{os_release.stdout}}/libnvidia-container.list | \\\n    sed 's#deb https://#deb [signed-by=/etc/apt/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n    tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n  args:\n    creates: \"/etc/apt/sources.list.d/nvidia-container-toolkit.list\"\n\n- name: update packages\n  become: true\n  ansible.builtin.apt:\n    upgrade: \"yes\"\n    update_cache: yes\n\n# Headless 530 is the latest available as of 2023/5/16\n- name: install nvidia driver and container runtime docker\n  become: true\n  ansible.builtin.apt:\n    pkg:\n      - nvidia-driver-525-server\n      - nvidia-utils-525-server\n      - nvidia-container-toolkit-base\n      - nvidia-docker2\n\n- name: Blacklist nouveau drivers\n  become: true\n  ansible.builtin.copy:\n    src: nouveau-blacklist.conf\n    dest: /etc/modprobe.d/nouveau-blacklist.conf\n    owner: root\n    group: root\n    mode: '0644'\n  notify:\n    - Restart machine\nOf particular note is the step to blacklist the nouveau drivers. I’m not 100% sure since I didn’t do either the bare metal or virtualized install on the systems I was testing this on, but it appears that nouveau drivers get automatically installed on virtualized systems on top of esxi. Because of that, you have to blacklist them or else you get all sorts of esoteric errors that do a terrible job of telling you where the issue actually is.\n\n\nExtra stuff to make it work with rootless docker\nA couple pieces of this got covered in the above section, specifically installing nvidia-container-toolkit-base and nvidia-docker2. I’m not actually sure nvidia-container-toolkit-base is required, I couldn’t get anything working when I had just it installed, nvidia-docker2 did the trick though, along with the extra steps below.\n- name: Add CDI support\n  become: true\n  ansible.builtin.shell: |\n    nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml\n  args:\n    creates: \"/etc/cdi/nvidia.yaml\"\n\n- name: Disable cgroups\n  become: true\n  ansible.builtin.lineinfile:\n    path: /etc/nvidia-container-runtime/config.toml\n    regexp: '^no-cgroups '\n    insertafter: '^#no-cgroups '\n    line: 'no-cgroups = true'\n\n\nConclusion\nIf you want to install nvidia drivers on hosts using ansible, don’t trust some hacked together code from some guy on the internet, use the official Nvidia role. But if that role doesn’t handle rootless docker integration, or you run into weird issues getting it working on VMs virtualized on top of esxi, take a look at this stuff and see if it helps you out."
  },
  {
    "objectID": "posts/2020-07-20-pi-bluetooth.html",
    "href": "posts/2020-07-20-pi-bluetooth.html",
    "title": "Connecting a Harmony remote to a raspberry pi",
    "section": "",
    "text": "Introduction\nThis is a little stub post for me. I have a Logitech Harmony remote that I use to control a raspberry pi running Kodi for my media center. Occasionally I have to reinstall it and I always have to google a few things to remember how to do it, so this is just to consolidate that info.\n\n\nGet the Harmony Bluetooth ID\nSince I don’t have a GUI on my pi beyond Kodi, I have to do all the connecting over the command line. Which means it will be much easier if I have the device ID for the remote available. To find it I pair the remote with a Windows machine, and then from Control Panel\\Hardware and Sound\\Devices and Printers I can right click on the device, bring up its properties, and from the bluetooth tab get the unique identifier. For my remote that’s 00:04:20:f8:65:d1.\n\n\nCreate a device in Harmony\nFrom the Harmony app go to create a new device. Under the manufacturer choose Microsoft and for the device choose Kodi. I always forget this part and just try and set it up as a generic PC, which doesn’t work. When it gets to the pairing part of the setup we can connect from the pi\n\n\nConnect from the pi\nSSH into the pi:\nsudo bluetoothctl\nagent on\ndefault-agent\npair 00:04:20:f8:65:d1\nconnect 00:04:20:f8:65:d1\ntrust 00:04:20:f8:65:d1\nThat should do it. Harmony will say it’s connected, you can also check with sudo bluetoothctl paired-devices."
  },
  {
    "objectID": "posts/2020-05-09-samba.html",
    "href": "posts/2020-05-09-samba.html",
    "title": "A basic SAMBA share for home networks",
    "section": "",
    "text": "The original post dealt with setting up a share from an Ubuntu server to provide read/write access without a password. Since writing that part I purchased a Synology NAS and am now including sections on setting up shares for that.\nMost SAMBA guides I find online are some combination of out of date or focused on the enterprise. My objective is to provide a quick reference for setting up files shares from a Linux server to Windows clients, or to properly mount SAMBA shares from a NAS device onto Linux clients. This is only appropriate for a home network. In the case of the Linux server I’m sacrificing security/specific user permissions for being easily able to connect to my file share. On a small LAN where I can easily physically monitor the devices I think this is worth it. Clearly you should not do this for an organization or if you have more sensitive data you’re sharing."
  },
  {
    "objectID": "posts/2020-05-09-samba.html#edit-october-2020",
    "href": "posts/2020-05-09-samba.html#edit-october-2020",
    "title": "A basic SAMBA share for home networks",
    "section": "",
    "text": "The original post dealt with setting up a share from an Ubuntu server to provide read/write access without a password. Since writing that part I purchased a Synology NAS and am now including sections on setting up shares for that.\nMost SAMBA guides I find online are some combination of out of date or focused on the enterprise. My objective is to provide a quick reference for setting up files shares from a Linux server to Windows clients, or to properly mount SAMBA shares from a NAS device onto Linux clients. This is only appropriate for a home network. In the case of the Linux server I’m sacrificing security/specific user permissions for being easily able to connect to my file share. On a small LAN where I can easily physically monitor the devices I think this is worth it. Clearly you should not do this for an organization or if you have more sensitive data you’re sharing."
  },
  {
    "objectID": "posts/2020-05-09-samba.html#what-im-installing-this-on",
    "href": "posts/2020-05-09-samba.html#what-im-installing-this-on",
    "title": "A basic SAMBA share for home networks",
    "section": "What I’m installing this on",
    "text": "What I’m installing this on\nThe current server I’m running this on is an Ubuntu 18.04 machine. Hopefully most of this will translate to similar setups. I’m sure I’ll be upgrading the OS soon so I’ll edit this if I encounter any breaking changes."
  },
  {
    "objectID": "posts/2020-05-09-samba.html#the-steps",
    "href": "posts/2020-05-09-samba.html#the-steps",
    "title": "A basic SAMBA share for home networks",
    "section": "The steps",
    "text": "The steps\n\nInstall samba\nsudo apt install samba\n\n\nBackup any existing smb.conf and then update\n# If you have one already\nsudo mv /etc/samba/smb.conf /etc/samba/smb.conf.bak\nNow setup the new smb.conf:\n[global]\n        map to guest = Bad User\n        logging = systemd\n        log level = 1\n        guest account = &lt;username&gt;\n\n[data]\n        # This share allows guest read write access\n        # without authentication, hope you trust everyone on your LAN\n        path = /mnt/data/ # Or whatever folder you're sharing\n        read only = no\n        guest ok = yes\n        guest only = yes\nWhere &lt;username&gt; is the user on your samba server that has appropriate access to the folder you’re sharing.\nAfter saving the config file you can run testparm to see if there are any syntax errors.\n\n\nRestart and enable SAMBA, give it a test\nFrom the samba server:\nsudo systemctl status smbd # check if it's running\n# If it's running do this\nsudo systemctl restart smbd\n# If it's not do this\nsudo systemctl start smbd\nTry and connect from a Windows machine, make sure you can create and delete files. Back on the samba client you can check if the files you created have the right permissions (should be assigned to the user you created).\nAssuming everything works enable the server so it will reload if you restart the machine. From the samba server:\nsudo systemclt enable smbd"
  },
  {
    "objectID": "posts/2020-05-09-samba.html#no-password-conclusion",
    "href": "posts/2020-05-09-samba.html#no-password-conclusion",
    "title": "A basic SAMBA share for home networks",
    "section": "No Password Conclusion",
    "text": "No Password Conclusion\nThat’s it! Super simple but every time I tried to get a SAMBA share going in the past I always ended up struggling. Hopefully this guide will be helpful to future me and anyone else who’s got a similar situation."
  },
  {
    "objectID": "posts/2020-05-09-samba.html#create-a-credentials-file",
    "href": "posts/2020-05-09-samba.html#create-a-credentials-file",
    "title": "A basic SAMBA share for home networks",
    "section": "Create a credentials file",
    "text": "Create a credentials file\nI’m going to try and pay a reasonable amount of attention to security in this implementation. The NAS has a user set up with read and write permissions for the share that I want to access. I’ll use the suggestions from the Arch Wiki to set up a credential file.\nsudo mkdir /etc/samba/credentials\nsudo echo \"username=&lt;shareusername&gt;\" &gt;&gt; /etc/samba/credentials/share\nsudo echo \"password=&lt;sharepassword&gt;\" &gt;&gt; /etc/samba/credentials/share\nsudo chown root:root /etc/samba/credentials/share\nsudo chmod 700 /etc/samba/credentials\nsudo chmod 600 /etc/samba/credentials/share"
  },
  {
    "objectID": "posts/2020-05-09-samba.html#find-the-appropriate-uid-and-gid-to-assign-ownership",
    "href": "posts/2020-05-09-samba.html#find-the-appropriate-uid-and-gid-to-assign-ownership",
    "title": "A basic SAMBA share for home networks",
    "section": "Find the appropriate UID and GID to assign ownership",
    "text": "Find the appropriate UID and GID to assign ownership\nWhen creating the /etc/fstab entry for the share mount we want to assign ownership to the user that will actually be accessing the files. This is done by UID and GID. For my single user systems that’s usually 1000:1000 but I like to double check and I usually have to look up the command so here it is.\nid -u &lt;username&gt;\nid -g &lt;username&gt;\nWill give the UID and GID for user"
  },
  {
    "objectID": "posts/2020-05-09-samba.html#create-the-fstab-entry",
    "href": "posts/2020-05-09-samba.html#create-the-fstab-entry",
    "title": "A basic SAMBA share for home networks",
    "section": "Create the fstab entry",
    "text": "Create the fstab entry\nNow we just need to create an entry in fstab to the share:\n//&lt;server&gt;/&lt;share_path&gt; /mnt/&lt;share_point&gt; cifs _netdev,uid=&lt;uid&gt;,gid=&lt;gid&gt;,credentials=/etc/samba/credentials/share 0 0"
  },
  {
    "objectID": "posts/2020-05-09-samba.html#nas-conclusion",
    "href": "posts/2020-05-09-samba.html#nas-conclusion",
    "title": "A basic SAMBA share for home networks",
    "section": "NAS Conclusion",
    "text": "NAS Conclusion\nThis extension to the post just has a few code snippets that I’ve found useful. There’s not a lot of exposition on why I’ve set things up the way I did. It’s more meant as a reference for future me, but hopefully it’s useful to others."
  },
  {
    "objectID": "posts/2023-04-08-managed-switch.html",
    "href": "posts/2023-04-08-managed-switch.html",
    "title": "Setting up my first managed switch",
    "section": "",
    "text": "I bought another switch, I guess I’m a glutton for punishment. This one has some different connectivity and configuration requirements so I’ll document them at the bottom of the post."
  },
  {
    "objectID": "posts/2023-04-08-managed-switch.html#try-some-other-consoles-and-commands",
    "href": "posts/2023-04-08-managed-switch.html#try-some-other-consoles-and-commands",
    "title": "Setting up my first managed switch",
    "section": "Try some other consoles and commands",
    "text": "Try some other consoles and commands\nFor whatever reason upon trying to reconnect I’m getting either nothing from the terminal or some random gibberish characters, or parts of what seem to be the prompt or menu screen, but not rendered correctly to actually read. I assume there’s something wrong with how my serial connection is configured. I tried connecting with minicom instead, and got similar results. I tried connecting at different baud rates, but that also didn’t imporve things. I read that the switch would auto-negotiate based on whatever rate I first connected to it with, so I reset the switch and tried connecting at 38400 since I’d seen that in some guides, but it was still basically the same. At least I learned the proper way to end a screen session with ctrl+a and then k."
  },
  {
    "objectID": "posts/2023-04-08-managed-switch.html#realize-i-can-telnet-in",
    "href": "posts/2023-04-08-managed-switch.html#realize-i-can-telnet-in",
    "title": "Setting up my first managed switch",
    "section": "Realize I can telnet in",
    "text": "Realize I can telnet in\nNow that I have an IP address assigned, it looks like I can telnet in from pfsense with telnet 192.168.10.2. I’d still like to get the serial interface figured out as a fallback, but at least this lets me work through the menus while I’m figuring that out.\nJust as a quick first test I see if I can ping out from the switch, and I cannot. I can ping my default gateway, but I can’t ping anything on the LAN or internet. That’s weird, but I’m coming back to that later, right now we’re getting the serial console working properly.\nFrom the telnet session I run show console to get my serial config:\n Console/Serial Link\n\n  Inbound Telnet Enabled [Yes] : Yes\n  Web Agent Enabled [Yes] : Yes\n  Terminal Type [VT100] : VT100\n  Screen Refresh Interval (sec) [3] : 3\n  Displayed Events [All] : All\n\n  Baud Rate [Speed Sense] : speed-sense\n  Flow Control [XON/XOFF] : XON/XOFF\n  Session Inactivity Time (min) [0] : 0"
  },
  {
    "objectID": "posts/2023-04-08-managed-switch.html#try-with-hard-coded-baud-rates-and-putty-finally-figure-it-out-sort-of",
    "href": "posts/2023-04-08-managed-switch.html#try-with-hard-coded-baud-rates-and-putty-finally-figure-it-out-sort-of",
    "title": "Setting up my first managed switch",
    "section": "Try with hard coded baud rates and putty, finally figure it out (sort of)",
    "text": "Try with hard coded baud rates and putty, finally figure it out (sort of)\nLet’s try hard coding the baud rate to what I’m using. First I run config to get into config mode, then console baud-rate 115200 to set the rate, then write memory to save the setting, and reload to reboot the switch. After giving it a minute to come back up I re-run my screen command. Still doesn’t work. I wonder if this is some weird quirk of trying to do things over ssh. Let’s connect my laptop directly and try it out. That will introduce the added factor of using putty into the mix, but oh well.\nWorking with putty seemed to work a bit better. I still ended up with blank screens but with a bit of fussing around I was able to get it started up again. I wonder if I just left it in a weird state before and if I can get back in cleanly remotely now.\nOk yeah, that seems to be it. I guess I’ll have to remember to leave the session in a clean state. Let’s see if I can quit out and come back in. I can, ok, must have just been something about the state I left it in. Back to actually setting up this switch.\nActually, one more thing. Let’s set the baud-rate back to auto. Again from the switch I run config to get into config mode, then console baud-rate speed-sense then write memory then reload. After rebooting the switch I reconnect with sudo screen /dev/ttyUSB0 without specifying a speed. It auto connects at 9600 baud, which seems to work fine."
  },
  {
    "objectID": "posts/2023-04-08-managed-switch.html#failure",
    "href": "posts/2023-04-08-managed-switch.html#failure",
    "title": "Setting up my first managed switch",
    "section": "Failure",
    "text": "Failure\nAfter all that I swapped over all the ports, took out my PoE injectors, fired everything up and couldn’t connect to anything wired or get power to my access points. I suppose I could keep hacking at this but we’ve gone well over the amount of effort I feel justified putting in to avoid having two PoE injectors in my rack so I’m going to give up. Fortunately the switch was fairly cheap."
  }
]