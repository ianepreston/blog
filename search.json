[
  {
    "objectID": "posts/2020-11-25-dotfiles.html",
    "href": "posts/2020-11-25-dotfiles.html",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "",
    "text": "This is part 3 of a 4 part series describing how I provision my systems. Links to each part are below:"
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#setup.sh",
    "href": "posts/2020-11-25-dotfiles.html#setup.sh",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "setup.sh",
    "text": "setup.sh\nThis file either links in or generates the config file for RCM (rcrc). This is the file that identifies which tags are applicable for the machine so it has to be configured properly before the rest of the dotfiles can be brought up. The base implementation checks what Operating System you’re running and adds tags for that. At work I have it generate additional tags for which user is running it so I can create user specific tagged files (for things like email addresses).\nAfter running this script, if we didn’t link in an already existing .rcrc file then you’ll have a host specific one generated, but it won’t be saved in the repository, it will just be a reglar file. If as prompted you run mkrc -o ~/.rcrc it will add a host specific rcrc file to the repository."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#base-rcrc",
    "href": "posts/2020-11-25-dotfiles.html#base-rcrc",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "base-rcrc",
    "text": "base-rcrc\nThis file is used by setup.sh to generate the host specific ~/.rcrc. The script adds tags to this file based on the operating system you’re running. You can add additional tags if you’d like."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#other-root-files",
    "href": "posts/2020-11-25-dotfiles.html#other-root-files",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "other root files",
    "text": "other root files\nThe other files in the root of the repository are generic repository management files. README.md will show on the base of the page on GitHub and should point back to this blog post for more details. I picked GPL V3 for the license somewhat arbitrarily. I think I used the GitHub license picker helper for it. .gitignore and .gitattributes handle files for git to ignore and enforce consistent line break characters. .editorconfig tells a variety of text editors things like whether to use tabs or spaces for indentation."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#completions",
    "href": "posts/2020-11-25-dotfiles.html#completions",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "completions",
    "text": "completions\nThese scripts let you tab complete commands for certain applications. At the time of this writing I have completions for git, pipx and poetry installed."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#distrosmanjaroaliases",
    "href": "posts/2020-11-25-dotfiles.html#distrosmanjaroaliases",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "distros/manjaro/aliases",
    "text": "distros/manjaro/aliases\nI don’t actually use manjaro, but I wanted to keep this in as an example for myself of how to set distribution specific functionality."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#linux",
    "href": "posts/2020-11-25-dotfiles.html#linux",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "linux",
    "text": "linux\nThis has a few commands to set start or open to run xdg-open in linux. Makes the syntax compatible against platforms. That would be for opening a file in a gui rather than with a command line app."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#macos",
    "href": "posts/2020-11-25-dotfiles.html#macos",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "macos",
    "text": "macos\nI don’t have any mac machines to test this stuff out on right now. It’s got a few files that presumably help make behaviour consistent on macs."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#windows-wsl",
    "href": "posts/2020-11-25-dotfiles.html#windows-wsl",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "windows-wsl",
    "text": "windows-wsl\nSimilar to the mac and linux entries above. Lets you use the same commands regardless of your specific platform."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#nerdfonts",
    "href": "posts/2020-11-25-dotfiles.html#nerdfonts",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "nerdfonts",
    "text": "nerdfonts\nThis maps a bunch of nerd fonts to environment variables so they can be included in shell scripts. It lets you do things like put a check mark in your command prompt. Very important stuff."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#shared",
    "href": "posts/2020-11-25-dotfiles.html#shared",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "shared",
    "text": "shared\nThis is where the bulk of the content is in the bash directory. All of these files are cross platform and should work the same on linux, mac or WSL.\n\naliases\nBasically these are all the command shortcuts. For example alias grep=\"grep --color\" means you can just type grep but get nicely coloured results.\n\n\nexports\nThis is where environment variables are set. For example EDITOR=vim is set here.\n\n\nfunctions\nThis is where user defined functions/tools live. For example extract is defined here to call the appropriate underlying app to extract a file based on its extension.\n\n\noptions\nSets a bunch of shell options. Things like turning on vi mode for the command line.\n\n\nother\nA catch all. Code to set up conda, manage the file path, and actually set the appearance of my command prompt all live here.\n\n\nthird party\nA place to dump cool code snippets you found on the internet that you want to be able to manage in your shell."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#bash_logout",
    "href": "posts/2020-11-25-dotfiles.html#bash_logout",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "bash_logout",
    "text": "bash_logout\nClear the screen when you log out. I’m not sure if I actually need this, doesn’t seem to hurt"
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#bash_profile",
    "href": "posts/2020-11-25-dotfiles.html#bash_profile",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "bash_profile",
    "text": "bash_profile\nI’m sure in theory there’s a difference between this file and .bashrc but in practice they seem to be the same. Just map this one to load ~/.bashrc so whichever one your terminal expects you get the same result."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#bashrc",
    "href": "posts/2020-11-25-dotfiles.html#bashrc",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "bashrc",
    "text": "bashrc\nbashrc configures your shell on login. Brennan has a nice modular design that I’m going to emulate. Basically nothing goes in bashrc itself, rather it walks through all the folders in the previously described bash folder and adds them in (at least those relevant to your Operating System). A snippet of what that looks like is below.\n# We want to walk \"outside\" in... which is to say run all options files first, then all\n# exports, then all functions, etc.\nfor folder in \"options\" \"exports\" \"functions\" \"third-party\" \"other\" \"aliases\"; do\n  for base in \"shared\" \"$OS_PRIMARY\" \"distros/$OS_SECONDARY\"; do\n    for root in \"$DOTFILES/bash\" \"$DOTFILES_PRIVATE/bash\"; do\n      if [[ -d \"$root/$base/$folder\" ]]; then\n        for file in $root/$base/$folder/*.bash; do\n          # shellcheck source=/dev/null\n          source \"$file\"\n        done\n      fi\n    done\n  done\ndone\nAll the actual functionality lives in the bash folders of the dotfiles repositories and only this file itself needs to be linked in by RCM. Distribution and OS specific functionality can be managed by just placing the script in the appropriate folder. Because of the order of execution the more granular files will overwrite more general settings if there’s a conflict."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#other-files",
    "href": "posts/2020-11-25-dotfiles.html#other-files",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "other files",
    "text": "other files\n\ndircolors: make ls show pretty colours.\ngitignore: files and patterns to ignore in all git repositories\ninputrc: manage basic keyboard mappings for the shell (home to go to the beginning of the line for example)\nprettierrc: configurations for the code formatter prettier. Kind of like black for other languages\ntmux.conf: configuration for tmux. I don’t use tmux enough to have strong opinions about these commands so the commenting is pretty sparse at the time of this writing"
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#config-folder",
    "href": "posts/2020-11-25-dotfiles.html#config-folder",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "config folder",
    "text": "config folder\nPolite applications store their configuration files here rather than your home directory. The Arch Wiki has a good list of polite applications and how to override some of the impolite ones. The folders all correspond to the name of the application they configure (e.g. git) so they layout is pretty self explanatory."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#host--folders",
    "href": "posts/2020-11-25-dotfiles.html#host--folders",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "host-* folders",
    "text": "host-* folders\nHost specific configs. Everything within here will have the same layout as the rcs folder above it, but will have machine specific configs. For my setup that’s just the ~/.rcrc file that sets the tags for everything else on the machine."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#tag--folders",
    "href": "posts/2020-11-25-dotfiles.html#tag--folders",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "tag-* folders",
    "text": "tag-* folders\nThe same idea as hosts, except each host can have multiple tags. In general this is used for OS specific configurations. At work I also add tags for each user on the system for things like configuring e-mail addresses."
  },
  {
    "objectID": "posts/2020-11-25-dotfiles.html#vifm-and-vim",
    "href": "posts/2020-11-25-dotfiles.html#vifm-and-vim",
    "title": "Automating provisioning Arch continued - dotfiles",
    "section": "vifm and vim",
    "text": "vifm and vim\nThese folder should really be under config. They’re just the settings for vim and vifm. Rude of them to demand their own space in ~."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html",
    "href": "posts/2023-01-21-proxmox3.html",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "",
    "text": "This is the third post ( part 1, part 2) documenting my adventures setting up a home cluster. In this one I will try a few different methods of getting VMs installed on proxmox. As with the previous posts, this is not intended to be a how to guide from an expert. I haven’t used proxmox before working on this project, so I’m mostly doing this to document what I do for future reference, or maybe provide others with the perspective of what it’s like to work on proxmox as a relative beginner."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html#create-the-vm",
    "href": "posts/2023-01-21-proxmox3.html#create-the-vm",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "Create the VM",
    "text": "Create the VM\nThe most obvious way to install a VM is through the UI. I know I won’t want to take this approach indefinitely as it involves manual work and isn’t reproducible (at least not easily), but it seems like the right place to start, both to ensure I don’t have any unforeseen issues with my setup, and also to provide a baseline for comparison when I try other methods later.\nSelecting one of my nodes from the web interface I click “Create VM”. In the first tab I pick the node to install to and give it a name, we’ll do ubuntu-test for this. I could also assign it to a resource pool if I had any of those created but I don’t so I won’t. The other thing I can assign is a VM ID, which is the unique numeric identifier proxmox uses internally. At this point I’m fine to let proxmox manage that though so I’ll leave it on the default.\nChecking the advanced options I can also configure the VM to start at boot so it will come back up if I reboot my cluster. I can also configure the order it starts/stops. The start at boot setting seems like it would be handy for production services, but I’m just testing so I’ll leave it for now.\nOn the next tab I can configure the OS. I’ve already configured my NAS (set up in part 2) to hold things like ISO images for installing and uploaded an Ubuntu 22.10 server image, so I’ll select that. The guest OS settings are already correctly set on Linux with a modern kernel so I’m all good there.\nNext up is the System tab. The first option is Graphic Card. There’s a ton of options under this one, but at this point I don’t have any intention of installing anything that will care so I’ll leave it at default. Maybe at some point I’ll have a system with a GPU that I want to pass through, or will need a Windows server, but not right now. I also have to pick a machine option. Based on the docs as long as I don’t want to do PCIe passthrough I can stick with the default, so I will for now. Next I pick a SCSI controller. Again, referring to the docs the VirtIO SCSI Single option that it had selected by default seems perfect for me. There’s also a checkbox for Qemu Agent. Reading the docs this seems like a handy thing to have, so I’ll turn it on (looks like mostly it’s for cleaner shutdown and pausing during disk backups). The last thing on this tab is whether to enable TPM. Since I’m not making a Windows image I don’t need this, so I’ll leave it unchecked.\nFollowing that we’re on to Disks. I can create multiple disks I’m sure, but for now let’s just set up one. First I make sure that the backing storage is my local-zfs storage, which is the NVME drive on the host, rather than my NAS. I haven’t configured the SSD in these hosts yet, I’m planning to set up ceph on them but that’s for a future post. The other basic thing to set on this page is disk size. I’m not planning to keep this image around, so I’ll stick with the default 32GB for now. The Bus/Device field defaults to the SCSI interface I set up on the last tab so that seems fine. There’s an option for cache mode as well. Right now I’m not really sure what that does, but from the docs the default of no cache seems like it will work for me, so I’ll leave it. Taking a look at the docs it seems like I want to have the Discard option checked so I’ll do that. From the docs IO Thread only seems like it really matters if I have multiple disks attached, but I don’t see the harm of turning it on so let’s do that. I’ll check SSD emulation since the underlying disk really is an SSD and the guest OS might as well think so too. I’ll uncheck the backup option on this one, since I’m planning to just destroy this VM shortly after I create it and I don’t need backups hanging around. I want to be able to try replicating this VM to different hosts, and I’d want this disk to be included, so I’ll leave skip replication unchecked. The last thing I have to pick is the Async IO option. From reading this it seems like the default io_uring will work for me, I’m not deep enough on how this sort of thing works to have strong opinions or requirements so I’ll go with the default.\nNow we come to CPU. For sockets I’ll leave it at 1, since all my hosts have only 1 physical socket. For cores my hosts have either 4 or 6 cores, so there’s definitely no point going over 6. Since this is just a test machine let’s just give it 2. For CPU type I’m going to leave it on the Default (kvm64). From the docs on CPU type if I set the CPU to host it will exactly match the CPU flags and type of my host system, but I might have migration issues across hosts, since they’re not all the exact same CPU. The default will allow easier migration, but might disable some specific CPU flags that would be beneficial. For now I’ll stick with the easy option. There’s some other stuff for CPU limits and extra CPU flags here that I’m also going to leave alone for now.\nNow we’re on to memory. Each of these hosts has 32GB of memory, so I don’t really have to be cheap here, at least while I’m testing. Under advanced I can set a memory floor and enable/disable ballooning. From the docs, I want to have ballooning enabled, even if I have the memory floor and ceiling set the same, so that my host can see how much memory the VM is actually using. If I was running a bunch of VMs with dynamic memory requirements I could see overallocating the max across them and setting the floor for each. In this case I’m just going to leave it at the default 2GB since I’m not going to actually run anything on this VM.\nAlmost done, next up is network. I’ve only got one bridge right now so I’ll leave that selected. I’m not currently doing any VLAN stuff in my network so I’ll leave the VLAN tag entry blank. For model I’ll stick with VirtIO as the docs say that’s the one to pick for best performance as long as your host supports it. The firewall option is checked by default. I haven’t looked into the proxmox firewall at all at this point, but let’s leave that on for now. I can also do some rate limiting and other fancy network config here but I’m going to leave those on the default for now.\nThe only thing to do now is confirm my settings and actually create the VM. I’ll check start after created so it fires up right away."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html#configure-the-vm",
    "href": "posts/2023-01-21-proxmox3.html#configure-the-vm",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "Configure the VM",
    "text": "Configure the VM\nAfter waiting a little bit I can see my newly created VM listed under the node I set it up on. Clicking into that VM and selecting the Console section I can see that I’m in the Ubuntu server installation wizard. Since this isn’t a post about installing Ubuntu server I’ll work through the menus without writing everything down. Going through the install worked fine until it came time to reboot and it failed to unmount the virtual CD-ROM that had the installation ISO. I went to the hardware tab on the VM in the proxmox interface, removed the CD-ROM and rebooted. After the reboot the VM came up fine, and I was able to ssh into it from my desktop."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html#thoughts-on-manual-templates",
    "href": "posts/2023-01-21-proxmox3.html#thoughts-on-manual-templates",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "Thoughts on manual templates",
    "text": "Thoughts on manual templates\nThis wasn’t too bad. There are a few tweaks I’d want to apply, like adding the guest agent into the machine, but overall template creation is pretty easy. I could see wanting to update my templates semi regularly when new versions of base OSs come out though, and I’d like to understand more of the theory behind how this actually works, since a lot of what I did was pretty much copy paste. To do that, I’ll clear these out and look into some other template creation options."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html#cleaning-up",
    "href": "posts/2023-01-21-proxmox3.html#cleaning-up",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "Cleaning up",
    "text": "Cleaning up\nI don’t want lingering stuff from this experiment hanging out on my nodes, so let’s go in and see what I have to get rid of. First is the created VMs - I can stop and remove them from the UI easily enough. Same deal for the template VM. I checked the local storage through the UI as well and it looks like any virtual disks I created were removed when I got rid of the VM. The only other thing to remove was that initially downloaded cloud image, so I went into the shell for the node and just ran rm to get rid of that."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html#automate-creating-another-template",
    "href": "posts/2023-01-21-proxmox3.html#automate-creating-another-template",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "Automate creating another template",
    "text": "Automate creating another template\nHaving a working Ubuntu template is pretty handy, but what if I want to branch out? Can I apply this approach to other distros? I’m pretty sure this approach will work fine with another debian based distro, and probably even another fairly standard Linux like CentOS will be fine (although I should test). But what about weird ones? Specifically I want to see if I can get this working on flatcar Linux since I want to try using it for my kubernetes nodes. Let’s walk before we try running though and extend to another version of Ubuntu.\nThe first thing I want to do is tweak how I’m numbering my templates. Right now each template gets a variable set for its whole VM ID. I’d like to break that out into chunks. The first digit should just always be 8 (at least for now) to indicate a template and keep it out of the range of actual VMs I’m deploying. The next one I’m thinking should be the node the template is created on, and then the last two digits can be an identifier for the specific template. This actually wasn’t bad at all. The one variable definition gets a little long, but basically I just go from one line in defaults of build_vm_id: \"8000\" to\nbuild_vm_start_digit: \"8\"\nbuild_vm_host_digit: \"{{ ansible_hostname[-1] }}\"\nbuild_vm_template_num: \"0\"\nbuild_vm_id: \"{{ build_vm_start_digit }}{{ build_vm_host_digit }}{{ '%02d' | format(build_vm_template_num|int) }\"\nThis relies on all my proxmox nodes having hostnames of the format pve<Num> but I can work with that. The number of digits in my IDs will change if I get more than 9 nodes or 99 templates too, but I’m not really expecting that to happen, and I don’t even think that would necessarily break anything if it did, so I won’t worry about it for now.\nWith that slight modification to the role complete I set my playbook to call the role twice, modifying the variables from the defaults for just the template name, the template number, and the URL of the cloud image to build from for Ubuntu Jammy and Kinetic.\nAround this time I realized it was going to be a little tedious running the template build script each time I added a template, so I added a handler to the role to execute the template build script whenever it made a change. It took a little bit of tweaking to figure out that I needed the full path of the script I wanted to run, as well as set it as the working directory so I could call the subscript that defines all the build variables. After those changes the handler triggered properly and built my templates whenever the script changed, or I added a new template to build.\nAt this point the general template creation process is working quite nicely for Ubuntu versions, but what about other distros? Let’s give debian a shot. I grabbed the cloudgeneric version of debian bullseye from their official cloud images page and plugged it into my playbook. No problem at all. The template built, I was able to build an image from it just the same as the Ubuntu ones.\nLet’s get a bit braver and branch out to an even more different distro Rocky Linux. This one might come in handy if I want to try out anything enterprisey or just want to see what the Red Hat experience is like. I found their generic cloud images here and plugged the link into my playbook. The template built ok, but trying to run the VM I ran into problems where it got stuck on a line that said Probing EDD (edd=off to disable)... ok and just hung out there. Similar to the weird boot loop I got deploying from my NAS I wasn’t able to shut down the VM from the Web UI and had to go into the terminal on the node and ps aux | grep \"/usr/bin/kvm -id <VM ID>\" to find its PID and kill -9 it before I could remove the VM. I guess I have to do some troubleshooting. A little searching finds that this error is pretty common, although it doesn’t actually relate to the message, but something that’s happening after. There are a few potential kernel configs I might be able to change, but as I’m poking around in the machine I notice something interesting, it’s got way more disk to start than my Ubuntu templates did. I wonder if I’m somehow filling the disk, so I use that command from the previous section and resize the disk on a newly cloned template before starting it up. Disappointingly this did not solve the problem. Another weird thing I noticed during the start up is that CPU usage on the VM is pinned at right around 50%. Since I gave it 2 cores that suggests that one core is working flat out on something. Several of the posts indicated that after about 10 minutes the system would come up. That’s obviously a terrible startup time, but I’d like to give it a while to see if I at least have the same problem. So I go do some reading and let this VM run for a while… and discover that tragically the usually perfect strategy of ignoring a problem and hoping it goes away doesn’t work in this case."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html#examine-the-template-creation-script-and-modify-it",
    "href": "posts/2023-01-21-proxmox3.html#examine-the-template-creation-script-and-modify-it",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "Examine the template creation script and modify it",
    "text": "Examine the template creation script and modify it\nSomething about how I have my VM configured is not playing nice with Rocky Linux. It could just be a very specific thing that I only want to modify for that distro, but I also just copy-pasted most of the other template creation parameters from some guy on the internet. So before I assume that my basic parameters are the best and it’s only Rocky that needs to be modified, let’s examine those options that I’m using and see if I want to modify any of them. Maybe while I’m at it I’ll fix my Rocky issue (or introduce new ones to working distros), but at a minimum I’ll have a better understanding of what’s going on.\nThe first little bit of the script downloads a cloud image, and then uses virt-customize to update packages, install a list of packages (just qemu-guest-agent and cloud-init by default), copy in a build-info file with some metadata about the template build, copy in some ssh related files and have a script to set them up on first boot (note to self, maybe that’s the part that’s breaking in Rocky, I’ve only tested that script in debian and Arch based distros so far). That stuff (except maybe the ssh part) is all straightforward and I understand what it’s trying to do, so let’s skip to the next line:\nqm destroy ${build_vm_id}\nRemove the old template before you build a new one, makes sense.\nqm create ${build_vm_id} --memory ${vm_mem} --cores ${vm_cores} --net0 virtio,bridge=vmbr0 --name ${template_name}\nCreate a new VM (that we’ll turn into a template later) with an ID of build_vm_id, memory and cores set to our variables, and a virtio network adapter, which is what I did in the manual template creation. Finally we give it a name based on the template_name variable. So far so good, but I had a lot more options available when I built a VM manually earlier in this post, anything else I should set? Reading back through my manual config I set basically everything else to defaults so I think I’m good here. Let’s see what’s next.\nqm importdisk ${build_vm_id} ${image_name} ${storage_location}\nOk, this is fine, I’m importing the disk image I downloaded and modified to the VM I created and putting it in the storage location I specify. All seems fine. Maybe I’ll need to revisit this if I take another crack at storing these templates on my NAS, but fine for now.\nqm set ${build_vm_id} --scsihw ${scsihw} --scsi0 ${storage_location}:vm-${build_vm_id}-disk-0\nOk, here’s where I deviate from what I picked in the manual build. In my defaults (based on the script I copied in) I had scsihw set to virtio-scsi-pci, whereas in my manual build I went with virtio-scsi-single. I’m struggling to find the actual difference between these settings, but let’s change it for kicks for now.\nqm set ${build_vm_id} --ide0 ${storage_location}:cloudinit\nAdd the cloud-init drive, seems fine. It’s emulating a CD drive so ide makes sense.\nqm set ${build_vm_id} --nameserver ${nameserver} --ostype l26 --searchdomain ${searchdomain} --ciuser ${cloud_init_user}\nAdd a couple defaults to the cloud-init template and set the ostype to linux (l26). No worries there.\nqm set ${build_vm_id} --boot c --bootdisk scsi0\n--boot c tells it to boot from hard disk (as opposed to CD or network) and we set the bootdisk to the image that’s been mounted to the VM. Seems fine.\nqm set ${build_vm_id} --agent enabled=1\nThis turns on qemu agent, which we want.\nOne thing I noticed from going through this is I had some lines that set multiple options, even though they weren’t necessarily related. So I cleaned that up to be one option per line. Easier to parse and modify that way.\nI took a quick look back at the manual config section and didn’t see anything else that stood out, so I guess I have to get back to fixing Rocky Linux."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html#get-back-to-making-rocky-linux-work",
    "href": "posts/2023-01-21-proxmox3.html#get-back-to-making-rocky-linux-work",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "Get back to making Rocky Linux work",
    "text": "Get back to making Rocky Linux work\nOk, that was a fun side quest, but let’s get back to figuring out Rocky. I re-run my template creation playbook, just in case that storage config changed anything. I also found a proxmox forum post where someone was having the same problem with a particular RHEL image, but no solution. That post also said it worked fine with RHEL 7 and the issue was with 6. I’m trying Rocky 9 (I believe they use the same version as RHEL for compatibility) so I don’t know if that’s helpful. This post suggests the output just means my console output is being redirected somewhere else, so I’m not seeing whatever the actual issue is. I guess I should fix that first regardless. One suggested solution there is to change the default tty from serial. An alternative approach there, is to check out the proxmox docs and enable serial out on the VM with qm set <VM ID> -serial0 socket. Let’s add that line to my template and see if I get anything. A little bit of progress in that it doesn’t just tell me I don’t have serial on that machine, but I also only see starting serial terminal on interface serial0 (press Ctrl+O to exit), which isn’t exactly informative. Let’s ditch my ssh script setup on first boot, just to make sure that’s not what’s hanging the template. Removing it from the template script gives me the same issue, so the problem is elsewhere. Just for kicks, let’s try a different Rocky cloud image. I found this blog that’s using the GenericCloud image rather than the GenericCloudBase image I was using. I’m not sure why I picked GenericCloudBase to begin with so let’s swap over and see what happens. Still nothing. Ok, the blog also has a bunch more cloud-init modules installed than I do. Maybe one of them will fix things. Let’s add them to the package list for that template. Still no luck. Ok, back to basics. We have a blog post where someone made a template, and apparently it worked. Let’s try manually working through those instructions and see what happens. Well, first problem their link goes to a pinned daily build of Rocky that’s no longer available on the site. Fine, we’ll do the latest one and hope that there’s not just some daily build issue that’s leading to all of this. So I get the same error following the guide. After a little more digging I find a link in the rocky vault to the exact image that the blog was using. Let’s apply my template to that image. Slightly better luck. It still bootloops, but I can actually see things in the console and by being very speedy I was even able to get a screencap of the kernel panic error it’s throwing.\n\n\n\n\n\nNodes\n\n\nFinding out that this was the issue led me to a proxmox forum post where it turns out lots of people are having this issue with Rocky 9 if they set their CPU to the default kvm64. I reset my VM to use host for the CPU. That fixed the boot loop error but led to another error. At this point I decided I didn’t feel like running Rocky Linux very much."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html#try-arch-linux",
    "href": "posts/2023-01-21-proxmox3.html#try-arch-linux",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "Try Arch Linux",
    "text": "Try Arch Linux\nRocky was supposed to be an intermediate difficulty distro to test out my template process. I don’t actually have a use case for it, I just figured it would be more different than debian but less different than flatcar when it comes to testing. I’m really hoping that I just got unlucky and that other distros won’t be so hard. Let’s see if that’s correct. I don’t have an immediate use case for Rocky, but I do like running Arch, it’s what my current server has. Let’s try that. Building the template goes fine, and this one actually boots to a login prompt from the proxmox UI, so we’re on a happier path than with Rocky already. At first it seemed like Arch wasn’t updating my default user. I started up the image and tried to ssh in but got a connection refused error. Trying to ssh in as the default arch user that the image uses got a permission denied error instead. After some testing it turns out that cloud-init just takes longer to complete on first boot in Arch, I think because it has to do more package updating. If I just left the VM running for a bit I was able to ssh right in."
  },
  {
    "objectID": "posts/2023-01-21-proxmox3.html#try-flatcar-linux",
    "href": "posts/2023-01-21-proxmox3.html#try-flatcar-linux",
    "title": "Home cluster part 3 - Setup VM templates on proxmox",
    "section": "Try flatcar Linux",
    "text": "Try flatcar Linux\nI’ve never used flatcar before, but it sounds interesting and this blog recommended it for self hosting kubernetes so I’d like to have it available in my environment. I found this repository which had its own scripts to create a flatcar template. Most of it looks broadly similar to the approach I’ve been taking, so let’s try it out. I notice that flatcar images come in .img.bz2 format instead of .img or .qcow2 like the other files I’ve downloaded. I may have to add in some logic to the script to extract images in that case. As a first step though I just tried running the whole workflow as is. That got me a template built, but the VM I created off it couldn’t find any bootable media, suggesting the disk creation didn’t work as intended. Probably because I have to extract it first. After adding a little bit of logic to my image building script:\nif [ \"${image_name##*.}\" == \"bz2\" ]; then\n    bzip2 -d ${image_name}\n    image_name=\"${image_name%.*}\"\nfi\nI got the VM to boot. I could auto login as user core over serial, but it looks like none of my other cloud-init config stuff worked. This post is already getting super long though so I’m going to save getting a fully working flatcar image for a separate post and declare victory on my general goal of “be able to make templates automatically”."
  },
  {
    "objectID": "posts/2020-07-20-pi-bluetooth.html",
    "href": "posts/2020-07-20-pi-bluetooth.html",
    "title": "Connecting a Harmony remote to a raspberry pi",
    "section": "",
    "text": "Introduction\nThis is a little stub post for me. I have a Logitech Harmony remote that I use to control a raspberry pi running Kodi for my media center. Occasionally I have to reinstall it and I always have to google a few things to remember how to do it, so this is just to consolidate that info.\n\n\nGet the Harmony Bluetooth ID\nSince I don’t have a GUI on my pi beyond Kodi, I have to do all the connecting over the command line. Which means it will be much easier if I have the device ID for the remote available. To find it I pair the remote with a Windows machine, and then from Control Panel\\Hardware and Sound\\Devices and Printers I can right click on the device, bring up its properties, and from the bluetooth tab get the unique identifier. For my remote that’s 00:04:20:f8:65:d1.\n\n\nCreate a device in Harmony\nFrom the Harmony app go to create a new device. Under the manufacturer choose Microsoft and for the device choose Kodi. I always forget this part and just try and set it up as a generic PC, which doesn’t work. When it gets to the pairing part of the setup we can connect from the pi\n\n\nConnect from the pi\nSSH into the pi:\nsudo bluetoothctl\nagent on\ndefault-agent\npair 00:04:20:f8:65:d1\nconnect 00:04:20:f8:65:d1\ntrust 00:04:20:f8:65:d1\nThat should do it. Harmony will say it’s connected, you can also check with sudo bluetoothctl paired-devices."
  },
  {
    "objectID": "posts/2020-05-09-samba.html",
    "href": "posts/2020-05-09-samba.html",
    "title": "A basic SAMBA share for home networks",
    "section": "",
    "text": "The original post dealt with setting up a share from an Ubuntu server to provide read/write access without a password. Since writing that part I purchased a Synology NAS and am now including sections on setting up shares for that.\nMost SAMBA guides I find online are some combination of out of date or focused on the enterprise. My objective is to provide a quick reference for setting up files shares from a Linux server to Windows clients, or to properly mount SAMBA shares from a NAS device onto Linux clients. This is only appropriate for a home network. In the case of the Linux server I’m sacrificing security/specific user permissions for being easily able to connect to my file share. On a small LAN where I can easily physically monitor the devices I think this is worth it. Clearly you should not do this for an organization or if you have more sensitive data you’re sharing."
  },
  {
    "objectID": "posts/2020-05-09-samba.html#what-im-installing-this-on",
    "href": "posts/2020-05-09-samba.html#what-im-installing-this-on",
    "title": "A basic SAMBA share for home networks",
    "section": "What I’m installing this on",
    "text": "What I’m installing this on\nThe current server I’m running this on is an Ubuntu 18.04 machine. Hopefully most of this will translate to similar setups. I’m sure I’ll be upgrading the OS soon so I’ll edit this if I encounter any breaking changes."
  },
  {
    "objectID": "posts/2020-05-09-samba.html#the-steps",
    "href": "posts/2020-05-09-samba.html#the-steps",
    "title": "A basic SAMBA share for home networks",
    "section": "The steps",
    "text": "The steps\n\nInstall samba\nsudo apt install samba\n\n\nBackup any existing smb.conf and then update\n# If you have one already\nsudo mv /etc/samba/smb.conf /etc/samba/smb.conf.bak\nNow setup the new smb.conf:\n[global]\n        map to guest = Bad User\n        logging = systemd\n        log level = 1\n        guest account = <username>\n\n[data]\n        # This share allows guest read write access\n        # without authentication, hope you trust everyone on your LAN\n        path = /mnt/data/ # Or whatever folder you're sharing\n        read only = no\n        guest ok = yes\n        guest only = yes\nWhere <username> is the user on your samba server that has appropriate access to the folder you’re sharing.\nAfter saving the config file you can run testparm to see if there are any syntax errors.\n\n\nRestart and enable SAMBA, give it a test\nFrom the samba server:\nsudo systemctl status smbd # check if it's running\n# If it's running do this\nsudo systemctl restart smbd\n# If it's not do this\nsudo systemctl start smbd\nTry and connect from a Windows machine, make sure you can create and delete files. Back on the samba client you can check if the files you created have the right permissions (should be assigned to the user you created).\nAssuming everything works enable the server so it will reload if you restart the machine. From the samba server:\nsudo systemclt enable smbd"
  },
  {
    "objectID": "posts/2020-05-09-samba.html#no-password-conclusion",
    "href": "posts/2020-05-09-samba.html#no-password-conclusion",
    "title": "A basic SAMBA share for home networks",
    "section": "No Password Conclusion",
    "text": "No Password Conclusion\nThat’s it! Super simple but every time I tried to get a SAMBA share going in the past I always ended up struggling. Hopefully this guide will be helpful to future me and anyone else who’s got a similar situation."
  },
  {
    "objectID": "posts/2020-05-09-samba.html#create-a-credentials-file",
    "href": "posts/2020-05-09-samba.html#create-a-credentials-file",
    "title": "A basic SAMBA share for home networks",
    "section": "Create a credentials file",
    "text": "Create a credentials file\nI’m going to try and pay a reasonable amount of attention to security in this implementation. The NAS has a user set up with read and write permissions for the share that I want to access. I’ll use the suggestions from the Arch Wiki to set up a credential file.\nsudo mkdir /etc/samba/credentials\nsudo echo \"username=<shareusername>\" >> /etc/samba/credentials/share\nsudo echo \"password=<sharepassword>\" >> /etc/samba/credentials/share\nsudo chown root:root /etc/samba/credentials/share\nsudo chmod 700 /etc/samba/credentials\nsudo chmod 600 /etc/samba/credentials/share"
  },
  {
    "objectID": "posts/2020-05-09-samba.html#find-the-appropriate-uid-and-gid-to-assign-ownership",
    "href": "posts/2020-05-09-samba.html#find-the-appropriate-uid-and-gid-to-assign-ownership",
    "title": "A basic SAMBA share for home networks",
    "section": "Find the appropriate UID and GID to assign ownership",
    "text": "Find the appropriate UID and GID to assign ownership\nWhen creating the /etc/fstab entry for the share mount we want to assign ownership to the user that will actually be accessing the files. This is done by UID and GID. For my single user systems that’s usually 1000:1000 but I like to double check and I usually have to look up the command so here it is.\nid -u <username>\nid -g <username>\nWill give the UID and GID for user"
  },
  {
    "objectID": "posts/2020-05-09-samba.html#create-the-fstab-entry",
    "href": "posts/2020-05-09-samba.html#create-the-fstab-entry",
    "title": "A basic SAMBA share for home networks",
    "section": "Create the fstab entry",
    "text": "Create the fstab entry\nNow we just need to create an entry in fstab to the share:\n//<server>/<share_path> /mnt/<share_point> cifs _netdev,uid=<uid>,gid=<gid>,credentials=/etc/samba/credentials/share 0 0"
  },
  {
    "objectID": "posts/2020-05-09-samba.html#nas-conclusion",
    "href": "posts/2020-05-09-samba.html#nas-conclusion",
    "title": "A basic SAMBA share for home networks",
    "section": "NAS Conclusion",
    "text": "NAS Conclusion\nThis extension to the post just has a few code snippets that I’ve found useful. There’s not a lot of exposition on why I’ve set things up the way I did. It’s more meant as a reference for future me, but hopefully it’s useful to others."
  },
  {
    "objectID": "posts/2022-12-31-proxmox2.html",
    "href": "posts/2022-12-31-proxmox2.html",
    "title": "Home cluster part 2 - Configuring Proxmox",
    "section": "",
    "text": "In part 1 of this series I laid out the rationale for building a homelab cluster, walked through my hardware selection, and ran through the proxmox installer. In this post I’ll document the further configuration of the cluster and its nodes. The actual implementation will be done in ansible and tracked in my recipes repository, so I will keep it light on code in this post. What I’m going to try and document here is the steps I took, why I took them, and any weird edge cases or other fun learning opportunities I encounter in the process."
  },
  {
    "objectID": "posts/2022-12-31-proxmox2.html#failed-first-attempt",
    "href": "posts/2022-12-31-proxmox2.html#failed-first-attempt",
    "title": "Home cluster part 2 - Configuring Proxmox",
    "section": "Failed first attempt",
    "text": "Failed first attempt\nI made a good amount of progress trying to set this up a certain way, but eventually hit a wall. Some of what I tried originally is still interesting, it just didn’t work. The core issue is that I was dynamically identifying which host had the UPS attached to it based on the output of a locally run command on each host. This worked fine in terms of setting conditionals to only do server setup stuff on the node with the UPS attached, but ran into troubles configuring the clients. The clients need to know the hostname of the server, but I couldn’t figure out any way to dynamically identify that host in an ansible playbook. Registered variables from commands (what I was using to identify which host was connected to the UPS) are host variables only, so the other hosts didn’t have access to it. From the look of it you can’t really make a host variable a global variable based on a condition. There might be a way to concatenate all host variables in a way that would let me get <server host> + '' + '' == <server host> as the output for all hosts, but that felt pretty hacky. Based on this I’m just going to hard code which host is directly connected to the UPS and build my playbook from that.\n\nBorked attempt write up\nOne node in my cluster is connected via USB to my UPS. In the event of a power failure I want it to be alerted via that USB connection, and then pass that alert on to the other nodes via NUT. I’m largely relying on the Arch wiki to set this up, even though proxmox is Debian based, just because that wiki is amazing. I also found this repository which has a role configured for setting up nut. It’s set up in a different way than I want, and also has a lot more abstraction that’s good for portability but bad for interpretability, so I won’t use it directly.\nThe first thing I want to do is install the nut utility on all the systems, as with the previous section this is easily accomplished with the apt module in ansible.\nNext I need to identify which system has the UPS USB cable connected to it, as this one will be the NUT server, and the others will be NUT clients. Realistically this is not going to change and I could just hard code it, but I thought it would be fun to figure out how to automate it.\nThe nut package comes with a nut-scanner utility which can be used to identify compatible devices. I can register the output of that command in ansible and then set a conditional to only perform certain operations if the output of the command listed a USB device. To test this before I actually applied anything with conditionals I used the debug module to output which host had the UPS attached. I won’t keep that debug in my final playbook, so I’ll reproduce that part here:\n- name: Check if USB for UPS is connected to this host\n  ansible.builtin.shell: nut-scanner -U -P\n  register: ups_driver\n\n- name: Show me which host has the UPS connected\n  ansible.builtin.debug:\n    msg: System {{ inventory_hostname }} has UPS driver {{ ups_driver }}\n  when: ups_driver.stdout.find('usbhid-ups') != -1\nNext is to configure the driver on connected system. To do this I copy over a ups.conf file based on the output of the nut-scanner command. After copying over the template I test it by sshing into the machine and running upsdrvctl start. Since that looked good I enable the nut-driver service with ansible’s systemd module.\nAfter that it’s time to set up the nut server for clients (both the local machine and the other nodes in the cluster) to connect to. Following the Arch wiki I created a upsd.users file with user configuration for the clients and then tried to enable and start the nut server. I didn’t get an error from ansible for this, but when I tried to check the server I got nothing, and checking the state of the service I saw that it was dead. The relevant lines in the service status seemed to be:\nupsd disabled, please adjust the configuration to your needs\nThen set MODE to a suitable value in /etc/nut/nut.conf to enable it\nTaking a look at that file I see this:\n##############################################################################\n# The MODE determines which part of the NUT is to be started, and which\n# configuration files must be modified.\n#\n# This file try to standardize the various files being found in the field, like\n# /etc/default/nut on Debian based systems, /etc/sysconfig/ups on RedHat based\n# systems, ... Distribution's init script should source this file to see which\n# component(s) has to be started.\n#\n# The values of MODE can be:\n# - none: NUT is not configured, or use the Integrated Power Management, or use\n#   some external system to startup NUT components. So nothing is to be started.\n# - standalone: This mode address a local only configuration, with 1 UPS\n#   protecting the local system. This implies to start the 3 NUT layers (driver,\n#   upsd and upsmon) and the matching configuration files. This mode can also\n#   address UPS redundancy.\n# - netserver: same as for the standalone configuration, but also need\n#   some more network access controls (firewall, tcp-wrappers) and possibly a\n#   specific LISTEN directive in upsd.conf.\n#   Since this MODE is opened to the network, a special care should be applied\n#   to security concerns.\n# - netclient: this mode only requires upsmon.\n#\n# IMPORTANT NOTE:\n#  This file is intended to be sourced by shell scripts.\n#  You MUST NOT use spaces around the equal sign!\n\nMODE=none\nSo based on this I think I need ansible to remove the MODE=none line and change it to MODE=netserver on the server. Probably it will have to be MODE=netclient on the clients, but let’s leave that alone for now. I can handle this using the lineinfile module. After doing this and restarting the nut-server service I ran upsc pveups and had the state of the UPS returned, indicating the config was good for the directly connected node. This is where I got stuck, see the write up above"
  },
  {
    "objectID": "posts/2022-12-31-proxmox2.html#working-second-attempt",
    "href": "posts/2022-12-31-proxmox2.html#working-second-attempt",
    "title": "Home cluster part 2 - Configuring Proxmox",
    "section": "Working second attempt",
    "text": "Working second attempt\nDon’t reinvent the wheel folks. I vendored in this role and got everything working pretty much right away. I did have to hard code which host was attached to the UPS, but that’s a small price to pay. Problem solved!"
  },
  {
    "objectID": "posts/2020-10-06-docker-compose.html",
    "href": "posts/2020-10-06-docker-compose.html",
    "title": "Notes on docker-compose",
    "section": "",
    "text": "Introduction\nThis is going to be a grab bag of docker-compose tips and snippets for things that I do commonly enough that I want to write them down but not commonly enough that I remember the syntax offhand.\n\n\nDifferent file names\nBy default docker-compose wants the compose file to be docker-compose.yml in the same directory as the command is being run. Generally you want to stick with that, but I did come up with a situation where I wanted to give them different names. You can do docker-compose -f <file name> <rest of your commands> to get around this. Note that the -f <file name> has to be at the start, you can’t just put it in like any other flag.\n\n\n.env files\nOne of the reasons I was trying to have different file names was because I wanted a bunch of different compose files to share a .env file. My new solution is to have a .env file in the root directory of where I keep my compose files and then use symbolic links to make a link to that master file in each directory. Depending on how I created the link though I would run into a too many levels of symbolic links error. A clean way to solve this is to navigate to the subdirectory and run ln -s ../.env .env. Whatever you put after -s is literally what’s included in the link, so as long as there’s a .env file in the parent folder this will work, regardless of where in your file system you move these directories.\n\n\nConclusion\nThat’s it for now. I’ll come back and update this document if anything else comes up."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html",
    "href": "posts/2021-12-30-wheretolive.html",
    "title": "Building a where to live app",
    "section": "",
    "text": "To start, here’s the code. I’ll include more specific links to specific parts of the process in detail below.\nI have two goals with this project:\n\nFigure out a good place to live when I move next\nLearn some data engineering and system administration type skills\n\nFor the first goal, I want to scrape real estate sites in my area and assemble a database of listings. I want to supplement that with open data from the city and other sources. I want all of this data to be collected and updated in an automated and efficient process. Finally, I want to be able to analyze this data in order to find the best place to live based on my personal preferences and requirements.\nThe second goal should come about as a consequence of the first. I’ve done web scraping before, but mostly for one off tasks where I can babysit if my results look weird. To store the data that I scrape I’ll use a database. I’ve done lots of querying of databases, but I haven’t had much opportunity to design one, so this will be a learning experience in the regard. I’ll also need to have an ETL pipeline to manage the scheduling, ingestion, and other tasks between the scraper and the database. Finally, I’ll need some way to serve the recommendations."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#too-much-upfront-validation",
    "href": "posts/2021-12-30-wheretolive.html#too-much-upfront-validation",
    "title": "Building a where to live app",
    "section": "Too much upfront validation",
    "text": "Too much upfront validation\nMy first instinct when ingesting data from a source I didn’t control (the API endpoints for rentfaster.ca and realtor.ca) was that I should do a bunch of cleaning and validation as early as possible, which would allow all of my downstream data processing steps to remain clean. On the plus side I got to learn a bit about how to use fastapi and pydantic. On the much larger down side, this approach meant that if I wanted to modify any of the filtering I was applying, or if there were unanticipated parsing errors (people put the weirdest stuff in the square footage field) there was no possible recovery. In the final implementation I downloaded results in the most raw format I could manage. While the uncompressed data was a little larger than I wanted to be dealing with daily, it compressed down to very manageable sizes. Separating extraction from any sort of filtering or processing was definitely the right call."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#trying-to-learn-this-and-cloud-at-once",
    "href": "posts/2021-12-30-wheretolive.html#trying-to-learn-this-and-cloud-at-once",
    "title": "Building a where to live app",
    "section": "Trying to learn this and cloud at once",
    "text": "Trying to learn this and cloud at once\nSince one of the goals of this project was learning, I fairly early on got the idea in my head that I should try doing this whole process “cloud native” on the “modern data stack”. I’d read a fair bit about these technologies, but hadn’t had the opportunity to implement much in them. In theory, the cool thing about the cloud is that everything is pay as you go, so for a relatively small data project like I had in mind, the costs should have been manageable and the learning curve shouldn’t have been insurmountable. In practice this turned out to be incorrect. First, trying to learn how to solve a specific problem at the same time as learning to use a general technology really compounds the difficulty of both. I did manage to learn a lot about creating and deploying Azure Functions but due to some issue that I still don’t fully understand I also managed to rack up a sizable cloud bill. It had something to do with a queue function getting stuck and reprocessing a message repeatedly rather than failing. I learned a very hard lesson about setting up cost alerts thanks to this. In a future project I’d like to reimplement this or a similar project in the cloud, as it is still a skillset I’d like to develop, but I will definitely do as much locally as I can before migrating to the cloud, rather than trying to prototype something there directly, at least until I get more experience."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#setting-up-my-environment",
    "href": "posts/2021-12-30-wheretolive.html#setting-up-my-environment",
    "title": "Building a where to live app",
    "section": "Setting up my environment",
    "text": "Setting up my environment\nOne of the most important, but also annoying, aspects of any project is configuring and managing your environment. Most of my custom built logic was in python, so I built a poetry project. On top of python there was a lot of adjacent infrastructure to manage. For one thing, even though I wasn’t using the cloud, I still had information I wanted to leverage but keep private (namely addresses and API keys), as well as other services that I needed to have up and running. To coordinate all of this I used ansible. Specifically I kept my secrets using ansible-vault. From the vault I could either use a .env file to load data in with python-dotenv or use them directly in a playbook (for example, to set my database password). You can see the playbook I used here and there’s some related errata at the root of that repository."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#scraping-the-listings",
    "href": "posts/2021-12-30-wheretolive.html#scraping-the-listings",
    "title": "Building a where to live app",
    "section": "Scraping the listings",
    "text": "Scraping the listings\nThere are two listings sources I’m interested in. realtor.ca for sales listings and rentfaster.ca for rental listings. That’s not going to be 100% comprehensive but in my experience it will cover the vast majority of listings.\nThe pattern for the initial scrape of both was very similar. Both sites have an endpoint that you can query to get a result back in JSON. There were a few examples online on GitHub that I was able to base mine on. In each case the endpoint has a limit on the number of results that it will return at one time, so I needed to find a way to iterate through. In the case of rentfaster it was easy, since it returned search results with a page number associated. For a given query I could start at page 1 and increment my page number until I had an empty result set. After each query I dumped the JSON to a raw date stamped folder. For realtor.ca it was a little trickier, as there was no automatic chunking. It did allow a price range though, so I picked a very high price ceiling, and then incremented my price floor to be the highest price seen in the previous result until I got an empty result back.\nThe end result of each of these scrapes was a date stamped folder for each containing zipped JSON files of the raw results from the endpoint. You can find the scraping code for realtor.ca here and for rentfaster here."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#parsing-the-listings",
    "href": "posts/2021-12-30-wheretolive.html#parsing-the-listings",
    "title": "Building a where to live app",
    "section": "Parsing the listings",
    "text": "Parsing the listings\nAfter downloading the raw listings data, the next step was to process and format it into something I’d want to consume. This was pretty tedious, but it’s a critical part of any data project. Lots of validating and transforming of various fields. I won’t go into the details here, but the code for parsing realtor.ca is here and for rentfaster here. As the final stage of parsing any given day I would write a pandas DataFrame out to parquet in a folder along with the compressed raw files. This setup made it easy to read in cleaned up data, while still giving me the flexibility to go back and modify my data cleaning process as necessary on historical results."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#storing-all-the-data",
    "href": "posts/2021-12-30-wheretolive.html#storing-all-the-data",
    "title": "Building a where to live app",
    "section": "Storing all the data",
    "text": "Storing all the data\nI probably could have done basically everything I needed to do for this project in pandas, or at least geopandas, but it didn’t seem like the most elegant solution, and I wanted to learn some stuff. With those two criteria in mind I went with a PostgreSQL using PostGIS to handle the geospatial aspects of the data (location being very important in selecting where to live after all). I deployed the database itself in a docker container using ansible to manage the deployment. I also wrote a small wrapper script to make it easier to connect to the database from python using sqlalchemy. The wrapper code is here."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#ingesting-listings-data-in-postgis",
    "href": "posts/2021-12-30-wheretolive.html#ingesting-listings-data-in-postgis",
    "title": "Building a where to live app",
    "section": "Ingesting listings data in PostGIS",
    "text": "Ingesting listings data in PostGIS\nThe last thing that needed to happen with the listings themselves was getting them into the database. First I created a table for each of rentfaster and realtor.ca in the final format I wanted. Here’s the sql used to create the realtor.ca one for example. With that created I used pandas and sqlalchemy to push the cleaned data into a staging table (no need to predefine this since it’s getting wiped each time and pandas can handle table creation). Once the data was up in staging I would do a few additional calculations, like turning the latitude and longitude records into PostGIS Points before moving the data into the final table. I also would update a materialized view of listing data joined to some other data sets at this point, but I haven’t talked about the other data yet so I’ll cover that later. Here’s an example of the ingestion script."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#adding-in-commute-data",
    "href": "posts/2021-12-30-wheretolive.html#adding-in-commute-data",
    "title": "Building a where to live app",
    "section": "Adding in commute data",
    "text": "Adding in commute data\nOne of the most critical things in terms of choosing where to live is how easy it is to get places from it. This was one of the key pain points that made me think to develop this project in the first place. Plugging a candidate location into google maps and then interating through commute times to various important locations (downtown, work, family) is quite tedious. To make this easier I wanted to compute isochrones for various transit modes and locations. I initially looked at Azure maps for this. They have a built in method for isochrones, which I got working. Unfortunately it wasn’t very granular in terms of the isochrones it produced, and it didn’t support public transit data at all.\nFortunately, I learned about an amazing project called Open Trip Planner that was exactly what I needed. It was definitely more work to set up, but the results were way better than I could get through Azure. Open Trip Planner doesn’t include any maps or transit information out of the box, so I had to set that up. I used this script to grab a map of my region from OpenStreetMap, supplemented it with detailed transit commute information for my city with this script and finally even added in some elevation data so that walking and cycling commute times would be more accurate from This government of Canada page. I couldn’t automate that last part at all as I had to queue up for my data request and then retrieve it from a personalized email link. Oh well.\nOnce I had OpenTripPlanner up and running (again, in a docker container) I was able to use the API it provided to compute isochrones of various time ranges, transit modes, and locations using this script (it still has the Azure maps code in it even though I didn’t end up using that if you’re curious).\nThe output of that API was saved to JSON files, and then ingested into PostGIS using this script.\nFinally, I needed some way to associate this isochrone data with all the listings I was saving. I wanted columns that would easily let me filter on things like “Is this more than a 30 minute walk/transit trip from downtown?”. Between the different transit modes (walk, cycle, transit, drive, plus combinations), time ranges (I did 5 minute intervals between 10 and 60 minutes) and finally locations of interest I had a lot of possible ways to slice the data. While I could have hand written a giant SQL statement that would create them all, that would have been very boring to do, error prone, and also required significant rework if I changed any of my criteria. Instead I did some hacky string manipulation in python to construct the various components of my query and then stuck it together to create a view in PostGIS that associated each listing with all the transportation related attributes I might be interested in. Here’s what that looks like for realtor.ca."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#adding-in-grocery-store-data",
    "href": "posts/2021-12-30-wheretolive.html#adding-in-grocery-store-data",
    "title": "Building a where to live app",
    "section": "Adding in grocery store data",
    "text": "Adding in grocery store data\nWhile commute time to various places is certainly important for location, another factor is nearby amenities. Specifically I was asked if I could include the nearest grocery store. For this I used the FourSquare API. Similar to the initial scraping above, I had some issues with chunking here. The FourSquare API only returns a maximum of 50 results, and there are (a few) more than 50 grocery stores in all of Calgary. One thing the API lets you specify is a NE and SW corner to define a rectangle to search within. I took advantage of that and numpy’s linspace method to chunk the city into many boxes, query for grocery stores in each of them, and combine the result. The scraping code is here. The results are a little messy. There are several locations that FourSquare considers a grocery store that I would disagree with. It hasn’t been enough of an issue to bother with, but between when I save the raw FourSquare results and when I upload the data into PostGIS (here) I could easily (but tediously) add in a step that drops the locations that I don’t want to consider as grocery stores.\nOnce the grocery store data is in the database I create a table that has a row for each listing, its nearest grocery store, and the distance in meters to that grocery store. This is just straight line distance and doesn’t consider commute time, but it’s fast to compute, gives a good idea, and doesn’t make me run every listing and every grocery store through OpenTripPlanner daily. That seemed like a reasonable tradeoff to me."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#adding-flood-zone-data",
    "href": "posts/2021-12-30-wheretolive.html#adding-flood-zone-data",
    "title": "Building a where to live app",
    "section": "Adding flood zone data",
    "text": "Adding flood zone data\nAnother thing I want to consider when choosing where to live is climate resiliency. Calgary experienced a very significant flood less than a decade ago, and I would like to avoid living somewhere likely to be impacted by a similar event in the future. To manage this, I grabbed some flood risk data from the City of Calgary Open Data Portal and ingested it into PostGIS (here). From that I could create a table that checked if any given listing was in the 1 in 20 or 1 in 100 year flood zones as defined by the city (here)."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#combining-the-results",
    "href": "posts/2021-12-30-wheretolive.html#combining-the-results",
    "title": "Building a where to live app",
    "section": "Combining the results",
    "text": "Combining the results\nAt this stage in the write up I have a table with listings and their details, as well as some views that have a foreign key identifying the listing, along with some other specific attributes (closest grocery store, flood zone status, commute details). Creating those views actually takes an appreciable amount of time (not massive, but the commute one for example is a solid 10 seconds). What I want to build off the combination of all these tables is a filtered list of just the listings that match my criteria. Both because I want to be able to iterate on my criteria quickly, and because I’m building similar criteria list for a few other people who are interested in finding a place to live, I don’t want to have to recompute all those queries every time I want to change something or need to find candidates for a new person. To manage this, I created a materialized view of all the data sets joined together (here’s the realtor.ca table for example). After I ingest a new day of listings I can refresh this materialized view, and then have quick access to all my updated criteria for current listings."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#creating-candidate-lists",
    "href": "posts/2021-12-30-wheretolive.html#creating-candidate-lists",
    "title": "Building a where to live app",
    "section": "Creating candidate lists",
    "text": "Creating candidate lists\nThe next piece is filtering down all of the possible listings to just the ones that I might actually want. I did this by making views on top of the wide table described above that applied whatever filter criteria I wanted, along with only returning a subset of the available columns that I’d want to see in advance before investigating a listing further. Here’s the code for making candidate views for realtor.ca for example."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#sharing-the-candidates",
    "href": "posts/2021-12-30-wheretolive.html#sharing-the-candidates",
    "title": "Building a where to live app",
    "section": "Sharing the candidates",
    "text": "Sharing the candidates\nNow to make the candidate listings accessible. To make it easier for me, and possible for others, I export the listings daily to Dropbox. This part of the process was actually delightfully easy. I made some minor modifications to the example code on the Dropbox page and then used pandas to_html method to push up a table of listings. From there I could use regular Dropbox functionality to share personalized folders with people interested in particular listings candidates. If I was trying to do this as an actual application I’d obviously need a more robust solution, but for myself and a couple other people this worked perfect. The basic dropbox export code is here and the actual listings upload code is here."
  },
  {
    "objectID": "posts/2021-12-30-wheretolive.html#scheduling-things",
    "href": "posts/2021-12-30-wheretolive.html#scheduling-things",
    "title": "Building a where to live app",
    "section": "Scheduling things",
    "text": "Scheduling things\nNow that I have all the components of the pipeline set up I need to automate it. I was tempted to go with something cool for this like airflow or dagster but it didn’t seem worth the complexity. I ended up adding a task to my ansible playbook to schedule cron jobs for realtor.ca and rentfaster listings. The script cron runs looks like this."
  },
  {
    "objectID": "posts/2020-05-13-conda_envs.html",
    "href": "posts/2020-05-13-conda_envs.html",
    "title": "How to work with conda environments in shell scripts and Makefiles",
    "section": "",
    "text": "I’ve struggled with automating working with the conda python environment manager for a while. It’s a relatively small part of my work flow so I haven’t made figuring it out a top priority, but it’s really bugging me. In this post I’m going to document the problem and all the troubleshooting steps I went through to resolve it. I’m writing this post in parallel with actually resolving the issue, so it’s going to be a bit stream of consciousness."
  },
  {
    "objectID": "posts/2020-05-13-conda_envs.html#makefile",
    "href": "posts/2020-05-13-conda_envs.html#makefile",
    "title": "How to work with conda environments in shell scripts and Makefiles",
    "section": "Makefile",
    "text": "Makefile\n# Oneshell means I can run multiple lines in a recipe in the same shell, so I don't have to\n# chain commands together with semicolon\n.ONESHELL:\n# Need to specify bash in order for conda activate to work.\nSHELL=/bin/bash\n# Note that the extra activate is needed to ensure that the activate floats env to the front of PATH\nCONDA_ACTIVATE=source $$(conda info --base)/etc/profile.d/conda.sh ; conda activate ; conda activate\n\ntest:\n    $(CONDA_ACTIVATE) eg_env\n    echo $$(which python)\n\n# format the file with black\nlint:\n    $(CONDA_ACTIVATE) eg_env\n    black eg.py\n\n# Run the file directly\nrun_py:\n    $(CONDA_ACTIVATE) eg_env\n    python eg.py\n\n# Run the file from a shell script\nrun_sh:\n    $(CONDA_ACTIVATE) eg_env\n    bash eg.sh"
  },
  {
    "objectID": "posts/2020-05-13-conda_envs.html#env.yml",
    "href": "posts/2020-05-13-conda_envs.html#env.yml",
    "title": "How to work with conda environments in shell scripts and Makefiles",
    "section": "env.yml",
    "text": "env.yml\nname: eg_env\ndependencies:\n    - numpy\n    - black"
  },
  {
    "objectID": "posts/2020-05-13-conda_envs.html#eg.py",
    "href": "posts/2020-05-13-conda_envs.html#eg.py",
    "title": "How to work with conda environments in shell scripts and Makefiles",
    "section": "eg.py",
    "text": "eg.py\nimport numpy as np\n\nprint(np.__version__)"
  },
  {
    "objectID": "posts/2020-05-13-conda_envs.html#eg.sh-for-makefile",
    "href": "posts/2020-05-13-conda_envs.html#eg.sh-for-makefile",
    "title": "How to work with conda environments in shell scripts and Makefiles",
    "section": "eg.sh (for makefile)",
    "text": "eg.sh (for makefile)\n#!/bin/bash\n\necho \"This is my test bash script\"\necho \"Running python script\"\npython eg.py"
  },
  {
    "objectID": "posts/2020-05-13-conda_envs.html#eg.sh-standalone",
    "href": "posts/2020-05-13-conda_envs.html#eg.sh-standalone",
    "title": "How to work with conda environments in shell scripts and Makefiles",
    "section": "eg.sh (standalone)",
    "text": "eg.sh (standalone)\n#!/bin/bash\n\necho \"This is my test bash script\"\necho \"Activating conda environment\"\neval \"$($(which conda) 'shell.bash' 'hook')\"\nconda activate eg_env\necho \"Running python script\"\npython eg.py"
  },
  {
    "objectID": "posts/2022-08-01-dunning.html",
    "href": "posts/2022-08-01-dunning.html",
    "title": "Dunning-Kruger is autocorrelation",
    "section": "",
    "text": "A while back I read this interesting post that explained how the Dunning-Kruger effect is an example of autocorrelation. Before proceeding with the rest of this post, I’d highly recommend reading that one. I liked their example of using simulated data to show that the supposed effect can be observed even in completely uncorrelated variables. It got me wondering what it would look like if we had other relationships between predicted and actual performance, and so this post is an opportunity to explore that."
  },
  {
    "objectID": "posts/2022-08-01-dunning.html#setup-all-the-code-is-here",
    "href": "posts/2022-08-01-dunning.html#setup-all-the-code-is-here",
    "title": "Dunning-Kruger is autocorrelation",
    "section": "Setup (all the code is here)",
    "text": "Setup (all the code is here)\nIf you don’t want to read code and just want to see the charts feel free to skip to the next section.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.style.use(\"dark_background\")\n\n\nN_SAMPLES = 1_000\n\n\nclass DunningKruger:\n    def __init__(self, actual_dist: np.ndarray, predicted_dist: np.ndarray) -> None:\n        \"\"\"Build dataframe of actual percentile vs predicted.\"\"\"\n        self.df = (\n            pd.DataFrame({\"Actual Distribution\": actual_dist, \"Predicted Distribution\":predicted_dist})\n            # Handle prediction errors that produce impossible percentiles\n            .clip(lower=0.0, upper=100.0)\n            .assign(predict_error=lambda df: df[\"Predicted Distribution\"] - df[\"Actual Distribution\"])\n            .rename(columns={\"predict_error\": \"Prediction Error\"})\n        )\n    \n    @property\n    def x_vs_y_chart(self) -> sns.axisgrid.FacetGrid:\n        \"\"\"Original style chart of actual vs predicted.\"\"\"\n        g = sns.lmplot(x=\"Actual Distribution\", y=\"Predicted Distribution\", data=self.df, scatter_kws={\"alpha\": 0.25})\n        g.ax.axline((0,0), (100,100), linewidth=2, color=\"white\", linestyle=\"--\", alpha=0.5)\n        plt.show()\n\n    @property\n    def x_vs_error_chart(self) -> sns.axisgrid.FacetGrid:\n        \"\"\"Actual vs prediction error (implicit reference in DK paper).\"\"\"\n        g = sns.lmplot(x=\"Actual Distribution\", y=\"Prediction Error\", data=self.df, scatter_kws={\"alpha\": 0.25});\n        g.ax.axline((0,0), (100,0), linewidth=2, color=\"white\", linestyle=\"--\", alpha=0.5)\n        plt.show()"
  },
  {
    "objectID": "posts/2022-08-01-dunning.html#scenarios",
    "href": "posts/2022-08-01-dunning.html#scenarios",
    "title": "Dunning-Kruger is autocorrelation",
    "section": "Scenarios",
    "text": "Scenarios\nIn the following sections I’ll simulate some actual vs predicted distributions and see what they look like when you compare them to the Dunning Kruger graph. I’m not going to plot quantile vs percentile as in the original chart, as I think that just obscures what’s actually going on. The percentile vs percentile view shows the same overall pattern if you fit a regression line through it, which is easy enough to do.\n\nPurely random\nThis was the example in the original blog post that convinced me of the argument. I’ll start by reproducing it here. In this example predicted percentile performance is completely random and independent of actual percentile.\n\nactual_dist = np.random.uniform(low=0.0, high=100.0, size=N_SAMPLES)\npredicted_dist = np.random.uniform(low=0.0, high=100.0, size=N_SAMPLES)\n\n\ndk = DunningKruger(actual_dist, predicted_dist)\n\n\ndk.x_vs_y_chart\n\n\n\n\nThe basic format of this x_vs_y_chart will be the same in all scenarios, so I’ll just explain how to interpret it in this first example. The x axis represents someone’s actual place in the percentile distribution, the y axis is their predicted place in that same distribution. Each point in the scatter plot represents a single observation of someone’s actual place in the distribution and their corresponding prediction of where they are in the distribution. The white dotted line moving up at a 45 degree angle represents perfect prediction, where all observations would be if everyone could exactly predict their place in the distribution. The blue-green line is a regression line of the relationship between someone’s actual place in the distribution and their prediction. The fact that it’s pretty much flat suggests no relationship, which is what we’d expect since that’s how the data was generated\n\ndk.x_vs_error_chart;\n\n\n\n\nAs with the previous chart, the basic format of this x_vs_error_chart will be the same in all scenarios, so I’ll just explain how to interpret it in this first example. The x axis again represents someone’s actual place in the percentile distribution, but now the y axis is the difference between their prediction and actual place. Each point in the scatter plot represents a single observation of someone’s actual place in the distribution and their corresponding prediction error. The horizontal white dotted line moving up at a zero again represents perfect prediction, where all observations would be if everyone could exactly predict their place in the distribution. The blue-green line is a regression line of the relationship between someone’s actual place in the distribution and their prediction error. Note that in this randomly generated scenario we’re seeing something that looks like Dunning Kruger, with people on the low endof the distribution tending to overestimate their place, and people on the high end tending to underestimate.\n\n\nUnbiased prediction error\nIn this scenario everyone is equally good at predicting their percentile placement with only a small, unbiased error.\n\nactual_dist = np.random.uniform(low=0.0, high=100.0, size=N_SAMPLES)\nerror = np.random.normal(loc=0.0, scale=10.0, size=N_SAMPLES)\npredicted_dist = actual_dist + error\n\n\ndk = DunningKruger(actual_dist, predicted_dist)\n\n\ndk.x_vs_y_chart;\n\n\n\n\n\ndk.x_vs_error_chart;\n\n\n\n\nEven here we can see a slight appearance of “overconfidence” for lower percentiles and “underconfidence” for higher percentiles simply because prediction errors are truncated at the top and bottom (If I’m actually in the lowest percentile the only errors I can possibly make are to overestimate my performance).\n\n\nActual Dunning Kruger\nLet’s see what this would actually look like if we simply had overconfidence in the bottom half of the distribution. I could add in underconfidence among the top half as well, but the message of the original paper focused on the overconfidence so let’s do the same here.\n\nactual_dist = np.random.uniform(low=0.0, high=100.0, size=N_SAMPLES)\nbase_error = np.random.normal(loc=0.0, scale=10.0, size=N_SAMPLES)\nbelow_median_indicator = actual_dist < 50.0\n# The further below the median you are the greater you might overestimate your position\nerror_scaler = (50.0 - actual_dist) * below_median_indicator\n# Scale up how overconfident this makes you\noverconfidence_error = error_scaler * np.random.uniform(low=0.0, high=2.0, size=N_SAMPLES)\npredicted_dist = actual_dist + error + overconfidence_error\n\n\ndk = DunningKruger(actual_dist, predicted_dist)\n\n\ndk.x_vs_y_chart;\n\n\n\n\n\ndk.x_vs_error_chart;"
  },
  {
    "objectID": "posts/2022-08-01-dunning.html#conclusion",
    "href": "posts/2022-08-01-dunning.html#conclusion",
    "title": "Dunning-Kruger is autocorrelation",
    "section": "Conclusion",
    "text": "Conclusion\nHonestly I was pretty convinced by the argument in the original post, so nothing earth shattering came out of this exercise for me. But it was interesting to see what the charts looked like under different situations. And it gave me an excuse to write some code and build some charts, which I haven’t had in a while.\nIf you’d like to try out other scenarios, you can click the binder link near the top of this post to open it in an interactive notebook. If you want to try other distributions for your actual/predicted/error distributions, check out the numpy docs on random sampling."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html",
    "href": "posts/2020-10-14-arch-bootstrap.html",
    "title": "Automating provisioning Arch - OS installation",
    "section": "",
    "text": "This is part 1 of a 4 part series describing how I provision my systems. Links to each part are below:\n\npart 1 - The base OS install\npart 2 - Software install and system configuration with Ansible\npart 3 - User level and python environment config with dotfiles and mkrc\npart 4 - The tldr that wraps up how to do the whole thing from start to finish]"
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#edit-2020-10-20",
    "href": "posts/2020-10-14-arch-bootstrap.html#edit-2020-10-20",
    "title": "Automating provisioning Arch - OS installation",
    "section": "edit 2020-10-20",
    "text": "edit 2020-10-20\nAs I use this script to provision machines I’m going to end up making edits to it. I’m not going to edit this post every time I do that. The latest version of the provision script is always available here.\nI’ve also updated the TLDR section a bit based on some experience from the install. That part I will update if I make changes since I use it for reference when building systems."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#actual-introduction",
    "href": "posts/2020-10-14-arch-bootstrap.html#actual-introduction",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Actual Introduction",
    "text": "Actual Introduction\nI’ve installed a lot of operating systems a lot of times. The goal of writing out this post is to force me to really think about and clearly document a reproducible workflow for building my workstation.\nA secondary goal is to get better at bash."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#setting-up-wifi",
    "href": "posts/2020-10-14-arch-bootstrap.html#setting-up-wifi",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Setting up WiFi",
    "text": "Setting up WiFi\nIn the case where I’m doing this on a laptop I’ll likely have to get on WiFi before I can continue.\nsystemctl start iwd.service\niwctl\ndevice list #magically changed my device name to wlan0 here somehow\nstation wlan0 connect <your SSID>  # You can enclose it in quotes if it has spaces\n<enter passphrase>\nexit\ndhcpcd wlan0\nThat should work, try pinging something just to be safe."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#make-sure-partitions-are-set-up",
    "href": "posts/2020-10-14-arch-bootstrap.html#make-sure-partitions-are-set-up",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Make sure partitions are set up",
    "text": "Make sure partitions are set up\nI’m a wuss and don’t trust a script to actually create partitions. lsblk will tell you what disks you have. If you need to create/delete partitions before proceeding use cfdisk /dev/sd<letter> to create them. If it’s a completely blank hard drive and you need to create a boot partition make one at the beginning of the disk with 500M of space in cfdisk and then run mkfs.vfat -F32 /dev/sd<letter>1 to format it. There’s probably a cleaner way to clean out the LVMs this script creates but for now it’s easier for me to just blow them away in cfdisk and create a fresh partition to install over. edit: I got braver. The new script has an option to just wipe the whole disk if you want."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#run-the-script",
    "href": "posts/2020-10-14-arch-bootstrap.html#run-the-script",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Run the script",
    "text": "Run the script\nbash <(curl -fsSL http://bootstrap.ianpreston.ca)"
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#post-install",
    "href": "posts/2020-10-14-arch-bootstrap.html#post-install",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Post install",
    "text": "Post install\n\nGet the Wifi going again. It’s a different command than you use from the installer:\n\nnmcli device wifi connect <SSID> password <password>\n\nSet up ssh keys - plug in the USB\n\nlsblk  # find where the partition with the keys is stored\nmkdir ssh  # make a mount point\nsudo mount /dev/sd<something> ssh\ncp -R ssh ssh_local  # Have to set permissions on keys (stupid NTFS)\ncd ssh_local/CA\nchmod 600 host_ca\nchmod 600 user_ca\ncd ../\nchmod +x setup_host.sh\nchmod +x setup_user.sh\nsudo ./setup_host.sh\n./setup_user.sh\nAfter this point you should be able to run ansible to complete the setup."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#prepping-the-vm",
    "href": "posts/2020-10-14-arch-bootstrap.html#prepping-the-vm",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Prepping the VM",
    "text": "Prepping the VM\nFirst thing to do for any install is download the ISO. I’m going to use Virtualbox as my hypervisor. No particular reason, I’ve just used it in the past and am comfortable with it.\nThen I create a base image to work off of.\n\n\n\nbase_vm\n\n\n\n\n\nvm_disk\n\n\nFire up the new VM, and select the Arch ISO at the prompt:\n\n\n\nvm_iso\n\n\nAfter that we’re at the boot prompt. Now comes the fun task of developing a bootstrap script that will automate the install process.\n\n\n\nvm_prompt"
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#getting-the-bootstrap-script-to-the-machine",
    "href": "posts/2020-10-14-arch-bootstrap.html#getting-the-bootstrap-script-to-the-machine",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Getting the bootstrap script to the machine",
    "text": "Getting the bootstrap script to the machine\nI’ve created a repository on GitHub to host code like this. I think I could probably just install git on the boot machine, clone the whole repo, navigate to the bootstrap script, and run it. That’s no fun though. Let’s see if I can find a more complicated approach just to save a few keystrokes.\nThe full URL to the raw script is at this page. That’s pointing to the branch I’m using while I develop the script. When I’ve got it working I’ll hopefully remember to come back here and point it to the master reference. From there I can go to my domain registrar and add a URL redirect record to point an easy to remember subdomain to that path:\n\n\n\nDNS\n\n\nSo now bootstrap.ianpreston.ca redirects directly to my shell script.\nTo actually get the script onto my machine and run it I’ll use curl. The exact syntax will be\nbash <(curl -fsSL http://bootstrap.ianpreston.ca)\n-f Specifies that the script should fail silently. Otherwise if there’s an http error it will return a 404 page, which I’d then try and run. I’d rather it just not return anything and fail that way.\n-L Specifies that if the server reports that the page has moved then curl will redo the request.\n-sS Means the script should run silently, unless there’s a failure, in which case it will show the output. -S means show error and -s means silent.\nAs I’m writing this the shell script doesn’t really do anything, it just prints something out so I know it worked. Here’s where it’s at at this point:\n#!/usr/bin/env bash\n\n# To install: bash <(curl -fsSL http://bootstrap.ianpreston.ca)\n\necho \"We got this far!\"\nAs a quick aside, I don’t write bash scripts often, so I often forget how exactly to set up the shebang. For bash scripts I’ve seen #!/bin/bash and #!/usr/bin/env bash. It seems like in most circumstances they’re interchangeable. This StackOverflow post suggests that the latter is slightly more flexible/portable so I’m going to try and make a habit of using it in my scripts going forward.\nBack at the VM I test my overly elaborate bootstrapping setup and…\n\n\n\ncurl\n\n\nSweet!"
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#strict-mode",
    "href": "posts/2020-10-14-arch-bootstrap.html#strict-mode",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Strict mode",
    "text": "Strict mode\nThe first couple lines of Brennan’s script include a bunch of things I don’t really understand. Since part of the goal of this is learning more bash I’m going to dissect them before moving on. The lines in question are:\nSOURCED=false && [ \"${0}\" = \"${BASH_SOURCE[0]}\" ] || SOURCED=true\nif ! $SOURCED; then\n  set -eEu\n  shopt -s extdebug\n  trap 's=$?; echo \"$0: Error on line \"$LINENO\": $BASH_COMMAND\"; exit $s' ERR\n  IFS=$'\\n\\t'\nfi\nLet’s break this up into tiny chunks. SOURCED=false is used to set a shell variable to false. Next up && is a list operator which will only run the next command if the previous one succeeded. So if we set SOURCED to false successfully then the next command will be executed.\nPutting something inside square brackets means to evaluate the expression inside and return success or failure based on that. You can use it for if statements, or use it to only execute a subsequent command based on the result of the conditional. Luke Smith has a good video explaining how to avoid if statements by writing code like the line we’re evaluating.\nSo what are we actually evaluating in the square brackets? This StackOverflow post explains the difference between ${0} and ${BASH_SOURCE[0]}. I’ll outline the difference below with an example script called a few different ways:\nI’ve got a script called experiment.sh which I’ll be running to check out these smaller components of the script\n#!/usr/bin/env bash\necho \"0: [$0] vs bash_source: [${BASH_SOURCE[0]}]\"\nHere’s the output of running that script a few different ways:\nroot@archiso ~ # bash ./experiment.sh\n0: [./experiment.sh] vs bash_source: [./experiment.sh]\nroot@archiso ~ # ./experiment.sh\n0: [./experiment.sh] vs bash_source: [./experiment.sh]\nroot@archiso ~ # . ./experiment.sh\n0: [./experiment.sh] vs bash_source: []\nroot@archiso ~ # source ./experiment.sh\n0: [./experiment.sh] vs bash_source: []\nIf I execute the script in a subshell, as with the first two examples, then they are equivalent and the expression will evaluate to true. If I source the script - that is I tell it to execute the commands in my current shell, then they will not be equivalent.\nBack to the script the shell variable name makes sense now and I can put this all together. Set the shell variable SOURCED to false, check whether the script is being sourced or not, and if it is, update the SOURCED shell variable to true (Since the || operator says to only execute the subsequent command if the previous did not return true).\nInside this block we have some commands that, as described above, will only run if the script is being run from its own subshell. The first command, set -eEu uses the set builtin to change the value of some shell options. -e forces the script to exit if almost any command in the script fails (the link provided above for set includes more details). -E let’s errors that occur within functions be trapped by the shells they inherit from. That’s confusing because it’s how I’d normally expect error handling to work, not a special case. This post explains what’s going on. Finally, -u treats unset variables and parameters as an error. Again, this is what I’d expect a sane language to do by default. I think the normal behavior is to just return an empty string in bash though. Gross.\nNext up is shopt -s extdebug. The shopt builtin lets us set additional shell options. The details of extdebug are in the previous link, but basically it allows better error tracing within function calls.\nThe next line is a bit of error handling as well. trap catches certain signals and runs a command in response. The basic syntax is trap <command to run when caught> <signals to catch>. Looking at trap 's=$?; echo \"$0: Error on line \"$LINENO\": $BASH_COMMAND\" exit $s' ERR that means if/when an ERR signal occurs in the script we’ll set the shell variable s to the exit status of the last task (that’s what $? is), print the filename of the shell script ($0), the line number of the error and it’s command, along with its exit status. Because of the options set above there’s no need to explicitly tell the trap to exit.\nThe final line in the block changes the internal field separator from the default of <space><tab><newline> to <newline><tab>. The link above explains in general what that’s for. We’ll have to get a bit farther along in the script to see why it’s being used here."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#text-formatting",
    "href": "posts/2020-10-14-arch-bootstrap.html#text-formatting",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Text formatting",
    "text": "Text formatting\nThat part was dense. The next few lines are easier:\n# Text modifiers\nBold=\"\\033[1m\"\nReset=\"\\033[0m\"\n\n# Colors\nRed=\"\\033[31m\"\nGreen=\"\\033[32m\"\nYellow=\"\\033[33m\"\nText enclosed within $Bold and $Reset will be bolded. Similarly, enclosing within one of the colours and $Reset will set the text to that colour."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#setup-paths",
    "href": "posts/2020-10-14-arch-bootstrap.html#setup-paths",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Setup paths",
    "text": "Setup paths\nThe next section sets up a log file:\nWORKING_DIR=$(pwd)\nLOG=\"${WORKING_DIR}/arch-install.log\"\n[[ -f ${LOG} ]] && rm -f \"${LOG}\"\necho \"Start log...\" >>\"${LOG}\"\npwd stands for “print working directory”. When you enclose a command in $() it means to take the result of the command. To quickly illustrate, EXAMPLE=pwd would set EXAMPLE to “pwd”, whereas EXAMPLE=$(pwd) would set EXAMPLE to something like /root. The first two lines therefor set the LOG variable to point to a file in the current directory named arch-install.log.\nThe next line checks if the logfile exists, and deletes it if it does.\nThe final line writes “Start log…” into the logfile. >> redirects the output of the previous command to the end of the file on the right hand side. Since we know this is a brand new file (because of the line above) this will be the first line of the logfile."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#flags-and-variables",
    "href": "posts/2020-10-14-arch-bootstrap.html#flags-and-variables",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Flags and variables",
    "text": "Flags and variables\nThe next section sets up some system based flags\nSYS_ARCH=$(uname -m) # Architecture (x86_64)\nUEFI=0\nKEYMAP=\"us\"\nWIFI=0\nuname returns system information, and the -m flag specifies to return the machine hardware. As the comment above describes this will likely return x86_64. Later in the script we’ll check if the system is UEFI or BIOS."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#user-provided-variables",
    "href": "posts/2020-10-14-arch-bootstrap.html#user-provided-variables",
    "title": "Automating provisioning Arch - OS installation",
    "section": "User provided variables",
    "text": "User provided variables\nHere we just provide some defaults to variables that the user will set later in the script.\n# User provided variables\nHOST_NAME=\"computer\"\nKERNEL_VERSION=\"default\"\nMAIN_DISK=\"/dev/sda\"\nROOT_PWD=\"\"\nANSIBLE_PWD=\"\""
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#common-helper-functions",
    "href": "posts/2020-10-14-arch-bootstrap.html#common-helper-functions",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Common helper functions",
    "text": "Common helper functions\nThe next section has a series of small functions that will be used throughout the larger script. Let’s see what they do. The first one is:\nprint_line() {\n  printf \"%$(tput cols)s\\n\" | tr ' ' '-' |& tee -a \"${LOG}\"\n}\nThe function name gives a pretty solid hint what it does. printf Allows you to print a combination of strings and variables along with specified formatting for the variables. There’s some good docs here. tput provides information about the current terminal, in this case cols says the number of columns that make up a row in the terminal. In my VM tput cols returns 100 so the printf would resolve to printf \"%100s\\n\" which means we’ll print spaces across the width of the terminal. |& means to take the output (both from standard error and standard output, as opposed to just | which only return standard output) of the previous command and pass it as an input to the following command. tr in turn replaces the first string with the second, so we turn spaces into dashes, creating a line of dashes across the screen. Finally, that line of dashes is piped to tee -a which sends its input both to standard output and a file. The -a flag means to append the output to the file rather than overwriting it. All of that to say this function prints a line of dashes across your screen and into the logfile we defined above.\nNext up we have blank_line, which based on the explanation above is pretty self explanatory.\nblank_line() {\n  echo -e \"\\n\" |& tee -a \"${LOG}\"\n}\nNext up is print_title\nprint_title() {\n  clear\n  print_line\n  echo -e \"# ${Bold}$1${Reset}\" |& tee -a \"${LOG}\"\n  print_line\n  echo \"\" |& tee -a \"${LOG}\"\n}\nclear clears the screen. Everything else has been explained except $1 which is just the first argument passed to the function. This means calling print_title \"This is the title\" would clear the screen, print a line of dashes to the screen and log file, print This is the title to the screen and log file, another line, and then start at the beginning of a new line for whatever text follows.\nAfter that is print_title_info\nprint_title_info() {\n  T_COLS=$(tput cols)\n  echo -e \"${Bold}$1${Reset}\\n\" | fold -sw $((T_COLS - 18)) | sed 's/^/\\t/' |& tee -a \"${LOG}\"\n}\necho just prints some text, the -e flag tells it to interpret escaped characters, so \\t will show a tab rather than the literal \\t. That gets piped to fold, which wraps the text at 18 characters less than the width of the terminal. -s tells it to wrap at the last whitespace before the column limit (so don’t wrap in the middle of a word) and w is how you specify the column width to wrap on. After that we pipe the output to sed which I frankly find intimidating. This one’s not so bad though. 's/<pattern>/<other pattern>/' just performs a find replace of <pattern> for <other pattern> in each line of text that’s passed in. The patterns can be regular expressions, which I also find intimidating to work with, but this one is just saying to replace the beginning of the line (that’s what ^) means with a tab. Not so terrible.\nThe next several functions repeat the general concepts above, just with different formatting (red font for errors for example) so I won’t reproduce them here.\nNext up is pause_function\npause_function() {\n  print_line\n  read -re -sn 1 -p \"Press enter to continue...\"\n}\nIn the original code the read line was in an if block that was based on a variable that wasn’t set anywhere in the script. I assume that was planned to build fully unattended builds eventually, but I took it out for now at least. read receives input from the user. -re specifies not to allow backslashes to escape characters and to use Readline to obtain the line in an interactive shell. -s tells read not to echo input to the terminal, n 1 tells it how many characters of input to wait for. -p tells it what text to prompt with.\nNext up is arch-chroot\narch_chroot() {\n  arch-chroot /mnt /bin/bash -c \"${1}\" |& tee -a \"${LOG}\"\n}\nThis is cool, when I’ve tried to build my own version of this in the past I broke things up into a pre and post chroot because I couldn’t figure out how to get my script to change contexts. This one does it by just sending the commands one at a time into the chrooted environment. Neat!\nThe final helper is is_package_installed\nis_package_installed() {\n  #check if a package is already installed\n  for PKG in $1; do\n    pacman -Q \"$PKG\" &>/dev/null && return 0\n  done\n  return 1\n}\npacman -Q searches for a package matching the subsequent argument. If it finds it it will return its full name and version. If it can’t it will return an error. So this function will return 0 if any packages are found, and 1 if none of them are."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#verification-functions",
    "href": "posts/2020-10-14-arch-bootstrap.html#verification-functions",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Verification functions",
    "text": "Verification functions\nThese are also helper functions, but they’re specifically designed to make sure the script is being run from the correct environment.\nFirst up is check_root\ncheck_root() {\n  print_info \"Checking root permissions...\"\n\n  if [[ \"$(id -u)\" != \"0\" ]]; then\n    error_msg \"ERROR! You must execute the script as the 'root' user.\"\n  fi\n}\nid -u returns the user id. Since Root is always user 0 on a system we know this isn’t being run as root and the script will fail.\nNext up is check_archlinux\ncheck_archlinux() {\n  if [[ ! -e /etc/arch-release ]]; then\n    error_msg \"ERROR! You must execute the script on Arch Linux.\"\n  fi\n}\n-e <file> checks if a file exists, so if /etc/arch-release does not exist (it’s an empty file on the USB boot system) then we’re not on Arch and had better exit.\nNext up is check_boot_system\ncheck_boot_system() {\n  if [[ \"$(cat /sys/class/dmi/id/sys_vendor)\" == 'Apple Inc.' ]] || [[ \"$(cat /sys/class/dmi/id/sys_vendor)\" == 'Apple Computer, Inc.' ]]; then\n    modprobe -r -q efivars || true # if MAC\n  else\n    modprobe -q efivarfs # all others\n  fi\n\n  if [[ -d \"/sys/firmware/efi/\" ]]; then\n    # Mount efivarfs if it is not already mounted\n    # shellcheck disable=SC2143\n    if [[ -z $(mount | grep /sys/firmware/efi/efivars) ]]; then\n      mount -t efivarfs efivarfs /sys/firmware/efi/efivars\n    fi\n    UEFI=1\n  else\n    UEFI=0\n  fi\n}\nThe purpose of this section is to verify the boot mode. Pretty much any system I can imagine installing on these days will be UEFI, but it doesn’t hurt to check. I’m not totally sure what the first little bit is doing, and I don’t have a mac to test. The -r flag is to remove a module from the kernel, rather than add it like the normal command. modprobe -q efivarfs will add the efivarfs module to the kernel, and fail quietly if it can’t find that module (that’s the -q flag). As described in the install guide, if you have a /sys/firmware/efi/ directory, which is what the first block of the second if statement is checking, then your system is EFI. The next part describes what it’s going to do (mount efivarfs if it’s not already mounted), but let’s dig into how it does that. -z returns true if the length of an evaluated string is zero. mount without any arguments returns all mountpoints in the system. We pipe that into grep which will return /sys/firmwar/efi/efivars if it’s mounted and an empty string if not, which accomplishes the goal. The last part of the script sets the variable UEFI to identify if the system is EFI or BIOS.\nNext up is check_wifi\ncheck_wifi() {\n  has_wifi=($(ls /sys/class/net | grep wlan))\n  if [ -n \"$has_wifi\" ]; then\n    WIFI=1\n  fi\n}\nAs per the Arch Wikie /sys/class/net lists all network devices. So if I list that directory and match on wlan then I know there’s a wireless device. I’ll use this to determine whether or not to install wireless tools when loading software."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#prompts-user-interaction",
    "href": "posts/2020-10-14-arch-bootstrap.html#prompts-user-interaction",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Prompts / User interaction",
    "text": "Prompts / User interaction\nThe first prompt asks for a hostname for the system. It had some code for auto naming that I trimmed out. The rest of it is pretty self explanatory:\nask_for_hostname() {\n  print_title \"Hostname\"\n  print_title_info \"Pick a hostname for this machine.  Press enter to have a random hostname selected.\"\n  read -rp \"Hostname [ex: archlinux]: \" HOST_NAME\n  if [[ $HOST_NAME == \"\" ]]; then\n    HOST_NAME=\"arch-$((1 + RANDOM % 1000)).tts.lan\"\n  fi\n}\nThe read command takes inputs, the -r flag prevents special characters from being included, and -p displays the text that follows as a prompt without a newline before taking the input.\nThe last block says if the input is empty to give a hostname like arch-[random number 1-1000].tts.lan.\nNext up we’re determine which hard disk to install on. Note that this isn’t handling partitioning or anything yet.\nask_for_main_disk() {\n  print_info \"Determining main disk...\"\n  devices_list=($(lsblk --nodeps --noheading --list --exclude 1,11,7 | awk '{print \"/dev/\" $1}'))\n\n  if [[ ${#devices_list[@]} == 1 ]]; then\n    device=${devices_list[0]}\n  else\n    print_title \"Main Disk Selection\"\n    print_title_info \"Select which disk to use for the main installation (where root and boot will go).\"\n    lsblk --nodeps --list --exclude 1,11,7 --output \"name,size,type\"\n    blank_line\n    PS3=\"Enter your option: \"\n    echo -e \"Select main drive:\\n\"\n    select device in \"${devices_list[@]}\"; do\n      if contains_element \"${device}\" \"${devices_list[@]}\"; then\n        break\n      else\n        invalid_option\n      fi\n    done\n  fi\n  MAIN_DISK=$device\n}\nThe first line calls lsblk to list all available block devices. --nodeps tells it not to show holder devices, so for example if I have an sda device with two partitions - sda1 and sda2 it will only show sda, which is what we want since we’re just picking the disk itself at this stage. --noheading drops column headers, which we want since we’re just going to parse this list. --list produces the output as a list, which we want in order to make a list of potential disks to select. --exclude 1,11,17 tells it not to list RAM or optical drive devices. The output of that list is piped into awk so that /dev/ can be prepended to it.\nThe next line says that if the array has only one entry (there’s only one disk available) then we just use that device. If we have more than one device the script prints out a list of them using the same command used to build the list of available disks but showing column headers and including some details to help select the correct disk.\nPS3 sets the prompt used by the select command. The select statement has you pick a device and loops if you haven’t selected one of the options in the list until you do. Finally we set MAIN_DISK to the device we want to install on.\nThe original script has some similar functions to pick a second disk, but I’m not going to be using that option so I’m omitting it.\nThere are a few other selection scripts (to set a root password and kernel version for example), but there’s nothing new in terms of BASH in them so I’ll omit them from this post."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#installationconfiguration-options",
    "href": "posts/2020-10-14-arch-bootstrap.html#installationconfiguration-options",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Installation/configuration options",
    "text": "Installation/configuration options\nNow we get to functions that actually help with the installation and configuration of the system.\nFirst is configure_mirrorlist\nconfigure_mirrorlist() {\n  print_info \"Configuring repository mirrorlist\"\n\n  pacman -Syy |& tee -a \"${LOG}\"\n\n  # Install reflector\n  pacman -S --noconfirm reflector |& tee -a \"${LOG}\"\n\n  print_status \"    Backing up the original mirrorlist...\"\n  rm -f \"/etc/pacman.d/mirrorlist.orig\" |& tee -a \"${LOG}\"\n  mv -i \"/etc/pacman.d/mirrorlist\" \"/etc/pacman.d/mirrorlist.orig\" |& tee -a \"${LOG}\"\n\n  print_status \"    Rotating the new list into place...\"\n  # Run reflector\n  /usr/bin/reflector --score 100 --fastest 20 --age 12 --sort rate --protocol https --save /etc/pacman.d/mirrorlist |& tee -a \"${LOG}\"\n\n  # Allow global read access (required for non-root yaourt execution)\n  chmod +r /etc/pacman.d/mirrorlist |& tee -a \"${LOG}\"\n\n  # Update one more time\n  pacman -Syy |& tee -a \"${LOG}\"\n}\npacman -Syy says to sync all available packages from the master repository. The -S flag is for sync, and yy forces a refresh even if the list appears to be up to date.\nNext the script installs reflector in order to update and optimize the list of mirrors that will be used for downloading packages.\nThe rest of the script is pretty well commented and straightforward.\n\nPartitioning\nThe script I’m templating off of is designed to wipe an entire disk. I generally dual boot Windows so I definitely don’t want that option. In light of that I had to tweak this section a fair bit. Rather than wiping the whole disk and creating new partitions like the template script, I want to identify an existing boot and linux partition and install to them. See the TLDR section for setting up a fresh disk. If you’ve already installed Arch on the disk you’re targeting you should also clear out the partition with the LVMs on it and start with a blank slate. That’s not done by the script, check the TLDR section for how to manage that.\nfind_install_partition() {\n  print_title \"Installation partition selection\"\n  print_title_info \"Select the partition to install Arch. This should be an already existing boot partition. If you don't see what you expect here STOP and run cfdisk or something to figure it out.\"\n  partition_list=($(lsblk $MAIN_DISK --noheading --list --output NAME | awk '{print \"/dev/\" $1}' | grep \"[0-9]$\"))\n  blank_line\n  PS3=\"Enter your option\":\n  lsblk $MAIN_DISK --output NAME,FSTYPE,LABEL,SIZE\n  echo -e \"select a partition\"\n  select partition in \"${partition_list[@]}\"; do\n    if contains_element \"$partition\" \"${partition_list[@]}\"; then\n      break\n    else\n      invalid_option\n    fi\n  done\n  INSTALL_PARTITION=$partition\n}\nThis works similarly to the main disk selection, except I formatted the output slightly differently. The one completely new command I added was the last | grep \"[0-9]$\". That filters the output to only show entries that end with a number ($ means end of line). I couldn’t figure out a way to have lsblk not list the block device (the opposite of what we wanted in the main disk selection) so I filter it out. As an example if I have a disk /dev/sda with partitions sda1, sda2, sda3 before the grep I’d get:\n/dev/sda\n/dev/sda1\n/dev/sda2\n/dev/sda3\nI really don’t want to accidentally try and make a partition on the whole device, so I filter that out so the list is just:\n/dev/sda1\n/dev/sda2\n/dev/sda3\nThere’s a practically identical function called find_boot_partition that does the same thing but identifies the boot partition for installation.\nThe next step is to create a physical and logical volume for the operating system using LVM\nsetup_lvm() {\n  print_info \"Setting up LVM\"\n\n  pvcreate $INSTALL_PARTITION\n  vgcreate \"vg_main\" $INSTALL_PARTITION\n\n  lvcreate -l 5%VG \"vg_main\" -n lv_var\n  lvcreate -l 45%VG \"vg_main\" -n lv_root\n  lvcreate -l 40%VG \"vg_main\" -n lv_home\n}\nThis one’s actually pretty readable. We create a physical volume on the install partition identified in the previous section, create a virtual group on it, and then create logical volumes within that.\nNext we format and mount the partitions:\nformat_partitions() {\n  print_info \"Formatting partitions\"\n\n  mkfs.ext4 \"/dev/mapper/vg_main-lv_var\"\n  mkfs.ext4 \"/dev/mapper/vg_main-lv_root\"\n  mkfs.ext4 \"/dev/mapper/vg_main-lv_home\"\n}\n\nmount_partitions() {\n  print_info \"Mounting partitions\"\n\n  # First load the root\n  mount -t ext4 -o defaults,rw,relatime,errors=remount-ro /dev/mapper/vg_main-lv_root /mnt\n\n  # Create the paths for the other mounts\n  mkdir -p \"/mnt/boot/efi\"\n  mkdir -p \"/mnt/var\"\n  mkdir -p \"/mnt/home\"\n\n  if [[ $UEFI == 1 ]]; then\n    mount -t vfat -o defaults,rw,noatime,utf8,errors=remount-ro \"${MAIN_DISK}1\" \"/mnt/boot/efi\"\n  fi\n\n  # Mount others\n  mount -t ext4 -o defaults,rw,noatime /dev/mapper/vg_main-lv_var /mnt/var\n  mount -t ext4 -o defaults,rw,noatime /dev/mapper/vg_main-lv_home /mnt/home\n}\nAgain, most of this is pretty readable. The one that I didn’t know about was the noatime option. In the original script it was set to relatime, but after reading this post it seems like I want noatime to improve the life of my SSD."
  },
  {
    "objectID": "posts/2020-10-14-arch-bootstrap.html#installation",
    "href": "posts/2020-10-14-arch-bootstrap.html#installation",
    "title": "Automating provisioning Arch - OS installation",
    "section": "Installation",
    "text": "Installation\nThe install_base_system function doesn’t really introduce any new bash stuff, which was my main goal in writing out how this all worked line by line. I’ll present it below without further comment.\ninstall_base_system() {\n  print_info \"Installing base system\"\n\n  pacman -S --noconfirm archlinux-keyring |& tee -a \"${LOG}\"\n\n  # Install kernel\n  case \"$KERNEL_VERSION\" in\n  \"lts\")\n    pacstrap /mnt base base-devel linux-lts linux-lts-headers linux-firmware |& tee -a \"${LOG}\"\n    [[ $? -ne 0 ]] && error_msg \"Installing base system to /mnt failed. Check error messages above.\"\n    ;;\n  \"hard\")\n    pacstrap /mnt base base-devel linux-hardened linux-hardened-headers linux-firmware |& tee -a \"${LOG}\"\n    [[ $? -ne 0 ]] && error_msg \"Installing base system to /mnt failed. Check error messages above.\"\n    ;;\n  *)\n    pacstrap /mnt base base-devel linux linux-headers linux-firmware |& tee -a \"${LOG}\"\n    [[ $? -ne 0 ]] && error_msg \"Installing base system to /mnt failed. Check error messages above.\"\n    ;;\n  esac\n\n  # Install file system tools\n  pacstrap /mnt lvm2 dosfstools mtools gptfdisk |& tee -a \"${LOG}\"\n  [[ $? -ne 0 ]] && error_msg \"Installing base system to /mnt failed. Check error messages above. Part 4.\"\n\n  # Install networking tools\n  pacstrap /mnt dialog networkmanager networkmanager-openvpn |& tee -a \"${LOG}\"\n  [[ $? -ne 0 ]] && error_msg \"Installing base system to /mnt failed. Check error messages above. Part 5.\"\n\n  if [[ $WIFI == 1 ]]; then\n    pacstrap /mnt iwd |& tee -a \"${LOG}\"\n    [[ $? -ne 0 ]] && error_msg \"Installing base system to /mnt failed. Check error messages above. Wifi\"\n  fi\n\n  # Remaining misc tools\n  pacstrap /mnt reflector git gvim openssh ansible terminus-font systemd-swap |& tee -a \"${LOG}\"\n  [[ $? -ne 0 ]] && error_msg \"Installing base system to /mnt failed. Check error messages above. Part 6.\"\n\n  # Add the ssh group\n  arch_chroot \"groupadd ssh\"\n\n  # Set the NetworkManager & ssh services to be enabled\n  arch_chroot \"systemctl enable NetworkManager.service\"\n  arch_chroot \"systemctl enable wpa_supplicant.service\"\n  arch_chroot \"systemctl enable sshd.service\"\n}\nNext up we have some user account setup and configuration for ansible, which will be used for the rest of the configuration of the machine. The original script added some public keys to the ansible user’s authorized keys file. I’d like to add some automation to handle my key management approach but the VM I’m working in makes USB passthrough a hassle. (I switched to Hyper-V part way through making this guide as Virtualbox and WSL2 were fighting on my machine).\nThere’s a script for updating the root user account, but it has a subset of what’s in the ansible account, so let’s just look at that one:\nsetup_ansible_account() {\n  print_info \"Setting up Ansible account\"\n\n  arch_chroot \"useradd -m -G wheel -s /bin/bash ansible\"\n\n  arch_chroot \"echo -n 'ansible:$ANSIBLE_PWD' | chpasswd -c SHA512\"\n\n  arch_chroot \"chfn ansible -f Ansible\"\n\n  mkdir -p /mnt/home/ansible/.ssh\n  chmod 0700 /mnt/home/ansible/.ssh\n  arch_chroot \"chown -R ansible:ansible /home/ansible/.ssh\"\n\n  # Add user to the ssh\n  arch_chroot \"usermod -a -G ssh ansible\"\n}\nuseradd does about what you’d expect. -m creates a home directory for that user if it doesn’t exist. -G is followed by a list of groups you want the user to be a part of. In this case we want ansible to be part of wheel so it can perform actions with elevated privileges. chpasswd is a pretty cool way to set a user password from a script without user interaction, man pages here. chfn is used to change user info, in this case to give the ansible user the first name Ansible."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html",
    "href": "posts/2020-07-09-pypack.html",
    "title": "Python packaging",
    "section": "",
    "text": "Probably the best way to introduce this post is to explain a bit of my background, and then describe the problem I’m trying to solve.\n\n\nI have been using python for data analysis work since about 2017, so around 3 years at the time of writing this post. I work on a small team, and so it’s necessary for us to be able to share code for things like implementing business logic, or connecting to internal data sources. I also maintain an open source package called stats_can that can be used to access Statistics Canada datasets in python.\n\n\n\nThe current way my team shares code is by having a repository with a lib folder in it, and adding that folder to the PYTHONPATH environment variable in Windows.\nThe current way I build new versions of stats_can is through a cargo cult sequence of steps that I kind of sort of understand.\n\n\n\nFor the shared team library all of our stuff is basically in one giant package, broken up into subpackages. This leads to all sorts of problems:\n\nIt’s very difficult to write tests for it.\nThere’s no version numbering so it’s impossible to pin code at a particular version.\nWe can’t share it easily with other teams, and we really can’t share just one particular subpackage of it with other teams.\nThe whole thing just feels very wrong to me. I knew it wasn’t the way to go when I set it up, but I was very new to python and just didn’t have the experience/capacity to find a better way and it worked for the time being.\n\nFor stats_can my current system more or less works, it just has two problems:\n\nI only build conda packages. I’d like to allow pip users to access it but…\nLike I said, the build process is a bit of a house of cards that I barely understand, so adding in another build steps scares me.\n\nBoth of the examples described above are for libraries. I’ve built a couple of small apps, but have even less of an idea the correct way to build/deploy them.\n\n\n\nBasically I want to figure out the current best practice way to do the following:\n\nbuild a library package with versions that can be installed with pip and conda\ndeploy those packages to both a privately hosted repository (for work specific stuff) as well as pypi and Anaconda Cloud or conda-forge for public open source stuff\nOriginally I was also going to include building user facing (web or CLI) apps but this got pretty long already so I think I’m going to leave that for another post\nDitto for CI/CD, linting, extensive testing, and all the other things that go into managing a project. Too big to include in this post.\n\nSo a library with conda and pip packages, hosted both publicly and privately means four total ways to manage the library.\n\n\n\nI find that most of the packaging guides I’ve read show either how to build a completely trivial project that demonstrates one narrow feature, or some giant project that’s a lot to take in all at once. My aim is to start from a single file script and gradually build it up to the final product that I laid out in the what I’m trying to accomplish section. I’ll host the repositories for the library/app on GitHub, and use tags in order to mark the progress of the project through various stages."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#preliminary-setup",
    "href": "posts/2020-07-09-pypack.html#preliminary-setup",
    "title": "Python packaging",
    "section": "Preliminary setup",
    "text": "Preliminary setup\n\nCreate repository\nThe first step in any project is to make a repository. This one has the uncreative name of ianlibdemo. If you want to follow along at home you can clone it and check out the tag for the associated stage in the tutorial. The state of the repository right after being created in this case can be accessed with git checkout eg01\n\n\nSet up environment\nSo I have somewhere to work from, and also to make this process reproducible for others the next thing I have to do is create an isolated python environment to work in. I’m a conda user so I’ll create an environment.yml file:\nname: ianlibdemo_conda_env\ndependencies:\n  - python\nThen I’ll create the environment with conda env create -f environment.yml.\nThere’s absolutely nothing to this environment, which is kind of the point.\n\n\nMake my super sweet library\nEnough talk! Let’s write some code! Well, actually, I’m not going to write any code. The point of this tutorial is to build a package, not write a super awesome library, so I’m just going to copy the demo project used in SciPy 2018 - the sheer joy of packaging. The original code is here. Basically what the module does is take a text file and output a copy with all the words capitalized (except a specified subset).\nIn the root directory of the repository I’ll copy the capital_mod.py file and cap_data.txt. I’ll also create an example_in.txt file that I can use to manually test the capitalize function.\nNow I have the following files in my repository:\n$ ls\n__pycache__/  capital_mod.py   example_in.txt   LICENSE\ncap_data.txt  environment.yml  README.md\nI can test the “package” out from the interactive prompt:\n$ python -i\nPython 3.8.3 (default, May 19 2020, 06:50:17) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import capital_mod\n>>> capital_mod.get_datafile_name()\nWindowsPath('C:/Users/ianep/Documents/ianlibdemo/cap_data.txt')\n>>> capital_mod.capitalize(\"example_in.txt\", \"example_out.txt\")\n>>> quit()\nEverything looks like it ran fine, and if I check in the directory I have file example_out.txt that is indeed a capitalized version of example_in.txt. If you want to get your repository to this point run git checkout eg02.\nSo everything works great and we can go home, right?"
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#run-into-problems",
    "href": "posts/2020-07-09-pypack.html#run-into-problems",
    "title": "Python packaging",
    "section": "Run into problems",
    "text": "Run into problems\nThis is all well and good, but I don’t just want to use this functionality in this folder. The idea is that this is a utility library. Presumably there are all sorts of scripts that I want to add this file capitalization capability to. Maybe I have coworkers I want to share this with, or use it in an app I’m building. As it stands how can I accomplish this?"
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#some-bad-ways-to-solve-the-problem",
    "href": "posts/2020-07-09-pypack.html#some-bad-ways-to-solve-the-problem",
    "title": "Python packaging",
    "section": "Some bad ways to solve the problem",
    "text": "Some bad ways to solve the problem\n\nJust copy the file everywhere\nFine. It only works from the local directory? I’ll just put a copy of it everywhere I want it. This is pretty clearly a bad idea. It will be annoying to copy the file into every location I might want to use it, if I ever have to update the functionality I will then have to track down every instance of that file and make the change repeatedly, and it violates DRY so any experienced developer that sees me do it will make fun of me. Better not do it this way.\n\n\nAdd it to the path\nThis is already going to be a really long guide so I don’t want to add too much about the python path directly. This guide by Chris Yeh is the best I’ve found on the python path and import statements, so if you’re curious by all means check that out. Briefly though, let’s demonstrate the two ways we could directly add this “package” to the path, and therefore run it without being in the same directory.\nTo set the stage I’ve created a new directory separate from the package, and created a text file that I will try and capitalize:\n(ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp\n$ ls\ndemo_in.txt\n(ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp\n$ cat demo_in.txt\ni want to capitalize this text file, but it's in the wrong folder. oh no!\nIf I just try and do the same steps I did from within the folder it will fail:\n>>> import capital_mod\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nModuleNotFoundError: No module named 'capital_mod'\nThat’s because the folder with capital_mod.py is not on my path.\nOne way I can solve this is by adding the path to capital_mod.py to my path. Like so:\n$ export PYTHONPATH=\"/c/Users/Ian/Documents/ianlibdemo\"\n(ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp\n$ python -i\nPython 3.8.2 | packaged by conda-forge | (default, Apr 24 2020, 07:34:03) [MSC v.1916 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import capital_mod\n>>> capital_mod.get_datafile_name()\nWindowsPath('C:/Users/Ian/Documents/ianlibdemo/cap_data.txt')\n>>> capital_mod.capitalize(\"demo_in.txt\", \"demo_out.txt\")\n>>> quit()\n(ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp\n$ cat demo_out.txt\nI Want to Capitalize This Text File, But It's In the Wrong Folder. Oh No!\nThis worked, but I don’t want to have to run that export command every time before I run a script, and sharing this code with other people and telling them to do that every time seems like a hassle. There are ways to permanently add folders to your python path. This guide covers them nicely. But we’re not actually going to go this route so let’s move on.\nThe slightly less hacky way is to use sys.path from within a python script. Back in my demo directory I can write a python script that looks like this:\nimport sys\nsys.path.append(r\"C:\\Users\\Ian\\Documents\\ianlibdemo\")\nimport capital_mod\ncapital_mod.capitalize(\"demo_in.txt\", \"demo_out.txt\")\nWe can see that this works as well:\n(ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp\n$ ls\ndemo_in.txt  syspathdemo.py\n(ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp\n$ python syspathdemo.py\n(ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp\n$ ls\ndemo_in.txt  demo_out.txt  syspathdemo.py\n(ianlibdemo_conda_env) Ian@terra ~/Documents/demo_tmp\n$ cat demo_out.txt\nI Want to Capitalize This Text File, But It's In the Wrong Folder. Oh No!\nThis also worked, but I had to import sys, and I had to know the exact path to the library. It’s going to be annoying to have to put that in every script, and if I try and share this code with anyone else they’re going to have to modify it to point to wherever they’ve saved my library code."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#get-hypermodern",
    "href": "posts/2020-07-09-pypack.html#get-hypermodern",
    "title": "Python packaging",
    "section": "Get hypermodern",
    "text": "Get hypermodern\nAs I was working on this guide I discovered a series of articles by Claudio Jolowicz called Hypermodern Python. The series is an opinionated (in a good way) look at how to configure a python project in 2020. It’s excellent and well worth a read, but I can’t completely adopt its recommendations for two related reasons. The first is that it assumes you’re either using a *NIX system or can load WSL2 on your Windows machine. For my work setup neither of those assumptions hold. It also assumes you’re working in the standard python ecosystem and therefore doesn’t reference conda either for environment management or packaging. For the remainder of this guide I’m going to try and follow Claudio’s suggestions where possible, but adapt them to incorporate conda."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#turn-our-code-into-a-poetry-package",
    "href": "posts/2020-07-09-pypack.html#turn-our-code-into-a-poetry-package",
    "title": "Python packaging",
    "section": "Turn our code into a poetry package",
    "text": "Turn our code into a poetry package\nPoetry seems to be the current best practice for building python packages. Let’s see if we can get it working with conda.\n\nPoetry init\nAfter adding poetry as a dependency to my conda environment and updating the environment I run poetry init:\n$ poetry init\n\nThis command will guide you through creating your pyproject.toml config.\n\nPackage name [ianlibdemo]:\nVersion [0.1.0]:\nDescription []:  Python packaging - how does it work?\nAuthor [[Ian Preston] <17241371+ianepreston@users.noreply.github.com>, n to skip]:  Ian Preston\nLicense []:  GPL-3.0-or-later\nCompatible Python versions [^3.7]:\n\nWould you like to define your main dependencies interactively? (yes/no) [yes] no\nWould you like to define your dev dependencies (require-dev) interactively (yes/no) [yes] no\nGenerated file\n\n[tool.poetry]\nname = \"ianlibdemo\"\nversion = \"0.1.0\"\ndescription = \"Python packaging - how does it work?\"\nauthors = [\"Ian Preston\"]\nlicense = \"GPL-3.0-or-later\"\n\n[tool.poetry.dependencies]\npython = \"^3.7\"\n\n[tool.poetry.dev-dependencies]\n\n[build-system]\nrequires = [\"poetry>=0.12\"]\nbuild-backend = \"poetry.masonry.api\"\n\n\nDo you confirm generation? (yes/no) [yes] yes\nAt the end of this process I have a pyproject.toml file in the root of my repository with the text listed above inside.\n\n\nsrc layout\nThe root folder of this repository is getting crowded. I’ve got various files that either describe the project or the environment I’m supposed to work on it in intermingled with the actual source code for the package. To address this I’ll make a separate folder for the actual package files, and as recommended by hypermodern python I’ll use src layout\n\n\npoetry install\nThe last step for a basic install is to use poetry to install the package into the environment. Since poetry 1.0 it should be able to detect conda environments and do its installation directly into them based on this PR.\n$ poetry install\nUpdating dependencies\nResolving dependencies... (0.1s)\n\nWriting lock file\n\nNo dependencies to install or update\n\n  - Installing ianlibdemo (0.1.0)\nSeems to work, let’s try that old example that wouldn’t run before:\n(ianlibdemo_conda_env) e975360@N2012 /c/tfs/text_demo\n$ ls\nexample_in.txt\n(ianlibdemo_conda_env) e975360@N2012 /c/tfs/text_demo\n$ python -i\nPython 3.7.7 (default, May  6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from ianlibdemo.capital_mod import capitalize\n>>> capitalize(\"example_in.txt\", \"example_out.txt\")\n>>> quit()\n(ianlibdemo_conda_env) e975360@N2012 /c/tfs/text_demo\n$ cat example_in.txt\nthese words will all get capitalized, except the ones in that super special text file, like is, or, and a.\n(ianlibdemo_conda_env) e975360@N2012 /c/tfs/text_demo\n$ cat example_out.txt\nThese Words Will All Get Capitalized, Except the Ones In That Super Special Text File, Like Is, Or, And A.\nMagic! Note that I have to do one more layer of importing from the ianlibdemo package whereas before I was directly importing the capital_mod module, but otherwise we’re gold.\nOf course this hasn’t really solved my problem yet, I still don’t have an actual package that other people can install. But still, progress!\n\n\npoetry build\nIt turns out that making it to the previous step was essentially all I needed to create a pip installable package. Just running poetry build from the root of the repository creates a dist folder containing a sdist and a wheel\n\ntest the build\nHaving built this package, how would I install it?\nTo start the test I’ll create a new empty conda environment and make sure I can’t import the ianlibdemo package.\n$ conda create -n pyonly python\n...\n$ conda activate pyonly\n$ python -i\n>>> import ianlibdemo\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nModuleNotFoundError: No module named 'ianlibdemo'\nThis verifies that I have a clean environment without that package installed. I can use pip to install it like so:\n$ pip install /c/tfs/ianlibdemo/dist/ianlibdemo-0.1.0-py3-none-any.whl\n$ python -i\nPython 3.8.3 (default, May 19 2020, 06:50:17) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\"\n>>> import ianlibdemo\nThe import ran successfully. I haven’t done a lot of validation that the package works the way I’d expect, but I’ll get to that when we set up testing later. Note that I installed the package using the .whl file that the build process created, but I could have also used the .tar.gz file in the same folder just as easily.\nSince we’ve now built a working package this seems like another good place for a checkpoint. To see the state of the project at this point you can run git checkout eg03."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#automate-testing",
    "href": "posts/2020-07-09-pypack.html#automate-testing",
    "title": "Python packaging",
    "section": "Automate testing",
    "text": "Automate testing\nThis is already going to be a big post so I’m definitely not going to offer extensive notes on testing, but I’d like to include enough to at least ensure it integrates with the rest of the process, and to save manually testing after each step.\n\nAdd a pytest dependency\nWe want to use pytest for testing, so the first step is to add it as a development dependency. Normally this would be a simple one liner, poetry add --dev pytest, but because of this bug between conda and poetry, at least at the time of this writing I had to install an update of msgpack before I could get it to run. I’ve amended the environment.yml file to include this fix, so between that and hopefully this bug being resolved in time this shouldn’t be an issue for anyone else following this guide, I just wanted to flag what I encountered and how I resolved it.\n\n\nWrite tests\nNow in the base of the repository we add a tests folder and add an empty __init__.py file and a test_capitalize.py file. The test file looks like this:\nfrom ianlibdemo import capital_mod\n\n\ndef test_capitalize_file(tmp_path):\n    in_file = tmp_path / \"in_file.txt\"\n    in_content = \"this is the lowercase input sentence\"\n    in_file.write_text(in_content)\n    out_file = tmp_path / \"out_file.txt\"\n    out_content = \"This is the Lowercase Input Sentence\\n\"\n    # Output shouldn't exist before we call the function\n    assert not out_file.exists()\n    capital_mod.capitalize(in_file, out_file)\n    assert out_file.exists()\n    assert out_file.read_text() == out_content\nNow from the base directory of the repository I can run pytest with poetry run pytest.\nTo see the project at this stage run git checkout eg04."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#publish-to-pypi",
    "href": "posts/2020-07-09-pypack.html#publish-to-pypi",
    "title": "Python packaging",
    "section": "Publish to pypi",
    "text": "Publish to pypi\nI’ve built a package, I can test that it works, the next step is to publish it somewhere for others to access. The defacto source for python packages is PyPi. However, since this is just a demo package I don’t want to publish it there, since it will just add clutter. Fortunately, there is a similar location designed exactly for testing out publishing packages, appropriately named Test PyPi.\n\nSet up for publishing\nIn order to publish packages you need an account. The registration process is straightforward. Note that pypi and test pypi use completely separate databases, and you will need an account for each of them. For now we’re just publishing to test pypi so it’s not an issue, but just something to keep in mind.\nNext I want to create an API token. You can just use your username and password to authenticate and publish packages, but tokens are the preferred method. Once you’re logged in you can click on your account, go to account settings, and under API tokens click “add API token”. Give it a descriptive name and save it somewhere secure (I put mine in a LastPass note). As they warn on the page it will only be displayed once, and if you lose it you’ll have to delete it and create a new one.\nNow we need to set up the test pypi repository in poetry. From the poetry docs you can see that repositories are added to your poetry config:\npoetry config repositories.testpypi https://test.pypi.org/legacy/\npoetry config pypi-token.testpypi <your api key>\nNote that these configurations are global to poetry, so they’re not saved in your repository. If you switch machines, or (I think) change conda environments since we installed poetry with conda you’ll have to redo these configurations.\n\n\nPublish\nOnce this is set up publishing is quite straightforward. If you haven’t already built the package do so with poetry build and then run poetry publish --repository testpypi.\n\n\n\n\n\ntest pypi\n\n\nLook at that! There it is!\n\n\nPull it back down and test\nLet’s just make sure that all worked.\nFirst make a clean conda environment with just pytest for testing and activate it:\nconda create -n test_env pytest\nconda activate test_env\nNavigate to the root of your package folder and try running tests. They should fail, because we don’t have the package installed in this environment:\ncd ~/Documents/ianlibdemo\npytest\n.\n.\n.\ntests\\test_capitalize.py:1: in <module>\n    from ianlibdemo import capital_mod\nE   ModuleNotFoundError: No module named 'ianlibdemo'\nNow pip install that package and try running tests again:\npip install --index-url https://test.pypi.org/simple/ ianlibdemo\npytest\n.\n.\n.\n======================== 1 passed, 1 warning in 0.09s =========================\nLooks good!"
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#publish-to-a-private-repository",
    "href": "posts/2020-07-09-pypack.html#publish-to-a-private-repository",
    "title": "Python packaging",
    "section": "Publish to a private repository",
    "text": "Publish to a private repository\nNot all of the code we develop should be published on the public internet. Some of it you just want accessible to an internal team. I have a private package index running using this docker container - setting that up will be its own post. Once you have that all set up though the process is exactly the same as for the public pypi so I’ll leave it at that for this guide.\nNone of the steps used to publish this package required changes to the library repository, so you can still use git checkout eg04 to view the state of the repository at this point."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#adding-dependencies",
    "href": "posts/2020-07-09-pypack.html#adding-dependencies",
    "title": "Python packaging",
    "section": "Adding dependencies",
    "text": "Adding dependencies\nOne thing I realized I should ensure is that all of this works with libraries that depend on other libraries. Let’s add a dependency on pandas and give that a shot.\nFortunately adding a dependency is easy. Since I want to require pandas I just run poetry add pandas and it’s now a dependency. I added a module called fun_pandas and a test for it in my tests suite. After that I rebuilt the package and uploaded it to a repository as described above, pulled it back down and tested it like before and everything worked! It’s nice when that happens.\nTo see the project at this stage you can run git checkout eg05."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#now-do-conda",
    "href": "posts/2020-07-09-pypack.html#now-do-conda",
    "title": "Python packaging",
    "section": "Now do conda",
    "text": "Now do conda\nThe next thing I want to work out is how to build a conda package. The first step is to add conda-build to my environment. The next step is to define a meta.yaml file to specify how to do the build.\n\nSort of working build\nRather than just dump the final working file, I think it will be useful to step through from the first version I got working to the final one I’m happy with. A lot of the steps for setting this up are hacky, so seeing what doesn’t work is as important as seeing what does for people who are trying to figure out how to apply this to their own projects.\nHere’s the first version of my meta.yaml that actually built:\n{% raw %}\n{% set name =  \"ianlibdemo\" %}\n{% set version = \"0.2.0\" %}\n\npackage:\n    name: \"{{ name|lower }}\"\n    version: \"{{ version }}\"\n\nsource:\n    path: ./dist/{{ name }}-{{ version }}-py3-none-any.whl\n\nbuild:\n    script: \"{{ PYTHON }} -m pip install ./{{ name }}-{{ version }}-py3-none-any.whl --no-deps --ignore-installed -vv \"\n\nrequirements:\n    host:\n        - pip\n        - python\n    run:\n        - python\n        - pandas\n\ntest:\n    imports:\n        - ianlibdemo\n{% endraw %}\nFrom an environment with conda-build installed I can build a package by running conda-build . from the base of the repository. It creates a conda package as a tar.bz2 file in a deeply nested directory. From there I can install it into an environment with something like:\nconda install /c/Users/e975360/.conda/envs/conda_build_test/conda-bld/win-64/ianlibdemo-0.2.0-py38_0.tar.bz2\nRunning pytest in an environment with that package installed resulted in one passed test and one failure for the one requiring pandas. As we’ll see below, that issue will get solved if I can load it to a package repository so I’ll leave that alone at this point.\n\nIssues with this build\n\nFirst off, note that it’s called meta.yaml not meta.yml. Despite .yml being the common and preferred extension for this file type (see this SO thread) it has to end with .yaml or conda-build can’t find it.\nAlso note that I’m pointing it to the .whl file that I built with poetry, rather than the .tar.gz that’s in the same folder. In theory I should be able to do either, and most examples online point to .tar.gz files, but I got errors about not having poetry in my build environment, and when I tried to add poetry I got a version conflict because apparently the main conda repository only has the python2.7 version of poetry and… it just seemed easier to use the .whl.\nIt makes a build that claims to be specific to windows and python 3.8 when in fact this should run on any OS and any python 3.\nI have to repeat the file name in two places\nI’m specifying the version number in two places now since it’s already in the pyproject.toml file. There’s a risk of them getting out of sync\nSimilar to the version number I have to specify dependencies in this file as well as pyproject.toml (pandas in this case). Unfortunately, since conda packages can have slightly different names than their pypi counterparts, and I have to actually specify python as a dependency here I don’t think there’s an automated way to keep these in sync. Fortunately I don’t expect dependencies to change as often as the package version so this will be less of a burden to manage.\nTo do anything with the created package I have to scroll up through a big install log and find the path to the file\nI get a bunch of build environments and intermediate files created on my machine (maybe this is why the build guide suggests using docker).\n\n\n\nFixing the issues\nSetting the build to work for any OS and python is an easy fix. Under the build section you just add one line. The build section now looks like this:\n{% raw %}\nbuild:\n    noarch: python\n    script: \"{{ PYTHON }} -m pip install ./{{ name }}-{{ version }}-py3-none-any.whl --no-deps --ignore-installed -vv \"\n{% endraw %}\nDefining the package file in once place is similarly easy. Jinja lets you concatenate variables with the ~ symbol. The updated relevant section looks like this:\n{% raw %}\n{% set name =  \"ianlibdemo\" %}\n{% set version = \"0.2.0\" %}\n{% set wheel = name ~ \"-\" ~ version ~ \"-py3-none-any.whl\" %}\npackage:\n    name: \"{{ name|lower }}\"\n    version: \"{{ version }}\"\n\nsource:\n    path: ./dist/{{ wheel }}\n\nbuild:\n    noarch: python\n    script: \"{{ PYTHON }} -m pip install ./{{ wheel }} --no-deps --ignore-installed -vv \"\n{% endraw %}\n\nAdding a Makefile\nThe rest of the issues outlined above aren’t directly the result of the meta.yaml file. To resolve them I’ll need to write some scripts, and to tie that all together I’ll use my good friend Make.\nTo begin I add some boilerplate to the beginning of the file to handle conda environments\n# Oneshell means I can run multiple lines in a recipe in the same shell, so I don't have to\n# chain commands together with semicolon\n.ONESHELL:\n# Need to specify bash in order for conda activate to work.\nSHELL=/bin/bash\n# Note that the extra activate is needed to ensure that the activate floats env to the front of PATH\nCONDA_ACTIVATE=source $$(conda info --base)/etc/profile.d/conda.sh ; conda activate ; conda activate\nENV_NAME = ianlibdemo_conda_env\nNext I create a python script that will read the version number from pyproject.toml and update the version in meta.yaml with it. I won’t reproduce that script here but it’s in the scripts folder of the ianlibdemo repository.\nFinally I add a target to sync the versions. I can then make that a pre-requisite of building the conda package.\n.PHONY: versionsync\n\nversionsync:\n    $(CONDA_ACTIVATE) $(PROJECT_NAME)\n    python scripts/version_sync.py\n.PHONY: means that target should be run each time it’s called. By default Make won’t redo a target if an output file already exists.\nNow running make versionsync from the root of the repository will take the version from pyproject.toml and put it in meta.yaml. Eventually I’ll also want to ensure that the python package has been built by poetry before building the conda package.\nPS: I documented how you can activate conda environments from within makefiles and bash scripts here. Since I had to refer back to it when doing this I thought it would be helpful to include a pointer.\nThe next issue I described above is that running conda-build generates the package in some obscure subdirectory and you have to scroll back up through the log file to find it. If I want to upload the package to a repository or install it directly that’s going to be a hassle. Fortunately conda-build comes with a --output flag that you can run to return where your package file would be saved if you actually ran conda-build. Knowing this I can write a small bash script which first builds the package and then uses the --output flag to find the generated package and copy it into my dist directory.\nThe new part of the Makefile looks like this:\nconda:\n    $(CONDA_ACTIVATE) $(ENV_NAME)\n    bash scripts/conda_build.sh\nAnd the bash script looks like this:\n#!/bin/bash\nconda-build .\nCONDA_PACK=$(conda-build . --output)\ncp $CONDA_PACK dist/\nI’m going to make a cleanup function later to remove all build artifacts so we’ll leave that alone for now.\n\n\n\n\nPublish to a public channel\nTo publish to an external public conda channel I have to install the anaconda-client package in my environment. The first time I do an upload I will need to log in with anaconda login and provide my username and password.\nAfter that I can add a new recipe to my makefile to publish the package:\nconda_ext_pub: conda\n    $(CONDA_ACTIVATE) $(ENV_NAME)\n    anaconda upload $$(conda-build . --output)\nconda_ext_pub depends on the conda recipe so this will build the package first and then upload it to Anaconda.org. After running make conda_ext_pub I can see that the package was indeed published to Anaconda.org:\n\n\n\nAnaconda\n\n\nAs with the previous installations I can create a new blank environment with just pytest installed, install this package into it with conda install -c ian.e.preston ianlibdemo and now both my tests pass, as pandas is installed as well.\n\n\nPublish to a private channel\nAs with the other private repository, actually setting up the repository is outside the scope of this post. This will assume that you have one created and that packages are stored on some sort of file share that you can access from your build machine. There’s no fancy way to publish conda packages to a private repository. You just drop the package file in the appropriate architecture subfolder (noarch in this case since this is a pure python package) and then run conda index on the repository folder. My server has a file watcher that detects changes and auto runs that, so all we have to do to publish a package is to make sure it’s in the right place. In this example the file share from my local machine is at \\\\r4001\\finpublic\\FP&A\\channel_test\\noarch and the web server is available at http://dml01:8081/.\nTo set up publishing I add the following to my makefile:\nCONDA_LIB_DIR = //r4001/finpublic/FP\\&A/channel_test/noarch\n.\n.\n.\nconda_int_pub: conda\n    $(CONDA_ACTIVATE) $(ENV_NAME)\n    cp $$(conda-build . --output) $(CONDA_LIB_DIR)\nAfter that I can install the package into a library by running conda install -c http://dml01:8081 ianlibdemo.\nTo see the project at this stage you can run git checkout eg06."
  },
  {
    "objectID": "posts/2020-07-09-pypack.html#put-it-all-together",
    "href": "posts/2020-07-09-pypack.html#put-it-all-together",
    "title": "Python packaging",
    "section": "Put it all together",
    "text": "Put it all together\nAll of the pieces are here, so the final thing to do is to put them all together. I started that process in the last section by creating a makefile, now I just have to finish it up by tying the pip packaging and publishing in with the conda packaging and publishing.\n\nClean slate\nAfter a package file is built and published we don’t really have any further need for it locally, but it’s not automatically deleted. Let’s make a clean task in Make that will clear out any previous builds. That way any new process can start fresh.\nThe clean task looks like this:\nclean:\n    # remove pip packages\n    rm -rf ./dist/*\n    # remove conda packages and build artifacts\n    $(CONDA_ACTIVATE) $(ENV_NAME)\n    bash scripts/conda_clean.sh\nand conda_clean.sh looks like this:\n#!/bin/bash\nexport CONDA_BLD_PATH=${CONDA_PREFIX}/conda-bld\nrm -rf $CONDA_BLD_PATH\n\n\nFull build chain\nThe last step is to add make tasks to build and publish the pip packages and set them as appropriate dependencies for the conda steps.\nFirst, add a task to build the pip installable package:\npip: clean\n    $(CONDA_ACTIVATE) $(ENV_NAME)\n    poetry build\nNext add tasks to publish to external and internal pip sources:\npip_ext_pub: pip\n    $(CONDA_ACTIVATE) $(ENV_NAME)\n    poetry publish --repository testpypi\n\npip_int_pub: pip\n    $(CONDA_ACTIVATE) $(ENV_NAME)\n    poetry publish --repository localpypi\nFinally as an example we can make wrapper tasks that will publish pip and conda packages to external/internal sources:\nall_int_pub: pip_int_pub conda_int_pub\n    echo \"publishing to internal conda and pip repository\"\n\nall_ext_pub: pip_ext_pub conda_ext_pub\n    echo \"publishing to external conda and pip repository\"\nAt this point if you want to build and publish your package you can just run make all_int_pub and it will clear out old build artifacts, build a new pip installable package, upload it to the internal pip package repository, sync the version number with conda, build a conda package and publish that to the internal conda package repository. Not bad!\nThis is concludes the changes I’m planning to make in this repository. If you just clone the repository as is you should see it in this state, or you can run git checkout eg07."
  },
  {
    "objectID": "posts/2023-02-03-migration.html",
    "href": "posts/2023-02-03-migration.html",
    "title": "Migrating to quarto",
    "section": "",
    "text": "Update\nThis blog is now rendered with quarto rather than fastpages. Honestly fastpages was working fine for me, but they’ve deprecated it and I like being on the new thing. There are some nice navigation and filtering features added in this new tool, and the live preview on my dev machine works quite nicely. Mostly I’m making this post to ensure that my github action was set up correctly."
  },
  {
    "objectID": "posts/2021-01-09-traefik-lan.html",
    "href": "posts/2021-01-09-traefik-lan.html",
    "title": "Using traefik for internal services",
    "section": "",
    "text": "Introduction\nI have a small server at home that I use to run various services in docker containers. I don’t expose any of them to the internet, I can tunnel in through my vpn running on my pfsense router. Because of that up until recently it was enough to just publish the ports of my various services and make a bookmark pointing to my server at that port. Recently I wanted to set up a couple services that both expected to be exposed on port 80, the default http port. Just remapping the port would be insufficient, because their clients did not have options to specify a port to connect on. To get around this issue, I decided to suck it up and learn to use a reverse proxy. I looked into both nginx and traefik and settled on traefik. Unfortunately for me, pretty much all the tutorials online expect you to be exposing traefik to the web and have all sorts of stuff about TLS and letsencrypt that don’t matter to me, and don’t really explain how to do the routing if you only want it to work on your internal network. This guide is for me in the future if I ever have to set this up again, and anyone else that’s interested in a similar setup.\nThis guide isn’t going to cover what a reverse proxy is, or many of the details of traefik, there are lots of good guides for that online. It’s specifically going to cover the configuration specific to a LAN only connection.\n\n\nThe key configuration\nMy pfsense box manages DNS and allows me to resolve machines on my network by their name. For instance, my server is named mars and if I run ping mars it will correctly resolve to that machine’s IP. Traefik doesn’t like to play nice with that naming convetion though, at least I couldn’t get it to work with all sorts of combinations of mars and mars.localdomain. Fortunately, I found a forum post that addressed this issue. In my DNS settings I picked a nice sounding domain that I was sure I wouldn’t actually want to connect to (in my case I used ian.ca) and set my DNS to redirect any requests to that domain to go to my server. To do this, from pfsense I went to services -> DNS resolver, and added two lines to the “custom options” section near the bottom\nlocal-zone: \"ian.ca\" redirect\nlocal-data: \"ian.ca A <my server IP>\"\n\n\nSetting up Traefik\nMost of the rest of the traefik setup could be borrowed from other basic guides. I configure traefik and all my other docker containers in an ansible role. You can find that on my GitHub. A couple small gotchas that I ran into were making sure Traefik was on the same docker network as the containers I wanted it to route, and to remember the difference between exposing ports (available within the docker network) and publishing ports (mapping them to a port on the host).\n\n\nConclusion\nThis was one of those setups that is straightforward in retrospect, but I had to spend a lot of time googling and banging my head against the wall before I could get the correct combination of configurations to do what I want. Hopefully this is useful to others, or at least me next time I try and do this."
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html",
    "href": "posts/2020-05-06-pfsense.html",
    "title": "Building pfsense",
    "section": "",
    "text": "This guide is mostly for me. I’m trying to get better at documenting all the tech stuff I do on my home system, both to ensure I have a decent understanding of what I’m doing, and also to ensure I can reproduce it if I need to rebuild at some point in the future. This guide is heavily reliant on the guidance from Mark Furneaux in his YouTube series on the topic. The first video in that playlist provides a good overview of what pfsense is and why you might want to install it. If you watch that video and think you might like to install pfsense, but don’t want to watch about 9 hours of YouTube to figure out how to set it up, this might come in handy for you as well.\n\n\n\nAt a high level this will cover installing and configuring a pfsense box along with a Unifi wireless access point. Hardware selection is not covered here. The table of contents describes the sections of pfsense settings and applications that will be set up over the course of this guide."
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#addendum",
    "href": "posts/2020-05-06-pfsense.html#addendum",
    "title": "Building pfsense",
    "section": "Addendum",
    "text": "Addendum\nI have an old Kobo reader. After switching to the Unifi AP I couldn’t get it on the WiFi. An even older Kindle would connect, and my newer Kobo connected after manually entering the SSID, but no luck with the older one. Eventually I found this reddit thread with the solution. From the Unifi control panel I went to settings -> wireless networks -> <my SSID> -> advanced and unchecked “enable minimum data rate control” for the 2G network. After applying that and trying to connect a few more times I made it online with the Kobo."
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#system-general-setup",
    "href": "posts/2020-05-06-pfsense.html#system-general-setup",
    "title": "Building pfsense",
    "section": "System / General Setup",
    "text": "System / General Setup\nOnly two minor changes here:"
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#system-advanced-admin-access",
    "href": "posts/2020-05-06-pfsense.html#system-advanced-admin-access",
    "title": "Building pfsense",
    "section": "System / Advanced / Admin Access",
    "text": "System / Advanced / Admin Access\nTurn off https since we don’t have a certificate authority (maybe I’ll figure that part out later)\n\n\n\nadmin\n\n\nChange browser tab text so I know what page each tab is on.\n\n\n\ntext\n\n\nEnable SSH and change the default port to 2222 (security through obscurity). I’m going to set up keys later, for now let it work with a password but come back and update this later.\n\n\n\nssh\n\n\nFun piece of trivia, right as I enabled this my web front-end froze. The actual router was still functioning, but I couldn’t get any web admin. Part of it was that I had to switch from https to http in the URL, but even after that I would hit the login, it would accept my password, and just take me back to the login page. I found this reddit thread which had the same issue. For me I could connect in using a private browsing window, which led me to try restarting my browser, which fixed it. Computers…"
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#system-advanced-firewall-nat",
    "href": "posts/2020-05-06-pfsense.html#system-advanced-firewall-nat",
    "title": "Building pfsense",
    "section": "System / Advanced / Firewall & NAT",
    "text": "System / Advanced / Firewall & NAT\nSet Firewall optimization to conservative. I have plenty of CPU and RAM for the size of my network, so why not give idle connections a little longer to hang out? Apparently this can improve VOIP performance.\n\n\n\nfirewall"
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#system-advanced-networking",
    "href": "posts/2020-05-06-pfsense.html#system-advanced-networking",
    "title": "Building pfsense",
    "section": "System / Advanced / Networking",
    "text": "System / Advanced / Networking\nFor now I’m just going to disable IPv6. I don’t think my ISP supports it and I don’t see the need for it on my LAN. Maybe I’ll revisit that later, but for now turning it off seems like the safer approach.\n\n\n\nipv6\n\n\nSet everything possible to work on hardware. Given that I have Intel NICs in my router I think I can safely run all of these things. I ran iperf3 between two wired connections as well as speedtest before enabling the settings and then again after. Note that you have to reboot after changing these settings. After the reboot performance was essentially unchanged so I took that as a good sign and kept the settings.\n\n\n\nhardware"
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#system-advanced-miscellaneous",
    "href": "posts/2020-05-06-pfsense.html#system-advanced-miscellaneous",
    "title": "Building pfsense",
    "section": "System / Advanced / Miscellaneous",
    "text": "System / Advanced / Miscellaneous\nI don’t expect a lot of super heavy CPU needs on this system, so I might as well save some power and heat. I’ll put it on Adaptive to start, but I’ll check out Hiadaptive if some services seem to chug.\n\n\n\nhiadaptive\n\n\nMy CPU supports AES-NI and I’ll want that enabled for better VPN performance later.\n\n\n\naesni"
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#system-advanced-notifications",
    "href": "posts/2020-05-06-pfsense.html#system-advanced-notifications",
    "title": "Building pfsense",
    "section": "System / Advanced / Notifications",
    "text": "System / Advanced / Notifications\nI want email notifications if something gets borked on my router. I don’t want to use my actual gmail address to send these notifications though, as I’d like to keep the security on it a lot more locked down. I created a new gmail account just for the router, and then in my account settings (for the router email do not do this for your actual email) I set “less secure apps” to on so that I could enable sending emails with the following settings:\n\n\n\nnotifications\n\n\nAfter all that I hit “test SMTP settings” and received an email informing me it worked. Hurray!"
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#interfaces-wan-and-lan",
    "href": "posts/2020-05-06-pfsense.html#interfaces-wan-and-lan",
    "title": "Building pfsense",
    "section": "Interfaces / WAN (and LAN)",
    "text": "Interfaces / WAN (and LAN)\nI’m going to disable IPv6 here since as I discussed I don’t want to use it.\nFor the LAN I first have to disable the DHCPv6 service:\n\n\n\ndhcp6\n\n\nAnd then on each respective interface’s page I can disable IPv6\n\n\n\ndhcp6"
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#dashboard",
    "href": "posts/2020-05-06-pfsense.html#dashboard",
    "title": "Building pfsense",
    "section": "Dashboard",
    "text": "Dashboard\nLet’s add some widgets! Everyone loves widgets.\nAdd S.M.A.R.T status so I can see if my hard drive is failing. Services Status to see what’s running. Interface statistics to see if I’m getting any errors, and finally traffic graphs because who doesn’t like a nice live graph?"
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#static-mappings",
    "href": "posts/2020-05-06-pfsense.html#static-mappings",
    "title": "Building pfsense",
    "section": "Static Mappings",
    "text": "Static Mappings\nFor most of the devices on my network I’d like to give them easy to type and remember names, even if they don’t have the functionality to specify a hostname (or even if they do just to be safe). For one thing this makes it easy to connect to devices, but even for things I don’t want to connect to (like an e-reader) it’s nice to give them an obvious name. That way if I’m looking at the DHCP leases on my network it will be easier to notice a new device. To do this there are two steps. The first is adding a DHCP static mapping for each device, and the second is enabling DNS to resolve those names (skipping ahead a bit since DNS is next). While you can do static mappings from the DHCP services page, it’s actually easier to do from the DHCP status page. Beside each device that’s connected there’s an “add static mask icon” which you can click. After that you just have to fill in an IP, the hostname you want, and an optional description:\n\n\n\nstatic mask\n\n\nSince I’m going to set up most of my network this way I’m also going to narrow the range for regular DHCP leases to be handed out back on the DHCP Server service page:\n\n\n\nstatic mask\n\n\nAfter this I’ll have to trigger a release/renew cycle for all devices on the network. The easiest way to do this is probably just reboot the router.\nTo set a proper hostname on a device here are the instructions relevant to me:\nOn Linux: edit /etc/hostname to be whatever you want the hostname to be.\nOn Android: Who knows? I can’t seem to make this give a proper name. Fortunately I can rely on the name from pfsense.\nOn Windows: Hit start, type “pc name” and select the entry that comes up, click “Rename this PC” and change it to whatever you want.\nThe last step is to set the DNS resolver to resolve these names. In services -> DNS resolver I check these two boxes:\n\n\n\nresolver\n\n\nI think I only need the second one since I gave every client on my network a static lease, but the other one seems nice to have as well, so I’ll enable it."
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#basic-setup",
    "href": "posts/2020-05-06-pfsense.html#basic-setup",
    "title": "Building pfsense",
    "section": "Basic Setup",
    "text": "Basic Setup\n\nUnder VPN -> OpenVPN head to the wizard tab.\nLeave authentication backend as Local User Access\nFill in details for the CA. I left most things the default\nSame deal for the server cert, most options should persist from the CA\nSet up general server information\n\nI set hardware crypto on.\nTunnel network has to be an IP range not used by your LAN, and it shouldn’t be a common one. Since my LAN uses 172.17.1.0/24 I went with 172.17.2.0/24\nI left redirect gateway off because I’m just trying to set up LAN access remotely, I don’t need all my traffic tunneled through here if I’m remote\nLocal network points to my LAN so I can access it, so as described above that’s 172.17.1.0/24\nI set concurrent connections to 10. Probably more than I’ll need but why not?\nI enabled Inter-Client Communication\nI set the DNS default domain to localdomain\nI set DNS Server 1 to 172.17.1.1. These two options should allow me to access my servers by hostname even remotely\nI set the NTP server to 172.17.1.1 since I went to all the trouble of configuring it\nI don’t think I need NetBIOS or WINS so I left that blank\n\nCheck boxes for both firewall rules\nWizard complete, now I want to be able to export settings to clients, so over to System -> package manager and install openvpn-client-export\n\n\nDNS resolution\nI’m not sure why I had to set this, but in order to get my client machines to be able to resolve hostnames of servers I had to go into Services -> DNS resolver -> General settings and check “Register connected OpenVPN clients in the DNS resolver”. The way they describe it really sounds like it should allow me to resolve client names, but it fixed my issue so whatever."
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#create-users",
    "href": "posts/2020-05-06-pfsense.html#create-users",
    "title": "Building pfsense",
    "section": "Create Users",
    "text": "Create Users\nSystem -> User Manager -> Add User. For users I’m going with the <person>_<device>_vpn naming convention. So for example my phone’s certificate will be ian_phone_vpn. I don’t add the user to admins, and I create and store a nice secure password with LastPass. Make a certificate for the user, all the default’s should be fine, just fill in an appropriately descriptive name."
  },
  {
    "objectID": "posts/2020-05-06-pfsense.html#export-keys-to-devices",
    "href": "posts/2020-05-06-pfsense.html#export-keys-to-devices",
    "title": "Building pfsense",
    "section": "Export keys to devices",
    "text": "Export keys to devices\nBack to VPN -> OpenVPN -> Client Export Utility. Since I have dynamic DNS configured I changed over Host Name Resolution to my DynamicDNS name rather than my IP which is the default.\nI didn’t mess with any of the defaults. I have the OpenVPN Connect app on my phone so I scrolled down to the user I just created for my phone and clicked “OpenVPN Connect (iOS/Android). Downloaded the file, transferred it to my phone, imported it in the app and was good to go."
  },
  {
    "objectID": "posts/2022-03-17-mortgage.html",
    "href": "posts/2022-03-17-mortgage.html",
    "title": "Modeling out fixed closed vs capped variable mortgages",
    "section": "",
    "text": "I’m looking to buy a house in the near future. I’m pretty risk averse, so even though I know that in theory a variable rate mortgage should be better for me financially, I was willing to take a fixed rate mortgage in order to have more certainty. In discussion with a mortgage broker it was suggested that I could get the best of both worlds by taking out a capped variable mortgage and increasing my payment amount to what I would have paid under a fixed rate. In theory this gives me a hedge in that my payments are capped, and as long as my variable rate is below the fixed rate I’m making extra payments against my principle compared to what I’d be doing with a fixed rate. Let’s see what that looks like under some different scenarios using python. Some of these calculations I already implemented in my rent or own calculator but the scenarios I’m working on are different enough here that I’m only going to use that as a reference rather than importing any of the code."
  },
  {
    "objectID": "posts/2022-03-17-mortgage.html#get-some-data",
    "href": "posts/2022-03-17-mortgage.html#get-some-data",
    "title": "Modeling out fixed closed vs capped variable mortgages",
    "section": "Get some data",
    "text": "Get some data\nTo do any scenario planning it will be helpful to have some historical data on the relevant interest rates. Specifically I want to know 5 year fixed mortgage rates, and the big bank prime rate, as that forms the basis of variable rate mortgages. At the time of this writing RBC is offering variable mortgages at “RBC Prime Rate - 0.750%” but they’re also offering a “special rate” of 3.34% compared to their posted rate of 4.79% for 5 year fixed. The delta there is 1.45%. Their posted variable rate is just their posted prime. I’m less interested in the exact rates than I am the delta. It looks like right now the delta of their offered rates between fixed and closed (3.34 - 1.95 = 1.39) is less than their posted fixed and closed rate (4.79 - 2.70 = 2.09). Because the data set I can work with only has posted rates and I have no way of estimating the “true” delta in historical periods I’ll use it as the basis for this analysis. However, it will be important to remember that at least in this current low interest rate environment there’s less of a gap between fixed and variable rates than my data will suggest. Depending on how close these scenarios play out I can re-run them by adding 2.09 - 1.39 = 0.7 onto my prime rate to simulate that narrower gap.\n\nfrom collections import defaultdict\nimport pandas as pd\nimport requests\n\nFirst we grab some historical rate data in JSON from the Bank of Canada website. I tried to grab it as a CSV but it’s terribly formatted, so we’ll go with this and clean it up.\n\nrates_json = requests.get(\"https://www.bankofcanada.ca/valet/observations/group/chartered_bank_interest/json\").json()\n\nNext I have to clean it up into a dataframe for analysis.\n\nrenamer = {k: f\"{v['description']} {v['label']}\".strip() for k, v in rates_json[\"seriesDetail\"].items()}\n\n\nbase_df = (\n    pd.DataFrame(rates_json[\"observations\"])\n    # convert date series from string to datetime\n    .assign(d=lambda df: pd.to_datetime(df[\"d\"]))\n    .set_index(\"d\")\n    # Get the actual rate value out of the dictionary it's in\n    .applymap(lambda x: float(x[\"v\"]), na_action='ignore')\n    .rename(columns=renamer)\n)\n\nLet’s take a quick look at it to make sure it’s sensible.\n\nbase_df.tail()\n\n\n\n\n\n  \n    \n      \n      Non-Chequable Savings Deposits\n      5-year personal fixed term\n      Prime rate\n      Conventional mortgage 5-year\n      Conventional mortgage 1-year\n      Conventional mortgage 3-year\n      Guaranteed investment certificates 5-year\n      Guaranteed investment certificates 1-year\n      Guaranteed investment certificates 3-year\n      Daily Interest Savings (balances over $100,000)\n    \n    \n      d\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2022-02-16\n      0.01\n      1.15\n      2.45\n      4.79\n      2.79\n      3.49\n      1.65\n      0.60\n      1.25\n      0.01\n    \n    \n      2022-02-23\n      0.01\n      1.45\n      2.45\n      4.79\n      2.79\n      3.49\n      1.75\n      0.60\n      1.35\n      0.01\n    \n    \n      2022-03-02\n      0.01\n      1.45\n      2.45\n      4.79\n      2.79\n      3.49\n      1.75\n      0.60\n      1.35\n      0.01\n    \n    \n      2022-03-09\n      0.01\n      1.55\n      2.70\n      4.79\n      2.79\n      3.49\n      1.85\n      0.65\n      1.28\n      0.01\n    \n    \n      2022-03-16\n      0.01\n      1.55\n      2.70\n      4.79\n      2.79\n      3.49\n      1.85\n      0.78\n      1.30\n      0.01\n    \n  \n\n\n\n\nThat looks good to me. I’m just going to shrink it down to the just the columns I care about, resample it down to monthly frequency, and give them easier to work with names.\n\ndf = (\n    base_df\n    .rename(columns={\"Prime rate\": \"prime\", \"Conventional mortgage 5-year\": \"fiveyear\"})\n    .reindex(columns=[\"prime\", \"fiveyear\"])\n    # Most early dates don't have values for these series\n    .dropna(how=\"all\")\n    .reset_index()\n    # Group it to the first entry for each month\n    .groupby(pd.Grouper(key=\"d\", freq=\"M\", convention=\"start\"))\n    .first()\n    # Make it the start rather than end of the month to make date deltas easier\n    .reset_index()\n    .assign(d=lambda df: df[\"d\"].dt.to_period('M').dt.to_timestamp())\n    .set_index(\"d\")\n)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      prime\n      fiveyear\n    \n    \n      d\n      \n      \n    \n  \n  \n    \n      1975-01-01\n      11.0\n      12.00\n    \n    \n      1975-02-01\n      9.5\n      11.50\n    \n    \n      1975-03-01\n      9.0\n      10.75\n    \n    \n      1975-04-01\n      9.0\n      10.50\n    \n    \n      1975-05-01\n      9.0\n      10.75"
  },
  {
    "objectID": "posts/2022-03-17-mortgage.html#some-quick-exploration",
    "href": "posts/2022-03-17-mortgage.html#some-quick-exploration",
    "title": "Modeling out fixed closed vs capped variable mortgages",
    "section": "Some quick exploration",
    "text": "Some quick exploration\nLet’s do some basic charting on this data just to get a sense of what we’re working with.\n\n# Would rather use altair but I have lots of data points and I'm not sure how using render server will play with my static site generator\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Make my charts a decent size\nsns.set(rc = {'figure.figsize':(15,8)})\n\n\n# Seaborn likes data in tidy format\ncdf = df.melt(ignore_index=False).reset_index()\n\n\nsns.lineplot(data=cdf, x=\"d\", y=\"value\", hue=\"variable\");\n\n\n\n\nHere’s what prime vs 5 year mortgage rates have looked like for the last 50ish years. The variable rate tends to be below the fixed at any given point in time, but of course the variable rate can go up over your mortgage term while a fixed rate will stay fixed. Let’s start the actual analysis."
  },
  {
    "objectID": "posts/2022-03-17-mortgage.html#check-variable-vs-5-year-term",
    "href": "posts/2022-03-17-mortgage.html#check-variable-vs-5-year-term",
    "title": "Modeling out fixed closed vs capped variable mortgages",
    "section": "Check variable vs 5 year term",
    "text": "Check variable vs 5 year term\nFor any starting period I can compare a 5 year fixed rate to what the variable rate would have been over that same period and plot the delta between them. Let’s see what that looks like:\n\n# Get all the dates that have at least 5 years of history ahead of them\nstart_dates = list(df.index[:-60])\n\n\ndelta_dict = defaultdict(dict)\nfor start_date in start_dates:\n    five_year_rate = df.loc[start_date, \"fiveyear\"]\n    for delta in range(60):\n        variable_rate = df.loc[start_date + pd.DateOffset(months=delta), \"prime\"]\n        variable_delta = variable_rate - five_year_rate\n        delta_dict[delta][start_date] = variable_delta\ndelta_df = pd.DataFrame(delta_dict).T\ndelta_df.head()\n\n\n\n\n\n  \n    \n      \n      1975-01-01\n      1975-02-01\n      1975-03-01\n      1975-04-01\n      1975-05-01\n      1975-06-01\n      1975-07-01\n      1975-08-01\n      1975-09-01\n      1975-10-01\n      ...\n      2016-06-01\n      2016-07-01\n      2016-08-01\n      2016-09-01\n      2016-10-01\n      2016-11-01\n      2016-12-01\n      2017-01-01\n      2017-02-01\n      2017-03-01\n    \n  \n  \n    \n      0\n      -1.0\n      -2.0\n      -1.75\n      -1.5\n      -1.75\n      -2.00\n      -2.00\n      -2.25\n      -2.50\n      -2.25\n      ...\n      -1.94\n      -2.04\n      -2.04\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n    \n    \n      1\n      -2.5\n      -2.5\n      -1.75\n      -1.5\n      -1.75\n      -2.00\n      -2.00\n      -2.25\n      -1.75\n      -2.25\n      ...\n      -1.94\n      -2.04\n      -2.04\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n    \n    \n      2\n      -3.0\n      -2.5\n      -1.75\n      -1.5\n      -1.75\n      -2.00\n      -2.00\n      -1.50\n      -1.75\n      -2.25\n      ...\n      -1.94\n      -2.04\n      -2.04\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n    \n    \n      3\n      -3.0\n      -2.5\n      -1.75\n      -1.5\n      -1.75\n      -2.00\n      -1.25\n      -1.50\n      -1.75\n      -2.25\n      ...\n      -1.94\n      -2.04\n      -2.04\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n    \n    \n      4\n      -3.0\n      -2.5\n      -1.75\n      -1.5\n      -1.75\n      -1.25\n      -1.25\n      -1.50\n      -1.75\n      -2.25\n      ...\n      -1.94\n      -2.04\n      -2.04\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n      -1.94\n    \n  \n\n5 rows × 507 columns\n\n\n\n\ndelta_cdf = (\n    delta_df\n    .melt(ignore_index=False)\n    .reset_index()\n    # change from a date to a count of months since the first period\n    # makes it easier to add in a colour scale and get a sense of time periods.\n    .assign(variable=lambda df: (df[\"variable\"] - df[\"variable\"].min()).dt.days)\n    .rename(columns={\"index\": \"period\", \"variable\": \"start date delta\", \"value\": \"variable_delta\"})\n)\n\n\nsns.lineplot(data=delta_cdf, x=\"period\", y=\"variable_delta\", hue=\"start date delta\");\n\n\n\n\nThis chart shows the change in the delta between a fixed rate mortgage and the variable rate over a 5 year period. Each line represents a potential start month, with the delta between fixed and variable on the y axis and how many months into the 5 year term that mortgage is on the x axis. The colour corresponds to the number of days after Jan 1 1970 the scenario starts (I just needed a nice linear series to get the colour coding).\nThat’s way too noisy to make a ton of sense of, but it shows that variable rates can be quite a lot higher or lower than the 5 year fixed. Most of the crazy outliers are lighter in colour, which corresponds to earlier points in history and given the wildly volatile interest rates we saw in the previous chart that seems reasonable. Eyeballing it it definitely looks like the majority of series are below the 0 line throughout the 5 year period.\nOne could make the argument that including the 1980s is unfair given the crazy stagflation and associated high interest rates used to combat them. If you think more recent history is a better guide we can look at just the scenarios starting since the year 2000:\n\nsubset_cdf = delta_cdf.loc[lambda df: df[\"start date delta\"] >= 25 * 365]\nsns.lineplot(data=subset_cdf, x=\"period\", y=\"variable_delta\", hue=\"start date delta\");\n\n\n\n\nSo since 2000 it looks like your variable rate has almost never gone above the five year fixed rate. Again, recall that the actual fixed or variable rate you’d get at any of these points is not perfectly correlated with the posted rates I’m using, but it’s still a good indicator. Personally since I’m risk averse and we are currently looking at higher inflation than there’s been for 30 years I’m inclined to use the longer term scenarios, but it was an interesting exercise to see what more recent history looks like.\nNow, even though these charts are kind of pretty, they’re not very informative, there’s too much going on. What I can do instead is summarize the distribution of those lines. So for any point in the mortgage term I can show the mean delta, or the median, or any other quantile. Let’s take a look at that:\n\nfull_series = [delta_df.mean(axis=\"columns\").rename(\"mean\")]\nfor q in [0.05, 0.10, 0.25, 0.5, 0.75, 0.9, 0.95]:\n    full_series.append(delta_df.quantile(q, axis=\"columns\").rename(f\"q{q}\"))\nfull_distributions = pd.concat(full_series, axis=\"columns\")\n\n\nfdist_cdf = (\n    full_distributions\n    .melt(ignore_index=False)\n    .reset_index()\n    .rename(columns={\"index\": \"period\"})\n)\nsns.lineplot(data=fdist_cdf, x=\"period\", y=\"value\", hue=\"variable\");\n\n\n\n\nSo over the whole observation period the mean gap between fixed and variable rate tends to trend downward, which makes sense given interest rates were declining over most of the observed period. At least 3/4 of the time they stayed below the fixed rate for the entire period, but when we look at the 90th percentile and above they were over the fixed rate throughout the period (this is not saying any one series was consistently that high above throughout the 5 year period, just that at least 10% of them were that much above it at that point.) Let’s try it again, just focusing on since 2000.\n\nrdelta_df = delta_df.T.loc[\"2000\":].T\nfull_series = [rdelta_df.mean(axis=\"columns\").rename(\"mean\")]\nfor q in [0.01, 0.05, 0.10, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]:\n    full_series.append(rdelta_df.quantile(q, axis=\"columns\").rename(f\"q{q}\"))\nfull_distributions = pd.concat(full_series, axis=\"columns\")\nfdist_cdf = (\n    full_distributions\n    .melt(ignore_index=False)\n    .reset_index()\n    .rename(columns={\"index\": \"period\"})\n)\nsns.lineplot(data=fdist_cdf, x=\"period\", y=\"value\", hue=\"variable\");\n\n\n\n\nSo in the more recent period I checked out some of the wider end of the tails. At the 99th percentile we do see a couple spots where it goes above 0, but not by much. Variable rates over the last 20 years have been a pretty sweet deal."
  },
  {
    "objectID": "posts/2022-03-17-mortgage.html#model-out-capped-variable-strategy",
    "href": "posts/2022-03-17-mortgage.html#model-out-capped-variable-strategy",
    "title": "Modeling out fixed closed vs capped variable mortgages",
    "section": "Model out capped variable strategy",
    "text": "Model out capped variable strategy\nLooking at these trends, it definitely seems like going with the capped variable rate would be the way to go, but let’s model it out a bit and see. The first thing I need to do is write a class that can handle mortgage payments, including variable payments or a capped variable strategy. For that I can leverage some of the code I wrote for my rent or own calculator (with some tweaks).\n\nimport math\nfrom collections import OrderedDict\nimport numpy_financial as npf\nimport numpy as np\n\n\nclass Mortgage:\n    \"\"\"Base mortgage class.\n    Parameters\n    ----------\n    delta_series: pd.Series\n        The series of rate deltas over the 5 year term. Can pass in a series of 0s to simulate a fixed rate mortgage\n    capped_variable: bool\n        Whether to use the capped variable or regular variable payment strategy\n    principal: Int\n        Value of the mortgage, default $400K to simulate a $500K purchase with 20% down\n    amortize_years: int\n        Amortization period of the mortgage (not term of mortgage), default to standard 25 for a new purchase\n    fixed_rate: float\n        fixed 5 year APR rate as posted online, will use AER for actual calculations, default to 4.79 since that's the latest value\n        in the series at the time of this writing.\n    \"\"\"\n\n    def __init__(self, delta_series: pd.Series, scenario_name: str, capped_variable: bool = True, principal: int=400_000 , amortize_years: int = 25, fixed_rate: float = 4.79) -> None:\n        self.delta_series = delta_series\n        # So I can label dataframes or other returned objects\n        self.scenario_name = scenario_name\n        self.capped_variable = capped_variable\n        self.principal = principal\n        self.amortize_years = amortize_years\n        self.fixed_rate = fixed_rate\n        # Have to do different compounding period for variable (monthly) vs fixed (semi annual)\n        if delta_series.sum() == 0:\n            self.fixed = True\n        else:\n            self.fixed = False\n        self.fixed_pmt = self._calc_fixed_pmt()\n    \n    def periodic_rate(self, period: int) -> float:\n        \"\"\"Calculate the periodic interest rate from the posted APR.\n        \n        Parameters\n        ----------\n        period: int\n            Which period in the term we're in (used to identify the variable rate delta)\n        \n        Returns\n        -------\n        float: periodic interest rate\n        \"\"\"\n        apr = self.fixed_rate + self.delta_series[period]\n        apr_dec = apr / 100\n        if self.fixed:\n            compounds = 2\n        else:\n            compounds = 12\n        aer = (1 + (apr_dec / compounds)) ** compounds - 1\n        return (1 + aer) ** (1 / 12) - 1\n\n    def _calc_fixed_pmt(self) -> float:\n        \"\"\"Calculate what a fixed payment would be to compute capped variable payments.\"\"\"\n        apr_dec = self.fixed_rate / 100\n        compounds = 2 \n        aer = (1 + (apr_dec / compounds)) ** compounds - 1\n        periodic_rate = (1 + aer) ** (1 / 12) - 1\n        periods = self.amortize_years * 12\n        return -round(npf.pmt(periodic_rate, periods, self.principal), 2)\n        \n    def _amortizegen(self):\n        \"\"\"Yield a dictionary of a payment period.\n        \n        Have to do it incrementally because additional payments can mean recomputing things after every period.\n        \n        Parameters\n        ----------\n        period: int\n            Which period of the term we're in\n        \n        Yields\n        ------\n        Dict\n            All the data for another period of mortgage paments\n        \"\"\"\n        period = 0\n        beg_balance = self.principal\n        end_balance = self.principal\n        # Careful here, have to control how often this is called outside the function\n        while period < 60:\n            periodic_rate = self.periodic_rate(period)\n            amortization_left = (self.amortize_years * 12) - period\n            interest = -npf.ipmt(rate=periodic_rate, per=1, nper=amortization_left, pv=beg_balance)\n            if self.capped_variable:\n                if interest >= self.fixed_pmt:\n                    principal = 0\n                else:\n                    principal = self.fixed_pmt - interest\n            else:\n                principal = -npf.ppmt(rate=periodic_rate, per=1, nper=amortization_left, pv=beg_balance)\n            total_pmt = interest + principal\n            end_balance = beg_balance - principal\n            yield OrderedDict(\n                [\n                    (\"Period\", period),\n                    (\"Begin_balance\", beg_balance),\n                    (\"Payment\", total_pmt),\n                    (\"Principal\", principal),\n                    (\"Interest\", interest),\n                    (\"End_balance\", end_balance),\n                ]\n            )\n            # increment the counter, balance and date\n            period += 1\n            beg_balance = end_balance\n\n    def amortize(self) -> pd.DataFrame:\n        \"\"\"Show payments on the mortgage.\n        \n        Returns\n        -------\n        pd.DataFrame\n            Dataframe of mortgage payments showing principal and interest contributions and amount outstanding\n        \"\"\"\n        df = (\n            pd.DataFrame(self._amortizegen())\n            .set_index(\"Period\")\n        )\n        return df\n    \n    def summarize(self) -> dict:\n        \"\"\"Summarize the 5 year term.\"\"\"\n        df = self.amortize()\n        return {\n            \"scenario_name\": self.scenario_name,\n            \"end_balance\": df.iloc[-1][\"End_balance\"],\n            \"total_payments\": df[\"Payment\"].sum(),\n            \"average_payment\": df[\"Payment\"].mean(),\n            \"min_payment\": df[\"Payment\"].min(),\n            \"max_payment\": df[\"Payment\"].max(),\n            \"total_interest\": df[\"Interest\"].sum(),\n            \"total_principal\": df[\"Principal\"].sum(),\n        }\n\n    def pretty_summary(self) -> None:\n        \"\"\"Print out the summary in pretty format.\"\"\"\n        summary = self.summarize()\n        result = f\"\"\"\n            Scenario: {summary[\"scenario_name\"]}\n            Capped: {self.capped_variable}\n            Total Payments: ${summary[\"total_payments\"]:,.0f}\n            Ending mortgage balance: ${summary[\"end_balance\"]:,.0f}\n            Total interest payments: ${summary[\"total_interest\"]:,.0f}\n            Total principal payments: ${summary[\"total_principal\"]:,.0f}\n            Average payment: ${summary[\"average_payment\"]:,.0f}\n            Minimum payment: ${summary[\"min_payment\"]:,.0f}\n            Maximum payment: ${summary[\"max_payment\"]:,.0f}\n        \"\"\"\n        print(result)\n\nLet’s look at the summary of a few of these scenarios\n\nfixed_scenario = Mortgage(\n    delta_series=np.zeros(60),\n    scenario_name=\"fixed payments\",\n    capped_variable=False\n)\nfixed_summary = fixed_scenario.summarize()\nfixed_scenario.pretty_summary() \n\n\n            Scenario: fixed payments\n            Capped: False\n            Total Payments: $136,730\n            Ending mortgage balance: $352,851\n            Total interest payments: $89,581\n            Total principal payments: $47,149\n            Average payment: $2,279\n            Minimum payment: $2,279\n            Maximum payment: $2,279\n        \n\n\n\nMortgage(\n    delta_series=delta_df[\"1975-01-01\"],\n    scenario_name=\"1975-01-01\",\n    capped_variable=False\n).pretty_summary() \n\n\n            Scenario: 1975-01-01\n            Capped: False\n            Total Payments: $111,026\n            Ending mortgage balance: $340,142\n            Total interest payments: $51,168\n            Total principal payments: $59,858\n            Average payment: $1,850\n            Minimum payment: $1,536\n            Maximum payment: $2,801\n        \n\n\n\nMortgage(\n    delta_series=delta_df[\"1975-01-01\"],\n    scenario_name=\"1975-01-01 capped\",\n    capped_variable=True\n).pretty_summary() \n\n\n            Scenario: 1975-01-01 capped\n            Capped: True\n            Total Payments: $136,730\n            Ending mortgage balance: $311,809\n            Total interest payments: $48,539\n            Total principal payments: $88,191\n            Average payment: $2,279\n            Minimum payment: $2,279\n            Maximum payment: $2,279\n        \n\n\nBefore I run a bunch of scenarios from historical ones, let’s plug in something derived from a forecast. TD forecasts the overnight rate for the BoC will end 2022 at 1.25% (it’s 0.5% as I write this) and climb to 1.75% by the end of 2023 before leveling off for the remainder of the forecast period. Let’s make a scenario where variable rates follow that same hike schedule 1:1\n\n# Current delta based on \"special\" rates available on RBC site\nstart_delta = -1.39\ndelta_series = np.repeat(start_delta, 60)\n# let's do 3 0.25 rate hikes over the next 9 months\nfor cut in [3, 6, 9]:\n    delta_series[cut:] += 0.25\n#Then we need two hikes in 2023\nfor cut in [12, 18]:\n    delta_series[cut:] += 0.25\n\n\nMortgage(\n    delta_series=delta_series,\n    scenario_name=\"1.25% total hikes, capped\",\n    capped_variable=True\n).pretty_summary() \n\n\n            Scenario: 1.25% total hikes, capped\n            Capped: True\n            Total Payments: $136,730\n            Ending mortgage balance: $346,032\n            Total interest payments: $82,762\n            Total principal payments: $53,968\n            Average payment: $2,279\n            Minimum payment: $2,279\n            Maximum payment: $2,279\n        \n\n\n\nMortgage(\n    delta_series=delta_series,\n    scenario_name=\"1.25% total hikes, uncapped\",\n    capped_variable=False\n).pretty_summary() \n\n\n            Scenario: 1.25% total hikes, uncapped\n            Capped: False\n            Total Payments: $132,412\n            Ending mortgage balance: $351,139\n            Total interest payments: $83,552\n            Total principal payments: $48,861\n            Average payment: $2,207\n            Minimum payment: $1,981\n            Maximum payment: $2,250\n        \n\n\nSo in this scenario (which I’m definitely not saying will happen, but it’s plausible) either of the variable strategies beat a fixed strategy since we never get up to the fixed rate. Between capped and full variable you make bigger payments and also pay less interest, so as long as those payments are manageable that seems like a pretty solid deal. Let’s run this against a bunch of historical scenarios now.\nFirst we’ll look at the distribution of outcomes for the capped strategy based on historical rate delta development\n\ncapped_scenarios = pd.DataFrame([\n    Mortgage(\n        delta_series=delta_df[col],\n        scenario_name=\"1975-01-01 capped\",\n        capped_variable=True\n    ).summarize()\n    for col in delta_df.columns\n])\n\n/home/ipreston/miniconda3/envs/mortgage/lib/python3.10/site-packages/numpy_financial/_financial.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  (1 + rate*when)*(temp - 1)/rate)\n/home/ipreston/miniconda3/envs/mortgage/lib/python3.10/site-packages/numpy_financial/_financial.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  (1 + rate*when)*(temp - 1)/rate)\n\n\n\ndef plot_hist(data, col):\n    g = sns.histplot(data=data,x=col, bins=50, stat=\"percent\")\n    plt.axvline(x=fixed_summary[col], color=\"red\")\n    return g\n\nThis chart shows the distribution of end balances (the amount still owing on the mortgage at the end of the 5 year term) using the capped variable strategy across all the historical scenarios we outlined above. The red vertical line is the equivalent point for going with a fixed rate. The vast majority of points show a lower ending balance, which is definitely good, although there is some tail risk of ending with a balance of up to about $50k more.\n\nplot_hist(capped_scenarios, \"end_balance\");\n\n\n\n\nSimilarly, we can look at the distribution of maximum payments made over the whole 5 year term. As long as the cap at least covers your interest payments you never go over it, so unsurprisingly over 70% of the scenarios have a max payment equal to the fixed amount. Again though, there’s a long tail (presumably corresponding to the massive rate hikes we saw in the late 70s and early 80s where you would end up making at least one payment that’s more than double the fixed amount, ouch.\n\nplot_hist(capped_scenarios, \"max_payment\");\n\n\n\n\nFinally, let’s look at the same scenarios, but just paying the variable rate the whole way through. The graphs represent the same things as above, just with the uncapped variable rate mortgage.\n\nuncapped_scenarios = pd.DataFrame([\n    Mortgage(\n        delta_series=delta_df[col],\n        scenario_name=\"1975-01-01 capped\",\n        capped_variable=False\n    ).summarize()\n    for col in delta_df.columns\n])\n\n/home/ipreston/miniconda3/envs/mortgage/lib/python3.10/site-packages/numpy_financial/_financial.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  (1 + rate*when)*(temp - 1)/rate)\n/home/ipreston/miniconda3/envs/mortgage/lib/python3.10/site-packages/numpy_financial/_financial.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  (1 + rate*when)*(temp - 1)/rate)\n\n\n\nplot_hist(uncapped_scenarios, \"end_balance\");\n\n\n\n\n\nplot_hist(uncapped_scenarios, \"max_payment\");\n\n\n\n\nThe distribution of end balances looks reasonably similar, although the capped variable definitely has more weight on the lower end of the distribution, which is quite good. Unsurprisingly, the max payment is a much wider distribution than under the capped scenario, with a decent chunk of payments above what you’d do in the fixed rate scenario."
  },
  {
    "objectID": "posts/2022-03-17-mortgage.html#conclusion",
    "href": "posts/2022-03-17-mortgage.html#conclusion",
    "title": "Modeling out fixed closed vs capped variable mortgages",
    "section": "Conclusion",
    "text": "Conclusion\nI’m not sure I want to put a detailed conclusion here, that could sound like giving investment advice. Please keep in mind, that I’m not a mortgage/real estate/investment professional. I’m some guy on the internet who likes making charts and wants to buy a house soon. I found this process informative for my decision making, I think I’ll leave it at that."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html",
    "href": "posts/2020-02-15-windows-ds-software.html",
    "title": "Setting up for data science in python on Windows",
    "section": "",
    "text": "There are lots of great guides for setting up an environment to do data science. For my purposes though they generally lack two things:\nThis guide is intended to be useful for anyone trying to get set up for data science in python on windows. I think most of the steps are fairly generic, and I’ll make an effort to highlight the parts that are more opinionated. The sections below will go over the core software that will need to be installed, and some handy customizations.\nedit 2020-02-17 - I realized I wanted make as well, so I added a section on that. Also made note of an issue with launching interactive python from git bash, as well as solutions.\nedit 2020-08-04 - I started using pylance for a python language server, discovered SQL formatter, and switched to the regular Vim plugin.\nedit 2020-10-28 - Add instructions for clearing out an old install, do everything as a user level install. I cleaned up some other instructions as I went through as well.\nedit 2020-11-02 - Remove reference to pyenv-win. It’s too much of a hassle. Unfortunately my best advice there is to get access to a *NIX environment somehow."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#app-installation",
    "href": "posts/2020-02-15-windows-ds-software.html#app-installation",
    "title": "Setting up for data science in python on Windows",
    "section": "App installation",
    "text": "App installation\nHead to the VS code download page and pick the Win64 user installer.\nGo through the install process, I think all the defaults are fine, so just keep hitting next. You can optionally add in things like “Add open with code action to Windows Explorer”. It won’t directly impact the rest of what we’re doing, just integrates code with the rest of your desktop a little more fully.\nYou can sign in with either a GitHub or Microsoft account to enable setting sync. If you use VS code on multiple machines, or just want to easily restore all your settings if you get a new computer I recommend enabling it. See the person icon in the lower left corner.\n\n\n\n\n\nsettings sync\n\n\nOn the work machine I’m testing this guide on I’m getting a settings sync error, even though the same install is syncing on my home machine. I love computers. Oh well, it means I’ll get to really walk through a fresh install for this guide."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#extensions",
    "href": "posts/2020-02-15-windows-ds-software.html#extensions",
    "title": "Setting up for data science in python on Windows",
    "section": "Extensions",
    "text": "Extensions\nWhat makes VS code so great is all its extensions. Here are some that are great:\n\nBracket Pair Colorizer 2 colour codes your brackets. Super helpful for debugging.\nDracula Official is a nice dark theme. The Material themes are also nice.\nGitLens integrates a lot of git functionality into VS code, things like showing who made what changes inline in your code.\nindent-rainbow colour codes your indentation level, similar to the brackets\nmarkdownlint gives style suggestions when writing markdown files (like this guide!)\nMaterial icon theme makes the icons in the file explorer a little nicer\nPython is pretty core for this for obvious reasons\nPylance is a new language server for Visual Studio, it offers nicer autocomplete and seems worth using.\nBetter TOML handles the config format of python packages\nThe Remote Development extension pack lets you run VS code on a local machine while developing on a remote system, Docker container, or WSL install.\nSQL Formatter will clean up any sql queries you write\nSQL Server (mssql) is very handy if you interact with SQL server. The first time you open this it will run a bunch of installers in the background.\nVim - Only install this if you know what Vim is and you want to use its keybindings. If you do this extension will make you very happy, if you don’t it will make you very sad.\nEditorconfig - Lets VS Code parse editorconfig files."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#finish-up-later",
    "href": "posts/2020-02-15-windows-ds-software.html#finish-up-later",
    "title": "Setting up for data science in python on Windows",
    "section": "Finish up later",
    "text": "Finish up later\nThere’s more configuration to do in VS code, but prior to that let’s set up the actual applications we’ll use with it."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#customizing-git",
    "href": "posts/2020-02-15-windows-ds-software.html#customizing-git",
    "title": "Setting up for data science in python on Windows",
    "section": "Customizing git",
    "text": "Customizing git\nThere are a couple handy things that are useful to customize about git. For one, VS Code is going to make a .vscode folder anywhere we open a folder with it. We’re never going to want to commit that file to version control, so we’ll create a global gitignore file (as opposed to the more standard repository specific ones) and exclude that file. I got this idea from this blog, so credit there. In your %UserProfile%\\.config\\git folder, create a file named ignore. Mine is quite basic at this point:\n*.vscode\nIf you find your setup generates other files you’d like to ignore put them here, but don’t use this file for language specific stuff like .ipynb-checkpoints, leave that to project specific .gitignore files.\nThe other thing that’s nice to do is clean up your default prompt. Notably, git bash by default includes MINGW64 in the prompt. I guess this is the $MSYSTEM$ environment variable, but I can’t imagine why I’d care to see that in my prompt. The other stuff it includes by default are pretty handy, but if you don’t like them you can modify the same file I’m going to point to to update your setup.\nThe file that contains your prompt information should be in either %UserProfile%\\AppData\\Local\\Programs\\Git\\etc\\profile.d\\git-prompt.sh. This is a standard bash script, so if you’re familiar with bash scripting, or modifying your bashrc in Linux or Mac this will be familiar. If not, it’s generally pretty readable. Make a backup of it and fiddle. To get rid of the MINGW64 we just have to find the lines that say\nPS1=\"$PS1\"'\\[\\033[35m\\]'       # change to purple\nPS1=\"$PS1\"'$MSYSTEM '          # show MSYSTEM\nand delete them or comment them out. I also got rid of\nPS1=\"$PS1\"'\\n'                 # new line\nso that my conda environment would be on the same line as the rest of my setup info."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#set-up-ssh",
    "href": "posts/2020-02-15-windows-ds-software.html#set-up-ssh",
    "title": "Setting up for data science in python on Windows",
    "section": "Set up ssh",
    "text": "Set up ssh\nThis will actually work exactly the same as Linux, which is nice. GitHub has nice docs on how to generate keys and associate them with your GitHub account, so I won’t reiterate that here. Anything you can connect to via SSH rather than password you should though. It’s more secure, and more convenient.\n\nSSH note\nI had an old install of putty when I first set up git bash. Even though I told it to use OpenSSH I guess I still had putty set somewhere in my environment. I had to modify the GIT_SSH environment variable for my system to point to the git ssh utility, which in my case was at C:\\Program Files\\Git\\usr\\bin\\ssh. Most people shouldn’t have to do this."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#additional-utilities",
    "href": "posts/2020-02-15-windows-ds-software.html#additional-utilities",
    "title": "Setting up for data science in python on Windows",
    "section": "Additional utilities",
    "text": "Additional utilities\nGit bash has decent functionality out of the box, but there may be additional utilities you want to install. In my particular case, I’d like to be able to use make in my projects. Thanks to this gist I found that it’s pretty easy to do. I’ll reproduce the make install instructions here, but all credit for this part goes to the original author.\nWherever you installed git bash there should be a mingw64 folder. My home machine did a system install, so I found it in C:\\Program Files\\Git\\mingw64, but my work one was a user level install, so that one ended up in %UserProfile%\\AppData\\Local\\Programs\\Git\\mingw64. You can always find where it is by right clicking the shortcut to git bash in your start menu and hitting properties, that will show you the path.\n\nKeep in mind you can easy add make, but it doesn’t come packaged with all the standard UNIX build toolchain–so you will have to ensure those are installed and on your PATH, or you will encounter endless error messages.\n\n\nGo to ezwinports.\nDownload make-4.1-2-without-guile-w32-bin.zip (get the version without guile).\nExtract zip.\nCopy the contents to your Git\\mingw64\\ merging the folders, but do NOT overwrite/replace any existing files.\n\nThat was all I had to do to make the basic makefiles that I wanted to use. As noted above, if you want to actually build c packages or something your process will likely be more complex. For a great beginner friendly intro to makefiles in the context of python projects, check out calm code."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#further-reading",
    "href": "posts/2020-02-15-windows-ds-software.html#further-reading",
    "title": "Setting up for data science in python on Windows",
    "section": "Further reading",
    "text": "Further reading\nThe main git page has tons of resources. I’ve also collected a few that I found useful under my Tagpacker page."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#actually-building-environments",
    "href": "posts/2020-02-15-windows-ds-software.html#actually-building-environments",
    "title": "Setting up for data science in python on Windows",
    "section": "Actually building environments",
    "text": "Actually building environments\nConda environment management is a big separate topic. Their documentation is really good, and I refer to it regularly."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#user-settings",
    "href": "posts/2020-02-15-windows-ds-software.html#user-settings",
    "title": "Setting up for data science in python on Windows",
    "section": "User settings",
    "text": "User settings\nVS code comes mostly with sensible defaults, but there are a few things I like to change. In VS code hit F1 and type Open settings (JSON). If you don’t see that option (it didn’t pop up for me the first time I tried it) just open settings and look for a setting that tells you to update it in settings.json. Below are my settings, minus the stuff that just got added through configuring the extensions described above:\n{\n    \"diffEditor.renderSideBySide\": true,\n    \"workbench.editor.enablePreviewFromQuickOpen\": false,\n    \"workbench.editor.enablePreview\": false,\n    \"workbench.colorTheme\": \"Dracula\",\n    \"workbench.iconTheme\": \"material-icon-theme\",\n    \"editor.rulers\": [88],\n    \"editor.suggestSelection\": \"first\",\n    \"editor.acceptSuggestionOnEnter\": \"off\",\n    \"editor.minimap.enabled\": false,\n    \"editor.lineNumbers\": \"relative\",\n    \"explorer.confirmDelete\": false,\n    \"editor.wordWrap\": \"on\",\n    \"python.formatting.provider\": \"black\",\n    \"python.linting.enabled\": true,\n    \"python.linting.flake8Enabled\": true,\n    \"python.languageServer\": \"Pylance\",\n    \"git.confirmSync\": false,\n    \"git.autofetch\": true,\n    \"editor.codeActionsOnSave\": null,\n    \"search.exclude\": {\n        \"**/node_modules\": true,\n        \"**/bower_components\": true,\n        \"**/env\": true,\n        \"**/venv\": true\n    },\n    \"files.watcherExclude\": {\n        \"**/.ipynb_checkpoints/**\": true,\n        \"**/$tf/**\": true,\n        \"**/.git/objects/**\": true,\n        \"**/.git/subtree-cache/**\": true,\n        \"**/node_modules/**\": true,\n        \"**/env/**\": true,\n        \"**/venv/**\": true,\n        \"**/.hypothesis/**\": true,\n    },\n    \"files.exclude\": {\n        \"*.sublime-*\": true,\n        \"**/__pycache__\": true,\n        \"**/.DS_Store\": true,\n        \"**/.git\": true,\n        \"**/.hypothesis/**\": true,\n        \"**/.ipynb_checkpoints\": true,\n        \"**/.pytest_cache\": true,\n        \"**/.vscode\": true,\n        \"**/*.log\": true,\n        \"**/*.lst\": true,\n        \"**/$tf\": true,\n        \"**/node_modules\": true,\n        \"venv\": true\n    }\n}\nMost of the settings have fairly clear names, and if you put them in your settings.json you’ll get a little mouseover tip that will tell you what they do.\nWhile you can hand code in the option to have VS code use git bash it’s probably easier to hit F1 again and select “Terminal: Select Default Shell” and choose it there. That will add a line to your settings that tells VS code to use git bash."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#further-resources",
    "href": "posts/2020-02-15-windows-ds-software.html#further-resources",
    "title": "Setting up for data science in python on Windows",
    "section": "Further Resources",
    "text": "Further Resources\nThere’s tons of stuff to learn about VS code to make it super handy. At a minimum, check out their keyboard shortcuts. I’m collecting other useful resources (with a decent amount of overlap with vim stuff) on my tagpacker."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#uninstall-apps",
    "href": "posts/2020-02-15-windows-ds-software.html#uninstall-apps",
    "title": "Setting up for data science in python on Windows",
    "section": "Uninstall Apps",
    "text": "Uninstall Apps\nEven if you installed a program as a user level install, you’ll still be prompted for an Administrator password if you try and remove it from “Add/Remove Programs” in the control panel because… Windows. Running the actual uninstall exe file works fine though.\nFor git this should be located at %UserProfile%\\AppData\\Local\\Programs\\Git\\unins000.exe. The uninstaller won’t remove that folder itself so delete it after you’ve run it.\nVS code is in a similar location and follows a similar process %UserProfile%\\AppData\\Local\\Programs\\Microsoft VS Code\\unins000.exe.\nMiniconda as a user level install is in a slightly different place: %UserProfile%\\Miniconda3\\Uninstall-Miniconda3.exe You’ll be prompted for an admin password a couple times, but if you click “no” it still seems to work, which is cool I guess. It will prompt you to reboot and when it comes back up you should be clear."
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#remove-miscellaneous-crud",
    "href": "posts/2020-02-15-windows-ds-software.html#remove-miscellaneous-crud",
    "title": "Setting up for data science in python on Windows",
    "section": "Remove miscellaneous crud",
    "text": "Remove miscellaneous crud\nDepending on how long you’ve been using your machine and how many different installs you’ve done you may or may not have these files. Here’s a list of the things I checked for and then removed from my machine:\n\nOrphan start menu entries in %UserProfile\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\nRenmants in C:\\ProgramData. In my case I had a folder for jupyter"
  },
  {
    "objectID": "posts/2020-02-15-windows-ds-software.html#remove-config-files",
    "href": "posts/2020-02-15-windows-ds-software.html#remove-config-files",
    "title": "Setting up for data science in python on Windows",
    "section": "Remove config files",
    "text": "Remove config files\nIf you like how your configuration is set up there’s no major harm in keeping these things. I’m just removing them to make sure I have a fair comparison to how a fresh install would go. All paths in the list of stuff I’ve removed are relative to %UserProfile%.\n\n.conda\n.config/configstore\n.continuum\n.cookiecutter_replay\n.cookiecutters\n.ipynb_checkpoints\n.ipython\n.jupyter\n.matplotlib\n.poetry\n.software\n.vscode\nAppData\nAppData\nAppData\nAppData\nAppData-data\nAppData\nAppData\nAppData\nAppDataTools\nAppData\nAppData\nAppData\nAppData\nAppData\nAppData\nAppData\n.bash_history\n.bash_profile\n.flake8\n.gitconfig\n.git-credentials\n.gitignore_global\n.python_history\n\nAs you can see, managing a python development environment creates a lot of crud on your system."
  },
  {
    "objectID": "posts/2021-02-26-ferede.html",
    "href": "posts/2021-02-26-ferede.html",
    "title": "Alberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue",
    "section": "",
    "text": "This is my replication of the empirical results, tables, and figures produced in a paper by Dr. Ergete Ferede, published by the University of Calgary school of public policy in Volume 11:24, September 2018.\nThe original paper is here: https://www.policyschool.ca/wp-content/uploads/2018/09/NRR-Ferede.pdf\nI chose this paper to reproduce for two reasons. The first is pragmatic; the data it uses is all publicly available, so I actually can. The second is that it describes a topic of importance in the province of Alberta, where I live.\nYou can read the details of what the paper sets out to show in the paper itself, but in brief the idea is to show that provincial government spending increases in the year following an increase in non-renewable resource revenue, but it does not decrease accordingly in the year following declines in the same revenue source. This has a ratcheting effect on public finance that is a contributor to the “royalty rollercoaster” that is Alberta’s public finance.\nIn the following sections I’ll go through the code necessary to extract and transform the data set used in the paper, as well as reproduce its key empirical results. Since most economists don’t use python, and they make up a key part of my intended audience for this, I’ll be adding comments to my code that explicitly describe what some of the functions and methods I’m calling do.\nI’m including all of the code necessary to produce this reproduction, since that’s a big part of why I’m doing this exercise, but if you’re just interested in seeing how my reproduced results compare to the original paper you can skip all the code blocks. You can find the code for this notebook on my github\nA surprising result of this reproduction is that I’ve identified a single data point error in the original paper that negates its results. Read on to find out what the error was and the impact it had on the results."
  },
  {
    "objectID": "posts/2021-02-26-ferede.html#setup-and-data-acquisition",
    "href": "posts/2021-02-26-ferede.html#setup-and-data-acquisition",
    "title": "Alberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue",
    "section": "Setup and data acquisition",
    "text": "Setup and data acquisition\nThis section of the code loads required modules, downloads the required data sets, and reads them into DataFrames.\n\nimport datetime as dt\nfrom itertools import chain\nfrom pathlib import Path\n\nimport altair as alt\nfrom arch.unitroot import DFGLS, ADF, PhillipsPerron\nfrom IPython.display import Image\nimport pandas as pd\nimport pandas_datareader as pdr\nimport requests\nimport seaborn as sns\nimport stats_can\nimport statsmodels\nfrom statsmodels.tsa.api import VAR\n\n\n%matplotlib inline\nalt.renderers.enable(\"jupyterlab\");\n\nWe start by loading the required libraries that will be used to support the analysis. For reference here are links to the libraries that are being used:\n\nPathlib\ndatetime\nrequests\npandas\npandas_datareader\nnumpy\nstats_can\naltair\nseaborn\narch\nstatsmodels\nmatplotlib\n\n\nHistorical budget data\nFunctions in this section are concerned with acquiring historical Alberta budget data and reading it into a DataFrame\n\n\nCode\ndef download_budget_data() -> Path:\n    \"\"\"Download the excel file for the analysis from the policy school page.\n\n    Note the readme sheet on the first file. Credit to Kneebone and Wilkins for\n    assembling it, and policy school for hosting it.\n\n    Originally used this URL, but found it was missing some later heritage\n    contributions. After discussion with Dr. Kneebone an updated set has been provided\n    https://www.policyschool.ca/wp-content/uploads/2019/01/Provincial-Government-Budget-Data-January-2019FINAL-USE.xlsx\n\n    Returns\n    -------\n    pathlib.Path\n        A path object with the location and name of the data\n    \"\"\"\n    print('Downloading data set')\n\n    url = 'https://www.policyschool.ca/wp-content/uploads/2019/03/Provincial-Government-Budget-Data-March-2019.xlsx'\n    # send a request to the url for the file\n    response = requests.get(\n        url,\n        stream=True,\n        headers={'user-agent': None}\n    )\n    # create a path object for the file in the data folder above\n    # where this notebook is saved with the file named\n    # budget.xlsx for easy later access.\n    fname = Path('.').joinpath('data').joinpath('budgets.xlsx')\n    # write the response from the request to the file in the path specified above\n    with open (fname, 'wb') as outfile:\n        for chunk in response.iter_content(chunk_size=512):\n            if chunk: # filter out keep-alive new chunks\n                outfile.write(chunk)\n    # Return the location of the file so we can load it later easily\n    return fname\n\n\ndef get_budget_file(force_update: bool=False) -> Path:\n    \"\"\"Get the budget file, downloading if required.\n\n    Parameters\n    ----------\n    force_update: bool\n        Download the data file even if you already have it\n\n    Returns\n    -------\n    pathlib.Path\n        A path object with the location and name of the data\n    \"\"\"\n    # This is where we're expecting the file to be saved if it exists\n    fname = Path('.').joinpath('data').joinpath('budgets.xlsx')\n    if not fname.exists() or force_update:\n        download_budget_data()\n    return fname\n\n\ndef get_date_index(df: pd.DataFrame) -> pd.DatetimeIndex:\n    \"\"\"Helper function to turn budget year strings into datetimes.\n\n    The Fiscal year columns span across years, e.g. 1965-66. In order\n    to use all the date indexed functionality I want to convert them into\n    an actual datetime format. This function accomplishes that\n    \n    Parameters\n    ----------\n    df: pd.DataFrame\n        The budget dataframe with the fiscal year style columns\n    \n    Returns\n    -------\n    pd.DatetimeIndex\n        A datetime index showing January 1 of the beginning of each\n        fiscal year for each period.    \n    \"\"\"\n    date_index = pd.to_datetime(\n        df\n        .assign(year=lambda df: df['budget_yr'].str[0:4].astype(int))\n        .assign(month=1)\n        .assign(day=1)\n        [['year', 'month', 'day']]\n    )\n    return date_index\n\n\ndef read_ab_budget() -> pd.DataFrame:\n    \"\"\"Read Alberta budget data.\n\n    Downloads the data if necessary, reads it in and gives\n    the variables easier to work with names\n\n    Returns\n    -------\n    pd.DataFrame\n        Alberta's revenue and expenditure tables\n    \"\"\"\n    # Get the budget file, download if necessary  using functions\n    # defined above\n    fname = get_budget_file()\n    df = (\n        pd.read_excel(\n            fname,\n            sheet_name='Alberta',\n            # column titles are spaced over 3 rows\n            header=3,\n            # first column of data is B\n            index_col=1,\n            # there's a big footnote at the bottom we want to skip\n            skipfooter=21\n        )\n        # Because of the merged cells we get an empty first row\n        .loc[lambda x: x.index.notnull()]\n        # Not sure where the empty first column comes from but drop it\n        .drop(columns='Unnamed: 0')\n        .reset_index()\n        .rename(columns={\n            'index': 'budget_yr',\n            'Personal Income Tax': 'personal_income_tax',\n            'Corporation Income Tax': 'corporate_income_tax',\n            'Retail Sales Tax': 'retail_sales_tax',\n            'Federal Cash Transfers': 'federal_cash_transfers',\n            'Natural Resource Revenue': 'natural_resource_revenue',\n            'Other Own-Source Revenue': 'other_own_source_revenue',\n            'Total Revenue': 'total_revenue',\n            'Health': 'health_exp',\n            'Social Services': 'social_services_exp',\n            'Education': 'education_exp',\n            'Other Program Expenditures': 'other_program_exp',\n            'Total Program Expenditures': 'total_prog_exp',\n            'Debt Service': 'debt_service',\n            'Total  Expenditures': 'total_exp',\n            'Unnamed: 16': 'annual_deficit'\n        })\n        # Turn the fiscal year string into a datetime object\n        .assign(budget_dt=lambda df: get_date_index(df))\n        .set_index('budget_dt')\n    )\n    return df\n\n\ndef read_heritage() -> pd.DataFrame:\n    \"\"\"Read deposits to the heritage trust fund from a separate table.\n\n    The paper nets out contributions to the heritage trust fund when they are\n    made, so we have to read them in to be able to net them out of resource revenue.\n\n    They're stored in the same sheet of the workbook, just down below the big table we\n    read in with the function above.\n    \"\"\"\n    fname = get_budget_file()\n    df = (\n        pd.read_excel(\n            fname,\n            sheet_name='Alberta',\n            # Have to manually specify column names because of\n            # how the table is laid out\n            header=None,\n            usecols='D:G',\n            names=['budget_yr', 'resource_allocation', 'deposits', 'advance_edu'],\n            skiprows=71,\n            skipfooter=1\n        )\n        # more fiddly cleaning because of how the table is set up\n        # there's a blank row between 1986-87 and when\n        # contributions resume in 2005-06\n        .loc[lambda df: ~df['budget_yr'].isna()]\n        .set_index('budget_yr')\n        # missing entries have 0 contributions for that\n        # category in that year\n        .fillna(0)\n        # The three columns are all counted the same\n        # for the purposes of this analysis, they just have\n        # different labels/classifications depending on the year\n        .assign(total_heritage=lambda df: df.sum(axis='columns'))\n        # Add a dummy variable to indicate heritage fund deposit years\n        .assign(heritage_dummy=1)\n        .reset_index()\n        # convert the fiscal year column to a datetime index\n        .assign(budget_dt=lambda df: get_date_index(df))\n        .drop(columns='budget_yr')\n        .set_index('budget_dt')\n    )\n    return df\n\n\ndef clean_budget() -> pd.DataFrame:\n    \"\"\"Combine base budget with heritage deposits.\n\n    Pull all the logic together to create one dataframe with all the\n    fiscal data for the period of interest.\n\n    Returns\n    -------\n    pd.DataFrame\n        The full nominal budget data set.\n    \"\"\"\n    budg = read_ab_budget()\n    heritage = read_heritage()\n    budg_clean = (\n        # Start with the budget dataframe\n        budg\n        # consolidate some revenue categories\n        .assign(other_revenue=lambda df: df[['retail_sales_tax', 'federal_cash_transfers', 'other_own_source_revenue']].sum(axis='columns'))\n        # Just keep the columns we still need\n        .reindex(columns=['personal_income_tax', 'corporate_income_tax', 'natural_resource_revenue', 'other_revenue', 'total_prog_exp', 'debt_service'])\n        # add in the heritage contributions data\n        .merge(heritage[['total_heritage', 'heritage_dummy']], how='left', left_index=True, right_index=True)\n        # Set contributions and the heritage dummy to 0 for years where there were no contributions\n        .fillna(0)\n        # Net out heritage contributions from natural resources revenue\n        .assign(natural_resource_revenue_before_heritage=lambda df: df['natural_resource_revenue'])\n        .assign(natural_resource_revenue=lambda df: df['natural_resource_revenue'] - df['total_heritage'])\n        # consolidate revenue\n        .assign(total_revenue=lambda df: df[['personal_income_tax', 'corporate_income_tax', 'natural_resource_revenue', 'other_revenue']].sum(axis='columns'))\n        # consolidate expenditure\n        .assign(total_expenditure=lambda df: df[['total_prog_exp', 'debt_service']].sum(axis='columns'))\n        # calculate the deficit\n        .assign(deficit=lambda df: df['total_expenditure'] - df['total_revenue'])\n        # make all the budget numbers floating point\n        .astype('float64')\n    )\n    return budg_clean\n\n\n\n\nReal Per Capita budget\nAll of the analysis in the paper is done in terms of real per-capita data. Functions in this section transform the nominal total budget numbers acquired in the previous section into real per-capita figures.\n\n\nCode\ndef periodic_to_budget_annual(df: pd.DataFrame, index_name: str, year_periods: int = 4) -> pd.DataFrame:\n    \"\"\"Take a monthly or quarterly indexed dataframe and annualize it by budget period.\n\n    The inflation and population data we need to convert the budget into\n    real per-capita figures are monthly series. We need to get the average\n    population and price level for each fiscal year in the data set.\n\n    Rolling mean indexed on January year N+1 is the March to March\n    average population for fiscal year N\n    Applying a date offset of -1 year and taking only\n    January data of these rolling means gives us an average on the\n    same basis as the budget dates.\n\n    Parameters\n    ----------\n    df: pandas.DataFrame\n        DataFrame to be piped into this function\n    index_name: str\n        The name of the date index\n    year_periods: int, default 4\n        4 for quarterly data (population), 12 for monthly (inflation)\n\n    Returns\n    -------\n    pd.DataFrame\n        An annualized dataframe on a fiscal year basis for comparison\n        to annual budget figures.\n    \"\"\"\n    df = (\n        df\n        .copy()\n        .rolling(year_periods, closed='left')\n        .mean()\n        .reset_index()\n        .assign(budget_dt=lambda df: df[index_name] - pd.DateOffset(years=1))\n        .loc[lambda x: x['budget_dt'].dt.year >= 1965]\n        .loc[lambda x: x['budget_dt'].dt.month == 1]\n        .drop(columns=index_name)\n        .set_index('budget_dt')\n        .copy()\n    )\n    return df\n\n\ndef per_capita_data() -> pd.DataFrame:\n    \"\"\"Read in population data to calculate per capita estimates.\n\n    Quarterly population estimates for Alberta from Statistics Canada\n\n    Returns\n    -------\n    pd.DataFrame\n        Fiscal year annualized population estimates for Alberta over the\n        reference period.\n    \"\"\"\n    table = '17-10-0009-01'\n    df = (\n        stats_can.table_to_df(table, path='data')\n        .loc[lambda x: x['GEO'] == 'Alberta']\n        .loc[lambda x: x['REF_DATE'] >= '1965']\n        .set_index('REF_DATE')\n        [['VALUE']]\n        .rename(columns={'VALUE' : 'population'})\n        .pipe(periodic_to_budget_annual, 'REF_DATE', 4)\n    )\n    return df\n\n\ndef inflation_data() -> pd.DataFrame:\n    \"\"\"Read in inflation data to calculate real dollar estimates.\n\n    The whole series is scaled so 2017 budget year is = 1\n\n    Returns\n    -------\n    pd.DataFrame\n        Fiscal year annualized inflation data for Alberta over\n        the reference period. Normalized to 2017 = 1\n    \"\"\"\n    # Alberta inflation doesn't go back far enough, use Canada for earlier dates\n    vecs = ('v41692327', 'v41690973')\n    df = (\n        stats_can.vectors_to_df_local(vecs, path='data', start_date=dt.date(1965, 1, 1))\n        .rename(columns={'v41692327': 'ab_inflation', 'v41690973': 'ca_inflation'})\n    )\n    # fill in with Canadian inflation data where (early) Alberta inflation data is missing.\n    mask = df['ab_inflation'].isna()\n    # Could probably do some interpolation or scaling before this, but I looked\n    # at the raw series and they were pretty comparable\n    df.loc[mask, 'ab_inflation'] = df.loc[mask, 'ca_inflation']\n    df = (\n        df\n        .drop(columns='ca_inflation')\n        .pipe(periodic_to_budget_annual, 'REF_DATE', 12)\n    )\n    # Rescale to 2017 = 100 (this is fiscal year 2017,\n    # original may have done calendar year)\n    inf_2017 = float(df.loc['2017', 'ab_inflation'])\n    df = df / inf_2017\n    return df\n\n\ndef budget_real_per_capita() -> pd.DataFrame:\n    \"\"\"Get budget data in real per-capita terms.\n\n    Returns\n    -------\n    pd.DataFrame\n        Budget data in real per-capita terms.\n    \"\"\"\n    # Read in budget data using the function defined in the \n    # previous section\n    clean_budget_df = clean_budget()\n    # Everything except the dummy variable gets turned into\n    # real per-capita terms\n    scale_cols = clean_budget_df.columns.drop('heritage_dummy').tolist()\n    # Get population\n    per_capita = per_capita_data()\n    # Get inflation\n    inflation = inflation_data()\n    # Combine the datasets, can just use assign because they all\n    # have a datetime index\n    dfpc = (\n        clean_budget_df\n        .assign(pop=per_capita)\n        .assign(cpi=inflation)\n    )\n    # rescale to real per capita\n    dfpc[scale_cols] = (\n        dfpc[scale_cols]\n        # original data was in millions of dollars\n        .mul(1_000_000)\n        # divide by population and inflation for\n        # real per-capita\n        .div(dfpc['pop'], axis='index')\n        .div(dfpc['cpi'], axis='index')\n    )\n    return dfpc\n\n\n\n\nExogenous factors\nThe paper lists the Alberta employment rate, the Alberta unemployment rate, and the CAD/USD exchange rate as exogenous factors included in the model. Functions in this section acquire that data. I had to do some fiddling to get long enough historical series for some of the factors as you’ll note in the code. It’s hard to say for sure how the original author sourced this data. I’ll just have to compare my tables and charts to his to see if I got close enough.\n\n\nCode\ndef download_historical_cad_usd() -> pd.DataFrame:\n    \"\"\"Get exchange rates from before 1971.\n\n    FRED live data only goes back to 1971, I need a longer series\n    This was what I could find. It's annual only, so I can't do it on a budget\n    year basis, but hopefully it will be close enough\n\n    This whole function is just some gross munging to read in a table from a web page.\n    Once it's called we save it to the data folder so I don't have to re-call it every\n    time I run this notebook.\n    \"\"\"\n    url = 'https://fxtop.com/en/historical-exchange-rates.php?YA=1&C1=USD&C2=CAD&A=1&YYYY1=1953&MM1=01&DD1=01&YYYY2=2019&MM2=04&DD2=01&LANG=en'\n    df = pd.read_html(url)[29]\n    headers = df.iloc[0]\n    new_df  = (\n        pd.DataFrame(df.values[1:], columns=headers)\n        .rename(columns={'Year': 'year', 'Average USD/CAD': 'EXCAUS'})\n        .assign(month=1)\n        .assign(day=1)\n        .assign(budget_dt=lambda df: pd.to_datetime(df[['year', 'month', 'day']]))\n        .set_index('budget_dt')\n        .reindex(columns=['EXCAUS'])\n    )\n    new_df.to_csv('./data/early_cad_usd.csv')\n    return new_df\n\n\ndef read_historical_cad_usd(force_update: bool = False) -> pd.DataFrame:\n    \"\"\"Get exchange rates before 1971.\n\n    This wraps the above function to read in the downloaded data\n    if it's available and download and then read it if required.\n\n    Parameters\n    ----------\n    force_update: bool\n        Download the data set even if you already have it\n\n    Returns\n    -------\n    pd.DataFrame\n        Exchange rates from 1965 to 1971\n    \"\"\"\n    fname = Path('.').joinpath('data').joinpath('early_cad_usd.csv')\n    if not fname.exists() or force_update:\n        return download_historical_cad_usd()\n    else:\n        return pd.read_csv(fname).set_index('budget_dt')\n\n\ndef download_cad_usd() -> pd.DataFrame:\n    \"\"\"Download monthly exchange data from FRED.\n\n    For most of the period of interest I can get monthly\n    data from FRED, so I'll do that where possible.\n\n    Returns\n    -------\n    pd.DataFrame\n        Most of the CAD/USD exchange data I need for this analysis.\n    \"\"\"\n    df = pdr.get_data_fred('EXCAUS', start=dt.date(1970, 1, 1))\n    df.to_csv('./data/cad_usd.csv')\n    return df\n\n\ndef read_cad_usd(force_update=False):\n    \"\"\"Get monthly exchange data from FRED.\n\n    This wraps the above function to read in the downloaded data\n    if it's available and download and then read it if required.\n\n    Parameters\n    ----------\n    force_update: bool\n        Download the data set even if you already have it\n\n    Returns\n    -------\n    pd.DataFrame\n        Exchange rate data\n    \"\"\"\n    fname = Path('.').joinpath('data').joinpath('cad_usd.csv')\n    if not fname.exists() or force_update:\n        return download_cad_usd()\n    else:\n        return pd.read_csv(fname, parse_dates=['DATE']).set_index('DATE')\n\n\ndef annual_cad_usd() -> pd.DataFrame:\n    \"\"\"Full series of CAD/USD in fiscal year format.\n\n    Get FRED data and turn the monthly values into annualized on a budget\n    basis for as much as possible. Fill in the remainder with calendar annual\n    data from fxtop\n\n    Returns\n    -------\n    pd.DataFrame\n        Exchange data on an annualized basis.\n    \"\"\"\n    # Create a datetime index of all the points we need\n    annual_date_range = pd.date_range('1964-01-01', '2018-01-01', freq='AS', name='budget_dt')\n    # Get the old annual stuff to fill in later\n    old_df = read_historical_cad_usd()\n    df = (\n        # get the monthly series\n        read_cad_usd()\n        # annualize it\n        .pipe(periodic_to_budget_annual, 'DATE', 12)\n        # add in all the missing dates we need\n        .reindex(annual_date_range)\n        # fill those missing dates from the old annual data set.\n        .fillna(old_df)\n    )\n    return df\n\n\ndef stats_can_exog() -> pd.DataFrame:\n    \"\"\"Bring in exogenous StatsCan data. Employment and Unemployment rates.\n\n    Returns\n    -------\n    pd.DataFrame\n        Exogenous data required from StatsCan\n    \"\"\"\n    # Vectors for monthly series where available\n    ur_vec = \"v2064516\"\n    er_vec = \"v2064518\"\n    annual_date_range = pd.date_range('1964-01-01', '2018-01-01', freq='AS', name='budget_dt')\n    # for the earlier periods we only have annual data\n    old_df = (\n        stats_can.table_to_df('36-10-0345-01', path='data')\n        # Get Alberta data only\n        .loc[lambda x: x['GEO'] == 'Alberta']\n        # Keep only the categories we care about\n        .loc[lambda x: x['Economic indicators'].isin(['Population', 'Total employment', 'Unemployment rate'])]\n        # pivot so the year is the row and the variables are the columns\n        .pivot_table(index='REF_DATE', columns='Economic indicators', values='VALUE')\n        .rename(columns={'Unemployment rate': 'unemployment_rate'})\n        # calculate the employment rate\n        .assign(employment_rate=lambda x: (x['Total employment'] / x['Population']) * 100)\n        # drop the population, just used for calculating employment rate\n        .reindex(columns=['unemployment_rate', 'employment_rate'])\n        .rename_axis('budget_dt', axis='index')\n        .rename_axis(None, axis='columns')\n    )\n    # Get monthly data where available\n    df = (\n        stats_can.vectors_to_df_local([ur_vec, er_vec], path='data', start_date=dt.date(1964, 1, 1))\n        .rename(columns={ur_vec: 'unemployment_rate', er_vec: 'employment_rate'})\n        # annualize\n        .pipe(periodic_to_budget_annual, 'REF_DATE', 12)\n        # get the full range of data we want\n        .reindex(annual_date_range)\n        # fill in the gaps with the old annual series\n        .fillna(old_df)\n        # Not ideal but even the annual series doesn't go quite back\n        # far enough so we have to backfill the earliest available\n        # data point\n        .fillna(method='bfill')\n    )\n    return df\n\n\ndef exogenous_variables() -> pd.DataFrame:\n    \"\"\"Bring in exogenous parameters together.\n\n    From the paper:\n    We also include other exogenous variables that are likely to affect\n    the province’s budget. It is known that the various components of the\n    provincial budget can be influenced by the business cycle. Thus, following\n    Buettner and Wildsain (2006), we account for the potential effects of the\n    business cycle by including one-period lagged changes in the provincial\n    employment and unemployment rates. Another important exogenous factor\n    that is often cited in provincial budget documents as being important in\n    influencing the provincial government’s oil royalty revenue is the Canadian-U.S.\n    dollar exchange rate. For this reason, we control for this factor by\n    including one period lagged changes in the Canadian-U.S. dollar exchange rate\n\n    Returns\n    -------\n    pd.DataFrame\n        All the necessary exogenous factors for reproducing the paper.\n    \"\"\"\n    cadusd = annual_cad_usd()\n    ur_er = stats_can_exog()\n    df = pd.concat([cadusd, ur_er], axis='columns')\n    return df"
  },
  {
    "objectID": "posts/2021-02-26-ferede.html#exploratory-figures",
    "href": "posts/2021-02-26-ferede.html#exploratory-figures",
    "title": "Alberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue",
    "section": "Exploratory Figures",
    "text": "Exploratory Figures\n\nFigure 1\nPage 5 of the report charts Non-renewable Resource Revenue, Total Expenditure, and Total Revenue. All are in per-capita 2017 dollars. Reproducing this chart will be a good starting check that my data extraction and transformation matches the original author’s strategy.\n\n\nCode\ndef fig1(df: pd.DataFrame) -> alt.Chart:\n    \"\"\"Reproduce Figure 1 from the paper.\n\n    Parameters\n    ----------\n    df: pd.DataFrame\n        A dataframe with non-renewable resource revenue, total expenditure, and total revenue time series\n    \"\"\"\n    chart_df = (\n        df\n        .loc['1970':'2016', ['natural_resource_revenue', 'total_revenue', 'total_expenditure']]\n        .rename(columns={\n            'natural_resource_revenue': 'Non-renewable Resource Revenue',\n            'total_revenue': 'Total Revenue',\n            'total_expenditure': 'Total Expenditure'\n        })\n        .reset_index()\n        .melt(id_vars='budget_dt')\n    )\n    c_domain = [\"Non-renewable Resource Revenue\", \"Total Expenditure\", \"Total Revenue\"]\n    c_range = [\"green\", \"red\", \"blue\"]\n    chart = (\n        alt.Chart(chart_df)\n        .mark_line()\n        .encode(\n            x=alt.X('budget_dt:T', axis=alt.Axis(title=None)),\n            y=alt.Y('value:Q', axis=alt.Axis(title='Per capita in 2017 dollars'), scale=alt.Scale(domain=(0, 14_000))),\n            color=alt.Color('variable:N', legend=alt.Legend(title=None, orient='bottom'), scale=alt.Scale(domain=c_domain, range=c_range))\n        )\n        .properties(width=1250, height=500)\n    )\n    return chart\n\n\nHere’s the original chart from the paper:\n\nImage(filename=\"img/ferede_fig_1.png\")\n\n\n\n\nAnd here’s mine:\n\ndf = budget_real_per_capita()\nfig1(df)\n\n\n\n\nThis graph looks very similar to the chart in the paper, with a notable exception of the 1976/1977 budget year. My chart shows Non-renewable Resource Revenue as slightly negative, whereas the original chart has it largely in line with 1975/1976 and 1977/1978. NRR is negative in my chart because I have netted out contributions to the Alberta Heritage Savings Trust Fund (AHSTF). To the best of my understanding, the original paper does the same, and the consistent values between the two in all other years supports that. Quoting the original paper:\n\nThe part of resource revenue that is saved in the AHSTF is not expected to influence the provincial government’s spending and revenue-raising choices. For this reason, in our analysis, we exclude the part of the resource revenue that is saved in the AHSTF from the non-renewable-resource revenue data.\n\nFor comparison, here is the same chart, but without netting AHSTF contributions from revenue:\n\nno_net_df = (\n    df\n    .assign(total_revenue=lambda df: df[\"total_revenue\"] + df[\"total_heritage\"])\n    .assign(natural_resource_revenue=lambda df: df[\"natural_resource_revenue\"] + df[\"total_heritage\"])\n    .assign(deficit=lambda df: df[\"total_expenditure\"] - df[\"total_revenue\"])\n)\nfig1(no_net_df)\n\n\n\n\n1976/1977 more closely matches the original chart in the paper, but the remaining years in the period of mid 70s to mid 80s when there were significant contributions clearly do not match. Let’s try one more where I just substitute that one year.\n\nerror_df = df.copy()\nheritage_76 = error_df.loc[\"1976\", \"total_heritage\"]\nerror_df.loc[\"1976\", \"natural_resource_revenue\"] += heritage_76\nerror_df.loc[\"1976\", \"total_revenue\"] += heritage_76\nerror_df.loc[\"1976\", \"deficit\"] -= heritage_76\nfig1(error_df)\n\n\n\n\nHere’s the original figure again for easier comparison\n\nImage(filename=\"img/ferede_fig_1.png\")\n\n\n\n\nFrom eyeballing it that looks exactly like Figure 1 in the paper. It appears there’s a data error in the original paper. For the rest of this analysis I’ll compare both my base implementation of the data, as well as the one with the data error.\n\n\nFigure 2\nPage 6 of the paper produces a scatter plot of Real per capita non-renewable resource revenue on the X axis vs. Real per capita budget balance on the Y, along with a linear trend fit.\n\n\nCode\ndef fig2(df: pd.DataFrame) -> None:\n    \"\"\"Reproduce Figure 2 from the paper.\n    \n    Parameters\n    ----------\n    df: pd.DataFrame\n        The table with historical revenue and expenditure data.\n    \"\"\"\n    sns.set(rc={'figure.figsize':(11.7,8.27)})\n    chart_df = (\n        df\n        .loc['1970':'2016', ['natural_resource_revenue', 'deficit']]\n        .rename(columns={\n            'natural_resource_revenue': 'Non-renewable Resource Revenue',\n            'deficit': 'Deficit'\n        })\n        .assign(balance=lambda df: df['Deficit'] * -1)\n        .rename(columns={'balance': 'Budget Balance'})\n        .copy()\n    )\n    sns.regplot(x='Non-renewable Resource Revenue', y='Budget Balance', data=chart_df)\n\n\nHere’s the original figure\n\nImage(filename=\"img/ferede_fig_2.png\")\n\n\n\n\nHere’s the figure using my original data:\n\nfig2(df)\n\n\n\n\nAnd here’s the figure using the version with a data error:\n\nfig2(error_df)\n\n\n\n\nAgain, this chart is more consistent with the dataframe where I don’t net heritage fund contributions out of 1976 but do for all other years"
  },
  {
    "objectID": "posts/2021-02-26-ferede.html#model-specification-and-estimation",
    "href": "posts/2021-02-26-ferede.html#model-specification-and-estimation",
    "title": "Alberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue",
    "section": "Model Specification and estimation",
    "text": "Model Specification and estimation\nThis section combines the previously specified data extraction with transformations necessary to produce summary statistics, statistical tests, and the VAR model itself.\n\ndef model_df_levels(budg: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Combine real per capita budget data to get model data in levels.\n\n    lag exogenous variables (unemployment and employment rates, CAD/USD exchange)\n\n    Parameters\n    ----------\n    budg: pd.DataFrame\n        Budget data, either with or without data error\n\n    Returns\n    -------\n    pd.DataFrame\n        Budget data combined with exogenous factors\n    \"\"\"\n    exog = exogenous_variables()\n    df = (\n        pd.concat([budg, exog], axis='columns')\n        .rename(columns={'total_prog_exp': 'program_expenditure', 'EXCAUS': 'cad_usd'})\n        .assign(ur_lag=lambda df: df['unemployment_rate'].shift(periods=1))\n        .assign(er_lag=lambda df: df['employment_rate'].shift(periods=1))\n        .assign(cad_usd_lag=lambda df: df['cad_usd'].shift(periods=1))\n        .reindex(columns=[\n            'program_expenditure', 'debt_service', 'corporate_income_tax',\n            'personal_income_tax', 'other_revenue', 'natural_resource_revenue',\n            'deficit', 'heritage_dummy', 'ur_lag', 'er_lag', 'cad_usd_lag'\n        ])\n    )\n    return df\n\n\nmdfl = model_df_levels(df)\nmdfl_err = model_df_levels(error_df)\n\n\nSumary statistics for key variables, 1970-71, 2016-17 in levels\nPrior to any modeling, let’s compare the summary statistics for the data sets I’ve created against those in the paper:\n\n\nCode\nnumber = \"{:0<4,.1f}\"\npercent = '{:.1%}'\ncount = \"{:0.0f}\"\n\n\ndef tbl1_level(model_df: pd.DataFrame):\n    \"\"\"Produce summary statistics of the input data in levels.\n\n    Parameters\n    ----------\n    model_df: pd.DataFrame\n        Input data set\n\n    Returns\n    -------\n    pd.io.formats.style.Styler:\n        Nicely formatted summary statistics\n    \"\"\"\n    df = (\n        model_df\n        .loc['1970':'2016']\n        .copy()\n        .drop(columns=['heritage_dummy'])\n        .reindex(columns=[\n            'natural_resource_revenue', 'corporate_income_tax', 'personal_income_tax',\n            'other_revenue', 'debt_service', 'program_expenditure', 'deficit', 'ur_lag',\n            'er_lag', 'cad_usd_lag'\n        ])\n        .describe()\n        .T\n        .style.format({\n            'count': count,\n            'mean': number,\n            'std': number,\n            'min': number,\n            '25%': number,\n            '50%': number,\n            '75%': number,\n            'max': number\n        })\n    )\n    return df\n\n\nHere’s Table 1 from the paper:\n\nImage(filename=\"img/ferede_tbl_1.png\")\n\n\n\n\nHere’s my summary of the top half\n\ntbl1_level(mdfl)\n\n\n                    count        mean        std        min        25%        50%        75%        max    \n                \n                        natural_resource_revenue\n                        47\n                        2,737.2\n                        1,358.9\n                        -125.3\n                        1,726.4\n                        2,371.7\n                        3,941.6\n                        5,181.6\n            \n            \n                        corporate_income_tax\n                        47\n                        792.1\n                        349.6\n                        245.8\n                        544.6\n                        723.6\n                        1,010.9\n                        1,560.6\n            \n            \n                        personal_income_tax\n                        47\n                        1,862.1\n                        607.8\n                        767.1\n                        1,403.8\n                        1,889.6\n                        2,359.8\n                        2,830.0\n            \n            \n                        other_revenue\n                        47\n                        4,394.6\n                        1,084.9\n                        2,352.8\n                        3,632.2\n                        4,699.2\n                        5,209.2\n                        5,818.2\n            \n            \n                        debt_service\n                        47\n                        322.8\n                        334.9\n                        31.4\n                        82.7\n                        158.5\n                        486.9\n                        1,075.6\n            \n            \n                        program_expenditure\n                        47\n                        9,399.1\n                        2,161.2\n                        4,745.8\n                        7,623.0\n                        10,060.9\n                        11,142.6\n                        12,869.2\n            \n            \n                        deficit\n                        47\n                        -64.0\n                        1,618.0\n                        -3,184.0\n                        -1,320.5\n                        -140.6\n                        1,092.3\n                        3,775.8\n            \n            \n                        ur_lag\n                        47\n                        6.10\n                        2.20\n                        3.40\n                        4.50\n                        5.40\n                        7.50\n                        11.4\n            \n            \n                        er_lag\n                        47\n                        63.7\n                        9.90\n                        38.5\n                        65.1\n                        67.3\n                        69.0\n                        71.8\n            \n            \n                        cad_usd_lag\n                        45\n                        1.20\n                        0.20\n                        1.00\n                        1.10\n                        1.20\n                        1.40\n                        1.60\n            \n    \n\n\nAnd the same summary on the data with the introduced error\n\ntbl1_level(mdfl_err)\n\n\n                    count        mean        std        min        25%        50%        75%        max    \n                \n                        natural_resource_revenue\n                        47\n                        2,843.8\n                        1,325.6\n                        691.9\n                        1,760.7\n                        2,532.1\n                        3,972.6\n                        5,181.6\n            \n            \n                        corporate_income_tax\n                        47\n                        792.1\n                        349.6\n                        245.8\n                        544.6\n                        723.6\n                        1,010.9\n                        1,560.6\n            \n            \n                        personal_income_tax\n                        47\n                        1,862.1\n                        607.8\n                        767.1\n                        1,403.8\n                        1,889.6\n                        2,359.8\n                        2,830.0\n            \n            \n                        other_revenue\n                        47\n                        4,394.6\n                        1,084.9\n                        2,352.8\n                        3,632.2\n                        4,699.2\n                        5,209.2\n                        5,818.2\n            \n            \n                        debt_service\n                        47\n                        322.8\n                        334.9\n                        31.4\n                        82.7\n                        158.5\n                        486.9\n                        1,075.6\n            \n            \n                        program_expenditure\n                        47\n                        9,399.1\n                        2,161.2\n                        4,745.8\n                        7,623.0\n                        10,060.9\n                        11,142.6\n                        12,869.2\n            \n            \n                        deficit\n                        47\n                        -170.7\n                        1,587.7\n                        -3,184.0\n                        -1,425.7\n                        -202.8\n                        945.9\n                        3,775.8\n            \n            \n                        ur_lag\n                        47\n                        6.10\n                        2.20\n                        3.40\n                        4.50\n                        5.40\n                        7.50\n                        11.4\n            \n            \n                        er_lag\n                        47\n                        63.7\n                        9.90\n                        38.5\n                        65.1\n                        67.3\n                        69.0\n                        71.8\n            \n            \n                        cad_usd_lag\n                        45\n                        1.20\n                        0.20\n                        1.00\n                        1.10\n                        1.20\n                        1.40\n                        1.60\n            \n    \n\n\nAll the figures that I can validate against (exogenous variables aren’t reported in the paper) are reasonably close. The one noted difference is the previously described outlier in natural resource revenue which leads to my minimum for that variable being significantly lower than in the paper. That large one goes away again if I introduce the same data error described above. My guess for the remaining small discrepancies are differences in calculating population or CPI.\n\n\nSumary statistics for key variables, 1970-71, 2016-17, first difference\nReproduce the bottom half of table 1 from the paper\n\n\nCode\ndef model_df_first_diff(mdfl: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Produce the first difference of the level model df.\n\n    Parameters\n    ----------\n    mdfl: pd.DataFrame\n        The model dataframe in levels\n\n    Returns\n    -------\n    pd.DataFrame\n        The first differenced model dataframe\n    \"\"\"\n    df = (\n        mdfl\n        .diff()\n        .loc['1970':'2016']\n        .copy()\n        .assign(heritage_dummy=mdfl['heritage_dummy']) # don't want to lag diff this\n        .assign(constant=1)\n        .assign(zero=0)\n        .assign(nrrd=lambda df: df[['natural_resource_revenue', 'zero']].min(axis='columns'))\n        .assign(nrri=lambda df: df[['natural_resource_revenue', 'zero']].max(axis='columns'))\n        .reindex(columns=[\n            'natural_resource_revenue', 'nrri', 'nrrd', 'corporate_income_tax', 'personal_income_tax',\n            'other_revenue', 'debt_service', 'program_expenditure', 'deficit', 'ur_lag',\n            'er_lag', 'cad_usd_lag', 'heritage_dummy', 'constant'\n        ])\n    )\n    return df\n\n\n\n\nCode\ndef tbl1_diff(model_df: pd.DataFrame) -> pd.io.formats.style.Styler:\n    \"\"\"Produce summary statistics of the first differenced data set.\n\n    Parameters\n    ----------\n    model_df: pd.DataFrame\n        Input data set\n\n    Returns\n    -------\n    pd.io.formats.style.Styler:\n        Nicely styled summary statistics\n    \"\"\"\n\n    df = (\n        model_df_first_diff(model_df)\n        .drop(columns=['heritage_dummy', 'constant'])\n        .describe()\n        .T\n        .style.format({\n            'count': count,\n            'mean': number,\n            'std': number,\n            'min': number,\n            '25%': number,\n            '50%': number,\n            '75%': number,\n            'max': number\n        })\n    )\n    return df\n\n\nHere’s Table 1 from the paper again:\n\nImage(filename=\"img/ferede_tbl_1.png\")\n\n\n\n\nHere’s mine:\n\ntbl1_diff(mdfl)\n\n\n                    count        mean        std        min        25%        50%        75%        max    \n                \n                        natural_resource_revenue\n                        47\n                        -8.3\n                        1,365.3\n                        -4,543.2\n                        -551.3\n                        64.2\n                        463.5\n                        4,638.7\n            \n            \n                        nrri\n                        47\n                        437.8\n                        852.9\n                        0.00\n                        0.00\n                        64.2\n                        463.5\n                        4,638.7\n            \n            \n                        nrrd\n                        47\n                        -446.1\n                        858.9\n                        -4,543.2\n                        -551.3\n                        0.00\n                        0.00\n                        0.00\n            \n            \n                        corporate_income_tax\n                        47\n                        13.3\n                        201.2\n                        -463.0\n                        -108.5\n                        25.9\n                        137.8\n                        447.1\n            \n            \n                        personal_income_tax\n                        47\n                        42.7\n                        227.2\n                        -690.7\n                        -28.7\n                        40.0\n                        154.2\n                        936.9\n            \n            \n                        other_revenue\n                        47\n                        41.8\n                        577.2\n                        -1,268.7\n                        -288.9\n                        -28.0\n                        162.8\n                        2,417.6\n            \n            \n                        debt_service\n                        47\n                        4.50\n                        86.2\n                        -247.5\n                        -21.6\n                        2.00\n                        36.0\n                        222.3\n            \n            \n                        program_expenditure\n                        47\n                        147.4\n                        734.5\n                        -1,263.2\n                        -245.9\n                        202.7\n                        552.7\n                        2,477.9\n            \n            \n                        deficit\n                        47\n                        62.3\n                        1,578.2\n                        -4,722.8\n                        -806.6\n                        -39.0\n                        836.0\n                        4,431.3\n            \n            \n                        ur_lag\n                        47\n                        0.10\n                        1.10\n                        -1.8\n                        -0.7\n                        -0.1\n                        0.30\n                        3.90\n            \n            \n                        er_lag\n                        47\n                        0.60\n                        3.30\n                        -3.2\n                        -0.2\n                        0.40\n                        0.90\n                        21.4\n            \n            \n                        cad_usd_lag\n                        44\n                        0.00\n                        0.10\n                        -0.2\n                        -0.0\n                        0.00\n                        0.10\n                        0.20\n            \n    \n\n\nAnd with the data error:\n\ntbl1_diff(mdfl_err)\n\n\n                    count        mean        std        min        25%        50%        75%        max    \n                \n                        natural_resource_revenue\n                        47\n                        -8.3\n                        977.5\n                        -2,451.6\n                        -471.1\n                        64.2\n                        458.3\n                        2,757.8\n            \n            \n                        nrri\n                        47\n                        349.1\n                        579.4\n                        0.00\n                        0.00\n                        64.2\n                        458.3\n                        2,757.8\n            \n            \n                        nrrd\n                        47\n                        -357.4\n                        604.0\n                        -2,451.6\n                        -471.1\n                        0.00\n                        0.00\n                        0.00\n            \n            \n                        corporate_income_tax\n                        47\n                        13.3\n                        201.2\n                        -463.0\n                        -108.5\n                        25.9\n                        137.8\n                        447.1\n            \n            \n                        personal_income_tax\n                        47\n                        42.7\n                        227.2\n                        -690.7\n                        -28.7\n                        40.0\n                        154.2\n                        936.9\n            \n            \n                        other_revenue\n                        47\n                        41.8\n                        577.2\n                        -1,268.7\n                        -288.9\n                        -28.0\n                        162.8\n                        2,417.6\n            \n            \n                        debt_service\n                        47\n                        4.50\n                        86.2\n                        -247.5\n                        -21.6\n                        2.00\n                        36.0\n                        222.3\n            \n            \n                        program_expenditure\n                        47\n                        147.4\n                        734.5\n                        -1,263.2\n                        -245.9\n                        202.7\n                        552.7\n                        2,477.9\n            \n            \n                        deficit\n                        47\n                        62.3\n                        1,260.2\n                        -2,615.7\n                        -758.1\n                        -39.0\n                        747.4\n                        3,117.0\n            \n            \n                        ur_lag\n                        47\n                        0.10\n                        1.10\n                        -1.8\n                        -0.7\n                        -0.1\n                        0.30\n                        3.90\n            \n            \n                        er_lag\n                        47\n                        0.60\n                        3.30\n                        -3.2\n                        -0.2\n                        0.40\n                        0.90\n                        21.4\n            \n            \n                        cad_usd_lag\n                        44\n                        0.00\n                        0.10\n                        -0.2\n                        -0.0\n                        0.00\n                        0.10\n                        0.20\n            \n    \n\n\nAs with everything so far, the data overall matches, and Natural Resource Revenue matches a lot better if I neglect to net out the heritage fund in 1976.\n\n\nUnit-Root Tests\nTable A1 in the paper shows the results of unit root tests for both the level and first differenced variables in the model. This section will reproduce those tables\n\ndef stationarity_tests(df: pd.DataFrame, first_diff: bool = False) -> pd.DataFrame:\n    \"\"\"Compute stationarity test statistics.\n\n    Parameters\n    ----------\n    df: pd.DataFrame\n        The model input data\n    first_diff: bool, default False\n        Perform tests on first differenced version of the data\n    \"\"\"\n    if first_diff:\n        df = (\n            model_df_first_diff(df)\n            .drop(columns=[\"heritage_dummy\", \"constant\"])\n        )\n    else:\n        df = (\n            df\n            .loc['1970':'2016']\n            .copy()\n            .drop(columns=['heritage_dummy'])\n            .reindex(\n                columns=[\n                    'natural_resource_revenue',\n                    'corporate_income_tax',\n                    'personal_income_tax',\n                    'other_revenue',\n                    'debt_service',\n                    'program_expenditure',\n                    'deficit',\n                    'ur_lag',\n                    'er_lag',\n                    'cad_usd_lag',\n                ]\n            )\n        )\n    tests_dict = {'ADF': ADF, 'Phillips-Perron': PhillipsPerron, 'DF-GLS': DFGLS}\n    cols = df.columns\n    tests_df = pd.DataFrame()\n    for test_label, test in tests_dict.items():\n        for col in cols:\n            if test_label != 'Phillips-Perron':\n                col_test = test(df[col].dropna(), method='BIC')\n            else:\n                col_test = test(df[col].dropna())\n            test_val = col_test.stat\n            test_p = col_test.pvalue\n            test_summary = f'val: {test_val:0.3f}, p: {test_p:.1%}'\n            tests_df.loc[col, test_label] = test_summary\n    return tests_df\n\nHere’s the table from the paper:\n\nImage(filename=\"img/ferede_tbl_a1.png\")\n\n\n\n\nHere is mine with my data set\n\nstationarity_tests(mdfl, first_diff=False)\n\n\n\n\n\n  \n    \n      \n      ADF\n      Phillips-Perron\n      DF-GLS\n    \n  \n  \n    \n      natural_resource_revenue\n      val: -3.927, p: 0.2%\n      val: -4.056, p: 0.1%\n      val: -3.125, p: 0.2%\n    \n    \n      corporate_income_tax\n      val: -2.340, p: 15.9%\n      val: -2.285, p: 17.7%\n      val: -1.676, p: 9.2%\n    \n    \n      personal_income_tax\n      val: -1.650, p: 45.7%\n      val: -1.440, p: 56.3%\n      val: -0.688, p: 43.2%\n    \n    \n      other_revenue\n      val: -2.196, p: 20.8%\n      val: -2.057, p: 26.2%\n      val: -1.585, p: 11.0%\n    \n    \n      debt_service\n      val: -1.548, p: 51.0%\n      val: -1.697, p: 43.3%\n      val: -1.377, p: 16.2%\n    \n    \n      program_expenditure\n      val: -2.966, p: 3.8%\n      val: -2.137, p: 23.0%\n      val: -0.682, p: 43.4%\n    \n    \n      deficit\n      val: -3.510, p: 0.8%\n      val: -3.924, p: 0.2%\n      val: -3.537, p: 0.0%\n    \n    \n      ur_lag\n      val: -2.709, p: 7.3%\n      val: -2.071, p: 25.6%\n      val: -2.319, p: 2.1%\n    \n    \n      er_lag\n      val: -10.652, p: 0.0%\n      val: -2.931, p: 4.2%\n      val: -0.763, p: 39.7%\n    \n    \n      cad_usd_lag\n      val: -2.454, p: 12.7%\n      val: -1.796, p: 38.2%\n      val: -1.870, p: 6.1%\n    \n  \n\n\n\n\n\nstationarity_tests(mdfl, first_diff=True)\n\n\n\n\n\n  \n    \n      \n      ADF\n      Phillips-Perron\n      DF-GLS\n    \n  \n  \n    \n      natural_resource_revenue\n      val: -7.403, p: 0.0%\n      val: -10.300, p: 0.0%\n      val: -7.430, p: 0.0%\n    \n    \n      nrri\n      val: -6.885, p: 0.0%\n      val: -6.896, p: 0.0%\n      val: -6.533, p: 0.0%\n    \n    \n      nrrd\n      val: -7.835, p: 0.0%\n      val: -9.218, p: 0.0%\n      val: -7.634, p: 0.0%\n    \n    \n      corporate_income_tax\n      val: -7.228, p: 0.0%\n      val: -8.484, p: 0.0%\n      val: -7.103, p: 0.0%\n    \n    \n      personal_income_tax\n      val: -5.560, p: 0.0%\n      val: -8.831, p: 0.0%\n      val: -6.703, p: 0.0%\n    \n    \n      other_revenue\n      val: -7.914, p: 0.0%\n      val: -8.169, p: 0.0%\n      val: -7.717, p: 0.0%\n    \n    \n      debt_service\n      val: -3.918, p: 0.2%\n      val: -4.252, p: 0.1%\n      val: -3.931, p: 0.0%\n    \n    \n      program_expenditure\n      val: -2.545, p: 10.5%\n      val: -6.105, p: 0.0%\n      val: -2.461, p: 1.4%\n    \n    \n      deficit\n      val: -2.485, p: 11.9%\n      val: -10.105, p: 0.0%\n      val: -2.509, p: 1.2%\n    \n    \n      ur_lag\n      val: -4.924, p: 0.0%\n      val: -4.056, p: 0.1%\n      val: -4.901, p: 0.0%\n    \n    \n      er_lag\n      val: -2.697, p: 7.5%\n      val: -6.507, p: 0.0%\n      val: -2.742, p: 0.6%\n    \n    \n      cad_usd_lag\n      val: -3.318, p: 1.4%\n      val: -3.042, p: 3.1%\n      val: -3.384, p: 0.1%\n    \n  \n\n\n\n\nAnd with the error data set:\n\nstationarity_tests(mdfl_err, first_diff=False)\n\n\n\n\n\n  \n    \n      \n      ADF\n      Phillips-Perron\n      DF-GLS\n    \n  \n  \n    \n      natural_resource_revenue\n      val: -2.652, p: 8.3%\n      val: -2.875, p: 4.8%\n      val: -2.113, p: 3.5%\n    \n    \n      corporate_income_tax\n      val: -2.340, p: 15.9%\n      val: -2.285, p: 17.7%\n      val: -1.676, p: 9.2%\n    \n    \n      personal_income_tax\n      val: -1.650, p: 45.7%\n      val: -1.440, p: 56.3%\n      val: -0.688, p: 43.2%\n    \n    \n      other_revenue\n      val: -2.196, p: 20.8%\n      val: -2.057, p: 26.2%\n      val: -1.585, p: 11.0%\n    \n    \n      debt_service\n      val: -1.548, p: 51.0%\n      val: -1.697, p: 43.3%\n      val: -1.377, p: 16.2%\n    \n    \n      program_expenditure\n      val: -2.966, p: 3.8%\n      val: -2.137, p: 23.0%\n      val: -0.682, p: 43.4%\n    \n    \n      deficit\n      val: -2.566, p: 10.0%\n      val: -2.763, p: 6.4%\n      val: -2.606, p: 0.9%\n    \n    \n      ur_lag\n      val: -2.709, p: 7.3%\n      val: -2.071, p: 25.6%\n      val: -2.319, p: 2.1%\n    \n    \n      er_lag\n      val: -10.652, p: 0.0%\n      val: -2.931, p: 4.2%\n      val: -0.763, p: 39.7%\n    \n    \n      cad_usd_lag\n      val: -2.454, p: 12.7%\n      val: -1.796, p: 38.2%\n      val: -1.870, p: 6.1%\n    \n  \n\n\n\n\n\nstationarity_tests(mdfl_err, first_diff=True)\n\n\n\n\n\n  \n    \n      \n      ADF\n      Phillips-Perron\n      DF-GLS\n    \n  \n  \n    \n      natural_resource_revenue\n      val: -5.913, p: 0.0%\n      val: -7.791, p: 0.0%\n      val: -5.925, p: 0.0%\n    \n    \n      nrri\n      val: -6.119, p: 0.0%\n      val: -6.149, p: 0.0%\n      val: -5.731, p: 0.0%\n    \n    \n      nrrd\n      val: -7.846, p: 0.0%\n      val: -8.008, p: 0.0%\n      val: -7.640, p: 0.0%\n    \n    \n      corporate_income_tax\n      val: -7.228, p: 0.0%\n      val: -8.484, p: 0.0%\n      val: -7.103, p: 0.0%\n    \n    \n      personal_income_tax\n      val: -5.560, p: 0.0%\n      val: -8.831, p: 0.0%\n      val: -6.703, p: 0.0%\n    \n    \n      other_revenue\n      val: -7.914, p: 0.0%\n      val: -8.169, p: 0.0%\n      val: -7.717, p: 0.0%\n    \n    \n      debt_service\n      val: -3.918, p: 0.2%\n      val: -4.252, p: 0.1%\n      val: -3.931, p: 0.0%\n    \n    \n      program_expenditure\n      val: -2.545, p: 10.5%\n      val: -6.105, p: 0.0%\n      val: -2.461, p: 1.4%\n    \n    \n      deficit\n      val: -6.642, p: 0.0%\n      val: -8.574, p: 0.0%\n      val: -6.609, p: 0.0%\n    \n    \n      ur_lag\n      val: -4.924, p: 0.0%\n      val: -4.056, p: 0.1%\n      val: -4.901, p: 0.0%\n    \n    \n      er_lag\n      val: -2.697, p: 7.5%\n      val: -6.507, p: 0.0%\n      val: -2.742, p: 0.6%\n    \n    \n      cad_usd_lag\n      val: -3.318, p: 1.4%\n      val: -3.042, p: 3.1%\n      val: -3.384, p: 0.1%\n    \n  \n\n\n\n\nDocumentation on the test tools I used can be found here\nThere are some interesting differences. Most notable is that on the levels of the deficit series I reject the null hypothesis of a unit root using all three tests at a significance level < 1%. The paper specifically notes that if the deficit is stationary in levels then a Vector Error Correction model can be applied. As the original author’s fails to reject the null he implements a Vector AutoRegression model on the first differenced data. In levels the only other series that I find to be stationary is natural resource revenue. ADF on program expenditure would also reject the null at 5% significance, but would fail to reject it using the other two tests.\nLooking at the first differenced series, since that’s what the paper ultimately ends up using, I also reject the null hypothesis of a unit root for all variables using all tests at a 1% significant except program expenditure and deficit using Augmented Dickey Fuller. Those last two tests differ from what’s reported in the paper.\nThe paper notes that it uses the Schwarz Information Criterion (SIC) for determining optimal lags in the DF-GLS test. It doesn’t specify what it’s using in the other two tests. For ADF and DF-GLS I used the Schwarz/Bayesian IC (BIC), which is just another name for SIC. Phillips-Perron only uses 1 lag and then Newey-West for a long run variance estimator. I also ran these tests using Akaike IC (AIC) for optimal lags for ADF and DF-GLS, with similar results.\nAgain we can see that using the results with the data error more closely matches the table in the paper, specifically around resource revenue and deficits.\n\n\nThe Model\nThe conclusions from the paper are based on fitting a VAR to the first differenced data set we’ve been analyzing above. Let’s do that now and compare the results to the paper.\n\ndef fit_var(mdfl: pd.DataFrame) -> statsmodels.tsa.vector_ar.var_model.VARResults:\n    \"\"\"Fit a VAR to the model data.\n\n    Parameters\n    ----------\n    mdfl: pd.DataFrame\n        Input model data\n\n    Returns\n    -------\n    statsmodels.tsa.vector_ar.var_model.VarResults\n        The fitted model\n    \"\"\"\n    vec_df = (\n        model_df_first_diff(mdfl)\n        .drop(columns='natural_resource_revenue')\n        .dropna()\n    )\n    endog_df = vec_df[[\n        'nrri', 'nrrd', 'program_expenditure', 'debt_service', 'corporate_income_tax',\n         'personal_income_tax', 'other_revenue'\n    ]]\n    exog_df = vec_df[['ur_lag', 'er_lag', 'cad_usd_lag', 'heritage_dummy']]\n    model = VAR(endog=endog_df, exog=exog_df, freq='AS')\n    # Fit the model with 2 lags\n    results = model.fit(2)\n    return results\n\n\n\nCode\ndef highlight_significance(val):\n    \"\"\"Colour code statistical significance.\n\n    Takes a scalar and returns a string with\n    the css property `'color: <color>'` where\n    color is maroon for 1% significance, \n    red for 5% significance,\n    orange for 10%, and black otherwise\n\n    Parameters\n    ----------\n    val: float\n        The p value of a test\n\n    Returns\n    -------\n    str:\n        A formatted colour coded p value\n    \"\"\"\n\n    if val <= 0.01:\n        color = 'maroon'\n    elif val <= 0.05:\n        color = 'red'\n    elif val <= 0.1:\n        color = 'orange'\n    else:\n        color = 'black'\n    return f'color: {color}'\n\n\n\nresults = fit_var(mdfl)\nresults_err = fit_var(mdfl_err)\nsummary = results.summary()\n\n\n# Set up the rows and columns of my parameters to match the paper\nreindex_cols = [\"program_expenditure\", \"debt_service\", \"corporate_income_tax\", \"personal_income_tax\", \"other_revenue\", \"nrri\", \"nrrd\"]\nindex_order = [\"nrri\", \"nrrd\", \"program_expenditure\", \"debt_service\", \"corporate_income_tax\", \"personal_income_tax\", \"other_revenue\"]\nreindex_rows = list(chain.from_iterable((f\"L1.{row}\", f\"L2.{row}\") for row in index_order))\n\nHere’s the table from the paper:\n\nImage(filename=\"img/ferede_tbl_a2.png\")\n\n\n\n\n\nresults.params.reindex(index=reindex_rows, columns=reindex_cols)\n\n\n\n\n\n  \n    \n      \n      program_expenditure\n      debt_service\n      corporate_income_tax\n      personal_income_tax\n      other_revenue\n      nrri\n      nrrd\n    \n  \n  \n    \n      L1.nrri\n      0.307014\n      -0.028267\n      0.081846\n      -0.052581\n      -0.197417\n      0.157775\n      0.002814\n    \n    \n      L2.nrri\n      0.328607\n      0.006210\n      -0.044256\n      -0.031283\n      -0.073461\n      0.059988\n      -0.052033\n    \n    \n      L1.nrrd\n      0.131802\n      -0.034380\n      0.026578\n      -0.144894\n      0.193682\n      -0.067386\n      -0.333322\n    \n    \n      L2.nrrd\n      0.353466\n      -0.020362\n      0.089569\n      -0.037287\n      -0.147460\n      0.110970\n      -0.055394\n    \n    \n      L1.program_expenditure\n      -0.089949\n      0.024739\n      -0.000802\n      -0.053868\n      0.070080\n      -0.065026\n      -0.105625\n    \n    \n      L2.program_expenditure\n      -0.262484\n      0.009716\n      -0.031223\n      0.042743\n      0.410439\n      0.025392\n      -0.456895\n    \n    \n      L1.debt_service\n      0.794685\n      0.401266\n      -0.108643\n      0.336733\n      0.400447\n      -2.370450\n      -0.278888\n    \n    \n      L2.debt_service\n      -2.693856\n      0.228409\n      -0.088742\n      -0.179124\n      -0.041884\n      0.290135\n      -0.215989\n    \n    \n      L1.corporate_income_tax\n      -0.972853\n      -0.092308\n      -0.210123\n      0.169059\n      -0.071714\n      -0.914462\n      0.730905\n    \n    \n      L2.corporate_income_tax\n      0.088413\n      -0.199031\n      -0.206852\n      0.245321\n      -0.509617\n      -0.683630\n      -0.578293\n    \n    \n      L1.personal_income_tax\n      0.959002\n      -0.041882\n      0.292569\n      -0.378646\n      -0.250158\n      0.347750\n      0.862515\n    \n    \n      L2.personal_income_tax\n      0.814557\n      0.084507\n      -0.112748\n      -0.099020\n      -0.528812\n      0.385575\n      -0.226230\n    \n    \n      L1.other_revenue\n      0.347131\n      -0.015834\n      -0.042866\n      -0.022883\n      -0.296401\n      0.224576\n      -0.102018\n    \n    \n      L2.other_revenue\n      -0.140592\n      -0.010601\n      -0.009822\n      -0.010749\n      -0.339955\n      0.385613\n      0.043835\n    \n  \n\n\n\n\n\nresults.pvalues.reindex(index=reindex_rows, columns=reindex_cols).style.applymap(highlight_significance).format(\"{:.2%}\")\n\n\n                    program_expenditure        debt_service        corporate_income_tax        personal_income_tax        other_revenue        nrri        nrrd    \n                \n                        L1.nrri\n                        17.85%\n                        28.23%\n                        19.05%\n                        46.54%\n                        31.40%\n                        35.35%\n                        99.27%\n            \n            \n                        L2.nrri\n                        3.41%\n                        72.82%\n                        29.76%\n                        52.28%\n                        58.14%\n                        60.37%\n                        80.42%\n            \n            \n                        L1.nrrd\n                        54.73%\n                        17.31%\n                        65.78%\n                        3.61%\n                        30.34%\n                        67.97%\n                        26.07%\n            \n            \n                        L2.nrrd\n                        8.86%\n                        39.46%\n                        11.53%\n                        56.93%\n                        40.83%\n                        47.32%\n                        84.37%\n            \n            \n                        L1.program_expenditure\n                        63.72%\n                        26.03%\n                        98.78%\n                        37.09%\n                        66.89%\n                        64.73%\n                        68.24%\n            \n            \n                        L2.program_expenditure\n                        16.67%\n                        65.69%\n                        54.83%\n                        47.56%\n                        1.18%\n                        85.75%\n                        7.53%\n            \n            \n                        L1.debt_service\n                        62.80%\n                        3.37%\n                        80.89%\n                        51.54%\n                        77.63%\n                        5.24%\n                        90.00%\n            \n            \n                        L2.debt_service\n                        9.99%\n                        22.60%\n                        84.32%\n                        72.89%\n                        97.63%\n                        81.21%\n                        92.24%\n            \n            \n                        L1.corporate_income_tax\n                        17.66%\n                        26.58%\n                        28.68%\n                        45.69%\n                        90.77%\n                        8.83%\n                        45.31%\n            \n            \n                        L2.corporate_income_tax\n                        90.24%\n                        1.66%\n                        29.52%\n                        28.12%\n                        41.09%\n                        20.34%\n                        55.35%\n            \n            \n                        L1.personal_income_tax\n                        11.91%\n                        55.47%\n                        8.27%\n                        5.12%\n                        63.61%\n                        44.83%\n                        30.03%\n            \n            \n                        L2.personal_income_tax\n                        12.28%\n                        16.47%\n                        43.56%\n                        55.23%\n                        24.36%\n                        32.70%\n                        75.15%\n            \n            \n                        L1.other_revenue\n                        11.25%\n                        52.98%\n                        47.45%\n                        74.03%\n                        11.48%\n                        16.83%\n                        73.04%\n            \n            \n                        L2.other_revenue\n                        54.20%\n                        68.98%\n                        87.64%\n                        88.26%\n                        8.61%\n                        2.48%\n                        88.83%\n            \n    \n\n\n\nresults_err.params.reindex(index=reindex_rows, columns=reindex_cols)\n\n\n\n\n\n  \n    \n      \n      program_expenditure\n      debt_service\n      corporate_income_tax\n      personal_income_tax\n      other_revenue\n      nrri\n      nrrd\n    \n  \n  \n    \n      L1.nrri\n      0.531582\n      -0.024542\n      0.107731\n      -0.123072\n      -0.150617\n      0.208281\n      -0.080868\n    \n    \n      L2.nrri\n      -0.184916\n      -0.003392\n      -0.079855\n      -0.031832\n      -0.134564\n      0.023580\n      0.076075\n    \n    \n      L1.nrrd\n      -0.164689\n      -0.033454\n      0.002593\n      -0.085661\n      0.110608\n      -0.058941\n      -0.105753\n    \n    \n      L2.nrrd\n      0.179398\n      -0.020179\n      0.072508\n      0.027891\n      -0.178763\n      0.081864\n      -0.032297\n    \n    \n      L1.program_expenditure\n      -0.148569\n      0.024375\n      0.010583\n      -0.035676\n      0.095480\n      -0.051153\n      -0.027252\n    \n    \n      L2.program_expenditure\n      -0.274168\n      0.012469\n      -0.028204\n      0.060072\n      0.419366\n      0.043190\n      -0.266589\n    \n    \n      L1.debt_service\n      -0.138827\n      0.391381\n      -0.192682\n      0.456206\n      0.250624\n      -2.379002\n      0.182134\n    \n    \n      L2.debt_service\n      -1.839879\n      0.244616\n      0.030729\n      -0.321701\n      0.208719\n      0.387627\n      0.026273\n    \n    \n      L1.corporate_income_tax\n      -0.591167\n      -0.087419\n      -0.166213\n      0.193455\n      -0.009423\n      -1.069105\n      0.084121\n    \n    \n      L2.corporate_income_tax\n      0.986182\n      -0.171169\n      -0.180347\n      0.219469\n      -0.439124\n      -0.441616\n      0.337464\n    \n    \n      L1.personal_income_tax\n      0.446547\n      -0.050872\n      0.281996\n      -0.364826\n      -0.291701\n      0.365623\n      0.499642\n    \n    \n      L2.personal_income_tax\n      0.537147\n      0.068406\n      -0.161794\n      -0.159843\n      -0.598672\n      0.542697\n      -0.054906\n    \n    \n      L1.other_revenue\n      0.319686\n      -0.014068\n      -0.036482\n      -0.021636\n      -0.280981\n      0.235076\n      -0.052564\n    \n    \n      L2.other_revenue\n      -0.328932\n      -0.009975\n      -0.017606\n      0.013375\n      -0.362590\n      0.255418\n      -0.122366\n    \n  \n\n\n\n\n\nresults_err.pvalues.reindex(index=reindex_rows, columns=reindex_cols).style.applymap(highlight_significance).format(\"{:.2%}\")\n\n\n                    program_expenditure        debt_service        corporate_income_tax        personal_income_tax        other_revenue        nrri        nrrd    \n                \n                        L1.nrri\n                        6.09%\n                        42.34%\n                        12.95%\n                        12.90%\n                        50.73%\n                        26.95%\n                        75.32%\n            \n            \n                        L2.nrri\n                        53.56%\n                        91.63%\n                        28.56%\n                        70.91%\n                        57.36%\n                        90.54%\n                        77.87%\n            \n            \n                        L1.nrrd\n                        54.19%\n                        25.17%\n                        96.94%\n                        26.71%\n                        60.91%\n                        74.28%\n                        66.59%\n            \n            \n                        L2.nrrd\n                        49.68%\n                        47.94%\n                        27.30%\n                        71.17%\n                        39.79%\n                        64.10%\n                        89.27%\n            \n            \n                        L1.program_expenditure\n                        47.97%\n                        28.33%\n                        84.07%\n                        55.26%\n                        57.06%\n                        71.44%\n                        88.63%\n            \n            \n                        L2.program_expenditure\n                        18.15%\n                        57.39%\n                        58.33%\n                        30.58%\n                        1.07%\n                        75.16%\n                        15.20%\n            \n            \n                        L1.debt_service\n                        93.96%\n                        4.82%\n                        67.48%\n                        38.39%\n                        86.44%\n                        5.10%\n                        91.27%\n            \n            \n                        L2.debt_service\n                        31.50%\n                        21.64%\n                        94.66%\n                        53.88%\n                        88.68%\n                        75.02%\n                        98.74%\n            \n            \n                        L1.corporate_income_tax\n                        47.78%\n                        33.14%\n                        42.56%\n                        41.64%\n                        98.87%\n                        5.35%\n                        91.13%\n            \n            \n                        L2.corporate_income_tax\n                        17.90%\n                        3.09%\n                        32.66%\n                        29.54%\n                        45.50%\n                        36.55%\n                        61.21%\n            \n            \n                        L1.personal_income_tax\n                        51.65%\n                        49.41%\n                        10.20%\n                        6.37%\n                        59.67%\n                        42.44%\n                        42.34%\n            \n            \n                        L2.personal_income_tax\n                        39.82%\n                        31.95%\n                        30.98%\n                        37.91%\n                        23.97%\n                        19.93%\n                        92.41%\n            \n            \n                        L1.other_revenue\n                        16.84%\n                        57.49%\n                        53.04%\n                        74.43%\n                        13.07%\n                        12.78%\n                        80.28%\n            \n            \n                        L2.other_revenue\n                        18.72%\n                        71.13%\n                        77.81%\n                        85.12%\n                        6.95%\n                        12.35%\n                        58.85%\n            \n    \n\n\nOnce again, my results with the data error included are much closer to the original paper results. Note that without the data error, program spending is shown to rise in response to an increase or decrease in natural resource revenue, but only on the second lag (at least at a statistically significant level. That’s completely contrary to the main thesis of the paper. Now, given that I’ve shown with the corrected data set that budget deficits are stationary in levels, maybe a more appropriate form of analysis would have been to use a VECM as the paper states it would have been, but from this I can say that the data error has led to a significant change in the outcome of the analysis.\n\n\nImpulse Response Functions\nThe actual results of the paper involve taking the estimated impulse response functions derived from the VAR model and examining their implications. Given the results above I’m not sure there’s a lot of value in reproducing all of the other results of this model, but I do want to at least reproduce the IRFs.\n\n\nCode\ndef irf_tbl(result: statsmodels.tsa.vector_ar.var_model.VARResults, impulse: str) -> pd.DataFrame:\n    \"\"\"Show the IRF as in table 3 of the paper.\n\n    Parameters\n    ----------\n    result: statsmodels.tsa.vector_ar.var_model.VARResults\n        The fitted VAR\n\n    impulse: str\n        The impulse function, either an increase or decrease in natural resource revenue\n\n    Returns\n    -------\n    pd.DataFrame\n        A summary table\n    \"\"\"\n    irf = result.irf()\n    irf_stderr = irf.stderr()\n    irfs = irf.irfs\n    params = list(results.params.columns)\n\n    def _impulse_response(impulse: str, response: str) -> pd.DataFrame:\n        \"\"\"Get a specific IRF out of the big array.\n\n        Parameters\n        ----------\n        impulse: str\n            The impulse function\n        response: str\n            The response function\n\n        Returns\n        -------\n        pd.DataFrame\n            The 3 period IRF\n        \"\"\"\n        imp_ind = params.index(impulse)\n        res_ind = params.index(response)\n        ir = irfs[:, res_ind, imp_ind]\n        se = irf_stderr[:, res_ind, imp_ind]\n        imp_name = response + '_impulse'\n        se_name = response + '_se'\n        df = pd.DataFrame({imp_name: ir, se_name: se})\n        return df.loc[1:3].T\n\n    responses = params[2:]\n    return pd.concat([_impulse_response(impulse, response) for response in responses])\n\n\n\nTable 3\nIMPACTS ON ALBERTA’S BUDGET OF A ONE-DOLLAR INNOVATION IN NON-RENEWABLE-RESOURCE REVENUE (ASYMMETRIC CASE), 1970/71–2016/17\n\nImage(filename=\"img/ferede_tbl_3.png\")\n\n\n\n\n\nirf_tbl(results, \"nrrd\")\n\n\n\n\n\n  \n    \n      \n      1\n      2\n      3\n    \n  \n  \n    \n      program_expenditure_impulse\n      0.131802\n      0.152091\n      -0.263057\n    \n    \n      program_expenditure_se\n      0.219009\n      0.213435\n      0.189561\n    \n    \n      debt_service_impulse\n      -0.034380\n      -0.016984\n      -0.025572\n    \n    \n      debt_service_se\n      0.025235\n      0.026153\n      0.023777\n    \n    \n      corporate_income_tax_impulse\n      0.026578\n      0.022545\n      0.014045\n    \n    \n      corporate_income_tax_se\n      0.060010\n      0.062050\n      0.049948\n    \n    \n      personal_income_tax_impulse\n      -0.144894\n      0.050801\n      0.023045\n    \n    \n      personal_income_tax_se\n      0.069135\n      0.090279\n      0.082072\n    \n    \n      other_revenue_impulse\n      0.193682\n      -0.226313\n      0.116648\n    \n    \n      other_revenue_se\n      0.188179\n      0.190130\n      0.169276\n    \n  \n\n\n\n\n\nirf_tbl(results, \"nrri\")\n\n\n\n\n\n  \n    \n      \n      1\n      2\n      3\n    \n  \n  \n    \n      program_expenditure_impulse\n      0.307014\n      0.128759\n      0.032942\n    \n    \n      program_expenditure_se\n      0.228184\n      0.200129\n      0.172849\n    \n    \n      debt_service_impulse\n      -0.028267\n      -0.004321\n      -0.011658\n    \n    \n      debt_service_se\n      0.026292\n      0.026157\n      0.025746\n    \n    \n      corporate_income_tax_impulse\n      0.081846\n      -0.052562\n      -0.019982\n    \n    \n      corporate_income_tax_se\n      0.062524\n      0.057369\n      0.041195\n    \n    \n      personal_income_tax_impulse\n      -0.052581\n      -0.027779\n      0.040890\n    \n    \n      personal_income_tax_se\n      0.072031\n      0.085436\n      0.068562\n    \n    \n      other_revenue_impulse\n      -0.197417\n      -0.028068\n      0.187637\n    \n    \n      other_revenue_se\n      0.196063\n      0.172347\n      0.147472\n    \n  \n\n\n\n\n\nirf_tbl(results_err, \"nrrd\")\n\n\n\n\n\n  \n    \n      \n      1\n      2\n      3\n    \n  \n  \n    \n      program_expenditure_impulse\n      -0.164689\n      0.190170\n      0.024520\n    \n    \n      program_expenditure_se\n      0.270039\n      0.264733\n      0.231589\n    \n    \n      debt_service_impulse\n      -0.033454\n      -0.029727\n      -0.027865\n    \n    \n      debt_service_se\n      0.029185\n      0.033489\n      0.028892\n    \n    \n      corporate_income_tax_impulse\n      0.002593\n      0.041965\n      0.055297\n    \n    \n      corporate_income_tax_se\n      0.067657\n      0.068952\n      0.054178\n    \n    \n      personal_income_tax_impulse\n      -0.085661\n      0.064177\n      -0.029006\n    \n    \n      personal_income_tax_se\n      0.077188\n      0.085444\n      0.072486\n    \n    \n      other_revenue_impulse\n      0.110608\n      -0.211807\n      -0.018863\n    \n    \n      other_revenue_se\n      0.216285\n      0.206303\n      0.197149\n    \n  \n\n\n\n\n\nirf_tbl(results_err, \"nrri\")\n\n\n\n\n\n  \n    \n      \n      1\n      2\n      3\n    \n  \n  \n    \n      program_expenditure_impulse\n      0.531582\n      -0.303243\n      -0.034754\n    \n    \n      program_expenditure_se\n      0.283645\n      0.282106\n      0.247443\n    \n    \n      debt_service_impulse\n      -0.024542\n      -0.003484\n      -0.020839\n    \n    \n      debt_service_se\n      0.030655\n      0.033327\n      0.030126\n    \n    \n      corporate_income_tax_impulse\n      0.107731\n      -0.094389\n      -0.033291\n    \n    \n      corporate_income_tax_se\n      0.071066\n      0.074045\n      0.058701\n    \n    \n      personal_income_tax_impulse\n      -0.123072\n      -0.011699\n      0.079664\n    \n    \n      personal_income_tax_se\n      0.081077\n      0.093111\n      0.078465\n    \n    \n      other_revenue_impulse\n      -0.150617\n      -0.053069\n      0.289039\n    \n    \n      other_revenue_se\n      0.227183\n      0.225741\n      0.202924\n    \n  \n\n\n\n\nSame patters as we’ve seen above, the results with the data error included are pretty close to what the paper reports.\n\n\nPlot the IRFs\nOk, this is the last bit I want to reproduce, just because IRF charts look cool so it would be a shame to leave them out.\n\nImage(filename=\"img/ferede_fig_3.png\")\n\n\n\n\n\nresults.irf().plot(impulse='nrri', response='program_expenditure');\n\n\n\n\n\nresults.irf().plot(impulse='nrrd', response='program_expenditure');\n\n\n\n\n\nresults_err.irf().plot(impulse='nrri', response='program_expenditure');\n\n\n\n\n\nresults_err.irf().plot(impulse='nrrd', response='program_expenditure');\n\n\n\n\nThese are harder to eyeball because I’m showing the individual responses and the paper is showing the cumulative ones. But again, if I think about what adding up the points in my charts would look like, they end up closer to the data set with the error."
  },
  {
    "objectID": "posts/2021-02-26-ferede.html#conclusion",
    "href": "posts/2021-02-26-ferede.html#conclusion",
    "title": "Alberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue",
    "section": "Conclusion",
    "text": "Conclusion\nThe initial goal of this exercise was to practice my python and econometrics. I’ve certainly done that over the course of working on it, but as a bonus I’ve also demonstrated how sensitive results can be to even a single data point (at least when you have relatively few samples."
  },
  {
    "objectID": "posts/2021-02-26-ferede.html#appendix-raw-tables",
    "href": "posts/2021-02-26-ferede.html#appendix-raw-tables",
    "title": "Alberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue",
    "section": "Appendix: Raw tables",
    "text": "Appendix: Raw tables\nFor the purposes of validation, here are the full tables I used to produce both the summary statistics above, as well as all statistical models and tests below\n\nmdfl\n\n\n\n\n\n  \n    \n      \n      program_expenditure\n      debt_service\n      corporate_income_tax\n      personal_income_tax\n      other_revenue\n      natural_resource_revenue\n      deficit\n      heritage_dummy\n      ur_lag\n      er_lag\n      cad_usd_lag\n    \n    \n      budget_dt\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1964-01-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1965-01-01\n      2327.844291\n      11.272854\n      174.729232\n      231.093501\n      1442.925275\n      1397.833860\n      -907.464724\n      0.0\n      2.500000\n      37.935748\n      NaN\n    \n    \n      1966-01-01\n      2891.984579\n      10.750872\n      145.136773\n      301.024417\n      1542.750138\n      1290.104645\n      -376.280521\n      0.0\n      2.500000\n      37.935748\n      NaN\n    \n    \n      1967-01-01\n      4436.875339\n      10.199713\n      203.994268\n      407.988537\n      2351.033944\n      1116.868620\n      367.189683\n      0.0\n      2.500000\n      37.935748\n      NaN\n    \n    \n      1968-01-01\n      4368.535726\n      19.160244\n      239.503055\n      469.425988\n      2639.323668\n      1360.377353\n      -320.934094\n      0.0\n      2.700000\n      38.322148\n      NaN\n    \n    \n      1969-01-01\n      4396.849967\n      17.909776\n      286.556413\n      599.977490\n      2444.684401\n      1141.748209\n      -58.206771\n      0.0\n      3.300000\n      39.041995\n      NaN\n    \n    \n      1970-01-01\n      4745.762712\n      38.135593\n      245.762712\n      771.186441\n      2737.288136\n      978.813559\n      50.847458\n      0.0\n      3.400000\n      39.833226\n      NaN\n    \n    \n      1971-01-01\n      4993.823780\n      67.916003\n      271.664014\n      767.051333\n      2848.477084\n      1090.651114\n      83.896240\n      0.0\n      5.200000\n      39.686520\n      NaN\n    \n    \n      1972-01-01\n      4988.477844\n      85.114978\n      362.663820\n      858.551083\n      2768.087112\n      1224.915554\n      -140.624746\n      0.0\n      5.700000\n      38.456938\n      1.009883\n    \n    \n      1973-01-01\n      5004.356719\n      81.151731\n      382.089398\n      977.202089\n      2387.213408\n      2160.664826\n      -821.661272\n      0.0\n      5.700000\n      39.470588\n      0.990792\n    \n    \n      1974-01-01\n      6124.378806\n      92.793618\n      829.155879\n      1038.689856\n      2352.767225\n      4549.880638\n      -2553.321174\n      0.0\n      5.300000\n      40.843443\n      1.000233\n    \n    \n      1975-01-01\n      7071.218716\n      84.149869\n      686.347373\n      925.648564\n      2721.722340\n      4417.868145\n      -1596.217836\n      0.0\n      3.500000\n      42.840909\n      0.978133\n    \n    \n      1976-01-01\n      6845.363213\n      59.113672\n      326.307469\n      1038.036080\n      2830.362613\n      -125.320985\n      2835.091707\n      1.0\n      4.200000\n      43.439912\n      1.017267\n    \n    \n      1977-01-01\n      7061.151803\n      46.247496\n      554.969954\n      1206.639219\n      2720.193639\n      4513.335196\n      -1887.738708\n      1.0\n      3.891667\n      64.800000\n      0.986075\n    \n    \n      1978-01-01\n      6825.084687\n      48.260049\n      545.709790\n      1225.062794\n      2643.165786\n      4701.642511\n      -2242.236144\n      1.0\n      4.475000\n      64.633333\n      1.063525\n    \n    \n      1979-01-01\n      9302.990086\n      32.774318\n      342.491620\n      1265.088664\n      2525.261181\n      5181.619632\n      21.303307\n      1.0\n      4.750000\n      65.341667\n      1.140767\n    \n    \n      1980-01-01\n      9937.742745\n      31.376340\n      620.395823\n      1337.773062\n      2676.972321\n      4797.727697\n      536.250183\n      1.0\n      3.966667\n      67.125000\n      1.171558\n    \n    \n      1981-01-01\n      10441.551444\n      109.885646\n      701.577586\n      1597.568239\n      5094.588359\n      4001.769572\n      -844.066666\n      1.0\n      3.866667\n      68.050000\n      1.169408\n    \n    \n      1982-01-01\n      12281.969722\n      57.716023\n      639.073783\n      1731.480694\n      4808.269418\n      2887.899921\n      2272.961929\n      1.0\n      3.883333\n      69.500000\n      1.198892\n    \n    \n      1983-01-01\n      11489.190564\n      168.596206\n      771.007210\n      1488.773513\n      4813.372379\n      4001.941517\n      582.692150\n      1.0\n      7.750000\n      66.275000\n      1.233858\n    \n    \n      1984-01-01\n      11320.287262\n      218.545766\n      793.666203\n      1396.584127\n      5529.782999\n      4306.693537\n      -487.893837\n      1.0\n      11.008333\n      63.816667\n      1.232583\n    \n    \n      1985-01-01\n      12869.198063\n      168.831114\n      723.561918\n      1410.945740\n      5622.447161\n      3939.701880\n      1341.372479\n      1.0\n      11.408333\n      63.933333\n      1.295150\n    \n    \n      1986-01-01\n      11863.166376\n      263.862833\n      351.817111\n      1570.739021\n      4940.542816\n      1488.115306\n      3775.814955\n      1.0\n      9.775000\n      65.491667\n      1.365900\n    \n    \n      1987-01-01\n      10791.884860\n      486.136253\n      505.683690\n      1900.350807\n      5480.081397\n      2231.807343\n      1160.097876\n      0.0\n      10.008333\n      65.350000\n      1.389767\n    \n    \n      1988-01-01\n      10774.989850\n      655.047057\n      572.857965\n      1675.835569\n      5818.165763\n      1713.642551\n      1649.535059\n      0.0\n      9.525000\n      65.450000\n      1.326175\n    \n    \n      1989-01-01\n      10745.455671\n      865.692419\n      543.484030\n      1968.964999\n      5716.675586\n      1739.148895\n      1642.874581\n      0.0\n      7.983333\n      66.925000\n      1.230942\n    \n    \n      1990-01-01\n      10614.752643\n      923.523101\n      578.462597\n      2014.173627\n      5689.536231\n      1936.372929\n      1319.730359\n      0.0\n      7.158333\n      67.516667\n      1.184108\n    \n    \n      1991-01-01\n      10060.915107\n      878.291420\n      488.608088\n      2043.330951\n      5298.490171\n      1351.526066\n      1757.251250\n      0.0\n      6.916667\n      67.500000\n      1.167017\n    \n    \n      1992-01-01\n      10490.255898\n      920.232018\n      413.099221\n      1811.929709\n      5614.128666\n      1415.691681\n      2155.638638\n      0.0\n      8.250000\n      66.591667\n      1.145975\n    \n    \n      1993-01-01\n      9567.673764\n      1046.414892\n      540.289188\n      1820.154561\n      5604.076850\n      1782.195133\n      867.372924\n      0.0\n      9.458333\n      65.383333\n      1.208808\n    \n    \n      1994-01-01\n      8304.447335\n      1075.635389\n      661.029079\n      1886.982358\n      5328.892392\n      2081.040289\n      -577.861395\n      0.0\n      9.583333\n      64.875000\n      1.290167\n    \n    \n      1995-01-01\n      7542.256294\n      1000.994980\n      792.231321\n      1889.578759\n      4888.995090\n      1657.024370\n      -684.578266\n      0.0\n      8.783333\n      65.725000\n      1.365892\n    \n    \n      1996-01-01\n      7286.510699\n      838.743299\n      807.190029\n      1976.382124\n      4455.321793\n      2314.288966\n      -1427.928914\n      0.0\n      7.841667\n      66.650000\n      1.372650\n    \n    \n      1997-01-01\n      7598.400009\n      729.331650\n      1020.071271\n      2138.894710\n      4551.426710\n      2084.277589\n      -1466.938621\n      0.0\n      6.883333\n      67.316667\n      1.363700\n    \n    \n      1998-01-01\n      7647.600932\n      735.120709\n      884.383797\n      2452.712386\n      4366.478407\n      1262.339259\n      -583.192208\n      0.0\n      5.875000\n      67.925000\n      1.384867\n    \n    \n      1999-01-01\n      8342.393922\n      487.608742\n      640.113987\n      2601.260027\n      4640.443868\n      2371.737084\n      -1423.552301\n      0.0\n      5.583333\n      68.566667\n      1.483633\n    \n    \n      2000-01-01\n      8710.352822\n      474.863472\n      980.253881\n      1910.598641\n      4348.877202\n      5129.494603\n      -3184.008033\n      0.0\n      5.666667\n      68.500000\n      1.485825\n    \n    \n      2001-01-01\n      9338.287124\n      360.113309\n      1037.070500\n      1946.193764\n      4320.894451\n      2897.190669\n      -502.948950\n      0.0\n      4.966667\n      68.675000\n      1.485517\n    \n    \n      2002-01-01\n      8826.496403\n      209.515399\n      888.679810\n      2127.725707\n      3820.134757\n      3138.329395\n      -938.857868\n      0.0\n      4.725000\n      68.975000\n      1.549017\n    \n    \n      2003-01-01\n      8889.845744\n      112.157737\n      701.917057\n      1909.164731\n      4925.835384\n      3176.836868\n      -1711.750559\n      0.0\n      5.341667\n      68.975000\n      1.570592\n    \n    \n      2004-01-01\n      9563.476337\n      121.092191\n      947.887219\n      1864.098004\n      5040.562703\n      3907.027522\n      -2075.006920\n      0.0\n      5.116667\n      69.741667\n      1.401167\n    \n    \n      2005-01-01\n      10252.960247\n      95.080363\n      1118.344428\n      1793.108293\n      5214.467798\n      4829.545684\n      -2607.425593\n      1.0\n      4.591667\n      70.125000\n      1.301575\n    \n    \n      2006-01-01\n      10491.526962\n      77.006633\n      1291.562414\n      2729.974686\n      5203.857546\n      3943.455955\n      -2600.317006\n      1.0\n      3.941667\n      69.850000\n      1.211483\n    \n    \n      2007-01-01\n      11093.119793\n      71.131049\n      1560.562037\n      2749.181812\n      4712.930591\n      3359.113940\n      -1217.537538\n      1.0\n      3.458333\n      70.791667\n      1.134375\n    \n    \n      2008-01-01\n      11481.223206\n      65.508008\n      1339.134853\n      2742.517945\n      3444.209491\n      3752.538047\n      268.330878\n      0.0\n      3.566667\n      71.383333\n      1.074183\n    \n    \n      2009-01-01\n      11192.177199\n      111.838586\n      1464.684956\n      2426.866512\n      5009.321141\n      2085.188848\n      317.954328\n      0.0\n      3.691667\n      71.800000\n      1.066767\n    \n    \n      2010-01-01\n      11408.370379\n      141.808459\n      1001.672465\n      2292.670240\n      4699.207866\n      2532.122236\n      1024.506031\n      0.0\n      6.575000\n      69.416667\n      1.141442\n    \n    \n      2011-01-01\n      11288.615235\n      144.188676\n      1062.777455\n      2474.323912\n      4526.773140\n      3362.283433\n      6.645971\n      0.0\n      6.516667\n      68.133333\n      1.030125\n    \n    \n      2012-01-01\n      11491.360391\n      143.201492\n      1330.207614\n      2690.901483\n      4642.860889\n      2175.711738\n      794.880159\n      0.0\n      5.433333\n      69.508333\n      0.989025\n    \n    \n      2013-01-01\n      11803.343431\n      158.459375\n      1473.940765\n      2829.977012\n      5288.245930\n      2572.413384\n      -202.774285\n      0.0\n      4.700000\n      69.958333\n      0.999408\n    \n    \n      2014-01-01\n      11006.721496\n      182.199224\n      1479.028999\n      2817.708454\n      4975.008515\n      2283.359468\n      -366.184716\n      0.0\n      4.658333\n      69.591667\n      1.029992\n    \n    \n      2015-01-01\n      10664.784806\n      176.142114\n      1040.727000\n      2817.529567\n      4492.864354\n      691.915996\n      1797.890004\n      0.0\n      4.775000\n      69.058333\n      1.104683\n    \n    \n      2016-01-01\n      11322.810385\n      230.029051\n      913.571647\n      2608.854241\n      4407.637789\n      750.684901\n      2872.090858\n      0.0\n      6.091667\n      68.408333\n      1.278808\n    \n    \n      2017-01-01\n      11433.853017\n      319.856119\n      813.921696\n      2543.505301\n      5094.564399\n      1175.559759\n      2126.157981\n      0.0\n      8.133333\n      66.250000\n      1.325583\n    \n    \n      2018-01-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      7.883333\n      66.316667\n      1.297858\n    \n  \n\n\n\n\n\nmodel_df_first_diff(mdfl)\n\n\n\n\n\n  \n    \n      \n      natural_resource_revenue\n      nrri\n      nrrd\n      corporate_income_tax\n      personal_income_tax\n      other_revenue\n      debt_service\n      program_expenditure\n      deficit\n      ur_lag\n      er_lag\n      cad_usd_lag\n      heritage_dummy\n      constant\n    \n    \n      budget_dt\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1970-01-01\n      -162.934650\n      0.000000\n      -162.934650\n      -40.793701\n      171.208950\n      292.603734\n      20.225817\n      348.912745\n      109.054229\n      0.100000\n      0.791232\n      NaN\n      0.0\n      1\n    \n    \n      1971-01-01\n      111.837554\n      111.837554\n      0.000000\n      25.901302\n      -4.135108\n      111.188949\n      29.780410\n      248.061068\n      33.048782\n      1.800000\n      -0.146706\n      NaN\n      0.0\n      1\n    \n    \n      1972-01-01\n      134.264440\n      134.264440\n      0.000000\n      90.999806\n      91.499750\n      -80.389972\n      17.198975\n      -5.345936\n      -224.520986\n      0.500000\n      -1.229583\n      NaN\n      0.0\n      1\n    \n    \n      1973-01-01\n      935.749273\n      935.749273\n      0.000000\n      19.425579\n      118.651006\n      -380.873704\n      -3.963247\n      15.878875\n      -681.036526\n      0.000000\n      1.013650\n      -0.019092\n      0.0\n      1\n    \n    \n      1974-01-01\n      2389.215812\n      2389.215812\n      0.000000\n      447.066481\n      61.487767\n      -34.446183\n      11.641888\n      1120.022088\n      -1731.659902\n      -0.400000\n      1.372855\n      0.009442\n      0.0\n      1\n    \n    \n      1975-01-01\n      -132.012493\n      0.000000\n      -132.012493\n      -142.808507\n      -113.041292\n      368.955115\n      -8.643749\n      946.839909\n      957.103338\n      -1.800000\n      1.997466\n      -0.022100\n      0.0\n      1\n    \n    \n      1976-01-01\n      -4543.189130\n      0.000000\n      -4543.189130\n      -360.039903\n      112.387516\n      108.640274\n      -25.036197\n      -225.855503\n      4431.309543\n      0.700000\n      0.599003\n      0.039133\n      1.0\n      1\n    \n    \n      1977-01-01\n      4638.656180\n      4638.656180\n      0.000000\n      228.662485\n      168.603139\n      -110.168974\n      -12.866176\n      215.788591\n      -4722.830415\n      -0.308333\n      21.360088\n      -0.031192\n      1.0\n      1\n    \n    \n      1978-01-01\n      188.307315\n      188.307315\n      0.000000\n      -9.260164\n      18.423575\n      -77.027853\n      2.012553\n      -236.067116\n      -354.497436\n      0.583333\n      -0.166667\n      0.077450\n      1.0\n      1\n    \n    \n      1979-01-01\n      479.977121\n      479.977121\n      0.000000\n      -203.218170\n      40.025870\n      -117.904605\n      -15.485732\n      2477.905398\n      2263.539451\n      0.275000\n      0.708333\n      0.077242\n      1.0\n      1\n    \n    \n      1980-01-01\n      -383.891935\n      0.000000\n      -383.891935\n      277.904203\n      72.684398\n      151.711140\n      -1.397977\n      634.752660\n      514.946876\n      -0.783333\n      1.783333\n      0.030792\n      1.0\n      1\n    \n    \n      1981-01-01\n      -795.958126\n      0.000000\n      -795.958126\n      81.181763\n      259.795177\n      2417.616038\n      78.509306\n      503.808699\n      -1380.316848\n      -0.100000\n      0.925000\n      -0.002150\n      1.0\n      1\n    \n    \n      1982-01-01\n      -1113.869651\n      0.000000\n      -1113.869651\n      -62.503803\n      133.912455\n      -286.318941\n      -52.169623\n      1840.418278\n      3117.028595\n      0.016667\n      1.450000\n      0.029483\n      1.0\n      1\n    \n    \n      1983-01-01\n      1114.041596\n      1114.041596\n      0.000000\n      131.933427\n      -242.707181\n      5.102961\n      110.880183\n      -792.779158\n      -1690.269779\n      3.866667\n      -3.225000\n      0.034967\n      1.0\n      1\n    \n    \n      1984-01-01\n      304.752020\n      304.752020\n      0.000000\n      22.658992\n      -92.189386\n      716.410620\n      49.949560\n      -168.903301\n      -1070.585987\n      3.258333\n      -2.458333\n      -0.001275\n      1.0\n      1\n    \n    \n      1985-01-01\n      -366.991657\n      0.000000\n      -366.991657\n      -70.104285\n      14.361613\n      92.664162\n      -49.714652\n      1548.910801\n      1829.266316\n      0.400000\n      0.116667\n      0.062567\n      1.0\n      1\n    \n    \n      1986-01-01\n      -2451.586574\n      0.000000\n      -2451.586574\n      -371.744807\n      159.793281\n      -681.904345\n      95.031719\n      -1006.031688\n      2434.442476\n      -1.633333\n      1.558333\n      0.070750\n      1.0\n      1\n    \n    \n      1987-01-01\n      743.692037\n      743.692037\n      0.000000\n      153.866579\n      329.611786\n      539.538581\n      222.273420\n      -1071.281516\n      -2615.717079\n      0.233333\n      -0.141667\n      0.023867\n      0.0\n      1\n    \n    \n      1988-01-01\n      -518.164793\n      0.000000\n      -518.164793\n      67.174276\n      -224.515238\n      338.084366\n      168.910804\n      -16.895010\n      489.437183\n      -0.483333\n      0.100000\n      -0.063592\n      0.0\n      1\n    \n    \n      1989-01-01\n      25.506344\n      25.506344\n      0.000000\n      -29.373936\n      293.129430\n      -101.490177\n      210.645362\n      -29.534180\n      -6.660479\n      -1.541667\n      1.475000\n      -0.095233\n      0.0\n      1\n    \n    \n      1990-01-01\n      197.224034\n      197.224034\n      0.000000\n      34.978568\n      45.208628\n      -27.139354\n      57.830682\n      -130.703028\n      -323.144222\n      -0.825000\n      0.591667\n      -0.046833\n      0.0\n      1\n    \n    \n      1991-01-01\n      -584.846863\n      0.000000\n      -584.846863\n      -89.854509\n      29.157325\n      -391.046061\n      -45.231681\n      -553.837536\n      437.520891\n      -0.241667\n      -0.016667\n      -0.017092\n      0.0\n      1\n    \n    \n      1992-01-01\n      64.165615\n      64.165615\n      0.000000\n      -75.508867\n      -231.401242\n      315.638496\n      41.940598\n      429.340791\n      398.387388\n      1.333333\n      -0.908333\n      -0.021042\n      0.0\n      1\n    \n    \n      1993-01-01\n      366.503452\n      366.503452\n      0.000000\n      127.189967\n      8.224851\n      -10.051816\n      126.182874\n      -922.582134\n      -1288.265714\n      1.208333\n      -1.208333\n      0.062833\n      0.0\n      1\n    \n    \n      1994-01-01\n      298.845156\n      298.845156\n      0.000000\n      120.739891\n      66.827798\n      -275.184458\n      29.220498\n      -1263.226429\n      -1445.234319\n      0.125000\n      -0.508333\n      0.081358\n      0.0\n      1\n    \n    \n      1995-01-01\n      -424.015919\n      0.000000\n      -424.015919\n      131.202242\n      2.596401\n      -439.897302\n      -74.640409\n      -762.191041\n      -106.716872\n      -0.800000\n      0.850000\n      0.075725\n      0.0\n      1\n    \n    \n      1996-01-01\n      657.264596\n      657.264596\n      0.000000\n      14.958707\n      86.803365\n      -433.673297\n      -162.251681\n      -255.745595\n      -743.350647\n      -0.941667\n      0.925000\n      0.006758\n      0.0\n      1\n    \n    \n      1997-01-01\n      -230.011377\n      0.000000\n      -230.011377\n      212.881242\n      162.512585\n      96.104917\n      -109.411649\n      311.889310\n      -39.009707\n      -0.958333\n      0.666667\n      -0.008950\n      0.0\n      1\n    \n    \n      1998-01-01\n      -821.938330\n      0.000000\n      -821.938330\n      -135.687474\n      313.817676\n      -184.948302\n      5.789059\n      49.200923\n      883.746412\n      -1.008333\n      0.608333\n      0.021167\n      0.0\n      1\n    \n    \n      1999-01-01\n      1109.397825\n      1109.397825\n      0.000000\n      -244.269810\n      148.547641\n      273.965461\n      -247.511966\n      694.792990\n      -840.360093\n      -0.291667\n      0.641667\n      0.098767\n      0.0\n      1\n    \n    \n      2000-01-01\n      2757.757519\n      2757.757519\n      0.000000\n      340.139894\n      -690.661386\n      -291.566666\n      -12.745271\n      367.958900\n      -1760.455732\n      0.083333\n      -0.066667\n      0.002192\n      0.0\n      1\n    \n    \n      2001-01-01\n      -2232.303933\n      0.000000\n      -2232.303933\n      56.816619\n      35.595123\n      -27.982751\n      -114.750162\n      627.934302\n      2681.059082\n      -0.700000\n      0.175000\n      -0.000308\n      0.0\n      1\n    \n    \n      2002-01-01\n      241.138725\n      241.138725\n      0.000000\n      -148.390689\n      181.531943\n      -500.759694\n      -150.597911\n      -511.790721\n      -435.908918\n      -0.241667\n      0.300000\n      0.063500\n      0.0\n      1\n    \n    \n      2003-01-01\n      38.507474\n      38.507474\n      0.000000\n      -186.762753\n      -218.560977\n      1105.700627\n      -97.357661\n      63.349341\n      -772.892691\n      0.616667\n      0.000000\n      0.021575\n      0.0\n      1\n    \n    \n      2004-01-01\n      730.190654\n      730.190654\n      0.000000\n      245.970162\n      -45.066727\n      114.727320\n      8.934454\n      673.630593\n      -363.256361\n      -0.225000\n      0.766667\n      -0.169425\n      0.0\n      1\n    \n    \n      2005-01-01\n      922.518162\n      922.518162\n      0.000000\n      170.457209\n      -70.989711\n      173.905095\n      -26.011829\n      689.483910\n      -532.418673\n      -0.525000\n      0.383333\n      -0.099592\n      1.0\n      1\n    \n    \n      2006-01-01\n      -886.089729\n      0.000000\n      -886.089729\n      173.217986\n      936.866393\n      -10.610252\n      -18.073730\n      238.566715\n      7.108587\n      -0.650000\n      -0.275000\n      -0.090092\n      1.0\n      1\n    \n    \n      2007-01-01\n      -584.342016\n      0.000000\n      -584.342016\n      268.999623\n      19.207126\n      -490.926955\n      -5.875584\n      601.592831\n      1382.779468\n      -0.483333\n      0.941667\n      -0.077108\n      1.0\n      1\n    \n    \n      2008-01-01\n      393.424107\n      393.424107\n      0.000000\n      -221.427184\n      -6.663868\n      -1268.721100\n      -5.623041\n      388.103413\n      1485.868417\n      0.108333\n      0.591667\n      -0.060192\n      0.0\n      1\n    \n    \n      2009-01-01\n      -1667.349199\n      0.000000\n      -1667.349199\n      125.550103\n      -315.651432\n      1565.111650\n      46.330578\n      -289.046007\n      49.623450\n      0.125000\n      0.416667\n      -0.007417\n      0.0\n      1\n    \n    \n      2010-01-01\n      446.933388\n      446.933388\n      0.000000\n      -463.012491\n      -134.196272\n      -310.113274\n      29.969873\n      216.193180\n      706.551703\n      2.883333\n      -2.383333\n      0.074675\n      0.0\n      1\n    \n    \n      2011-01-01\n      830.161197\n      830.161197\n      0.000000\n      61.104990\n      181.653671\n      -172.434726\n      2.380217\n      -119.755144\n      -1017.860060\n      -0.058333\n      -1.283333\n      -0.111317\n      0.0\n      1\n    \n    \n      2012-01-01\n      -1186.571695\n      0.000000\n      -1186.571695\n      267.430159\n      216.577571\n      116.087749\n      -0.987183\n      202.745156\n      788.234188\n      -1.083333\n      1.375000\n      -0.041100\n      0.0\n      1\n    \n    \n      2013-01-01\n      396.701645\n      396.701645\n      0.000000\n      143.733151\n      139.075529\n      645.385041\n      15.257883\n      311.983040\n      -997.654445\n      -0.733333\n      0.450000\n      0.010383\n      0.0\n      1\n    \n    \n      2014-01-01\n      -289.053915\n      0.000000\n      -289.053915\n      5.088233\n      -12.268558\n      -313.237415\n      23.739849\n      -796.621934\n      -163.410431\n      -0.041667\n      -0.366667\n      0.030583\n      0.0\n      1\n    \n    \n      2015-01-01\n      -1591.443473\n      0.000000\n      -1591.443473\n      -438.301999\n      -0.178887\n      -482.144162\n      -6.057110\n      -341.936691\n      2164.074720\n      0.116667\n      -0.533333\n      0.074692\n      0.0\n      1\n    \n    \n      2016-01-01\n      58.768905\n      58.768905\n      0.000000\n      -127.155353\n      -208.675326\n      -85.226565\n      53.886937\n      658.025579\n      1074.200854\n      1.316667\n      -0.650000\n      0.174125\n      0.0\n      1\n    \n  \n\n\n\n\n\nmdfl_err\n\n\n\n\n\n  \n    \n      \n      program_expenditure\n      debt_service\n      corporate_income_tax\n      personal_income_tax\n      other_revenue\n      natural_resource_revenue\n      deficit\n      heritage_dummy\n      ur_lag\n      er_lag\n      cad_usd_lag\n    \n    \n      budget_dt\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1964-01-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1965-01-01\n      2327.844291\n      11.272854\n      174.729232\n      231.093501\n      1442.925275\n      1397.833860\n      -907.464724\n      0.0\n      2.500000\n      37.935748\n      NaN\n    \n    \n      1966-01-01\n      2891.984579\n      10.750872\n      145.136773\n      301.024417\n      1542.750138\n      1290.104645\n      -376.280521\n      0.0\n      2.500000\n      37.935748\n      NaN\n    \n    \n      1967-01-01\n      4436.875339\n      10.199713\n      203.994268\n      407.988537\n      2351.033944\n      1116.868620\n      367.189683\n      0.0\n      2.500000\n      37.935748\n      NaN\n    \n    \n      1968-01-01\n      4368.535726\n      19.160244\n      239.503055\n      469.425988\n      2639.323668\n      1360.377353\n      -320.934094\n      0.0\n      2.700000\n      38.322148\n      NaN\n    \n    \n      1969-01-01\n      4396.849967\n      17.909776\n      286.556413\n      599.977490\n      2444.684401\n      1141.748209\n      -58.206771\n      0.0\n      3.300000\n      39.041995\n      NaN\n    \n    \n      1970-01-01\n      4745.762712\n      38.135593\n      245.762712\n      771.186441\n      2737.288136\n      978.813559\n      50.847458\n      0.0\n      3.400000\n      39.833226\n      NaN\n    \n    \n      1971-01-01\n      4993.823780\n      67.916003\n      271.664014\n      767.051333\n      2848.477084\n      1090.651114\n      83.896240\n      0.0\n      5.200000\n      39.686520\n      NaN\n    \n    \n      1972-01-01\n      4988.477844\n      85.114978\n      362.663820\n      858.551083\n      2768.087112\n      1224.915554\n      -140.624746\n      0.0\n      5.700000\n      38.456938\n      1.009883\n    \n    \n      1973-01-01\n      5004.356719\n      81.151731\n      382.089398\n      977.202089\n      2387.213408\n      2160.664826\n      -821.661272\n      0.0\n      5.700000\n      39.470588\n      0.990792\n    \n    \n      1974-01-01\n      6124.378806\n      92.793618\n      829.155879\n      1038.689856\n      2352.767225\n      4549.880638\n      -2553.321174\n      0.0\n      5.300000\n      40.843443\n      1.000233\n    \n    \n      1975-01-01\n      7071.218716\n      84.149869\n      686.347373\n      925.648564\n      2721.722340\n      4417.868145\n      -1596.217836\n      0.0\n      3.500000\n      42.840909\n      0.978133\n    \n    \n      1976-01-01\n      6845.363213\n      59.113672\n      326.307469\n      1038.036080\n      2830.362613\n      4887.518397\n      -2177.747675\n      1.0\n      4.200000\n      43.439912\n      1.017267\n    \n    \n      1977-01-01\n      7061.151803\n      46.247496\n      554.969954\n      1206.639219\n      2720.193639\n      4513.335196\n      -1887.738708\n      1.0\n      3.891667\n      64.800000\n      0.986075\n    \n    \n      1978-01-01\n      6825.084687\n      48.260049\n      545.709790\n      1225.062794\n      2643.165786\n      4701.642511\n      -2242.236144\n      1.0\n      4.475000\n      64.633333\n      1.063525\n    \n    \n      1979-01-01\n      9302.990086\n      32.774318\n      342.491620\n      1265.088664\n      2525.261181\n      5181.619632\n      21.303307\n      1.0\n      4.750000\n      65.341667\n      1.140767\n    \n    \n      1980-01-01\n      9937.742745\n      31.376340\n      620.395823\n      1337.773062\n      2676.972321\n      4797.727697\n      536.250183\n      1.0\n      3.966667\n      67.125000\n      1.171558\n    \n    \n      1981-01-01\n      10441.551444\n      109.885646\n      701.577586\n      1597.568239\n      5094.588359\n      4001.769572\n      -844.066666\n      1.0\n      3.866667\n      68.050000\n      1.169408\n    \n    \n      1982-01-01\n      12281.969722\n      57.716023\n      639.073783\n      1731.480694\n      4808.269418\n      2887.899921\n      2272.961929\n      1.0\n      3.883333\n      69.500000\n      1.198892\n    \n    \n      1983-01-01\n      11489.190564\n      168.596206\n      771.007210\n      1488.773513\n      4813.372379\n      4001.941517\n      582.692150\n      1.0\n      7.750000\n      66.275000\n      1.233858\n    \n    \n      1984-01-01\n      11320.287262\n      218.545766\n      793.666203\n      1396.584127\n      5529.782999\n      4306.693537\n      -487.893837\n      1.0\n      11.008333\n      63.816667\n      1.232583\n    \n    \n      1985-01-01\n      12869.198063\n      168.831114\n      723.561918\n      1410.945740\n      5622.447161\n      3939.701880\n      1341.372479\n      1.0\n      11.408333\n      63.933333\n      1.295150\n    \n    \n      1986-01-01\n      11863.166376\n      263.862833\n      351.817111\n      1570.739021\n      4940.542816\n      1488.115306\n      3775.814955\n      1.0\n      9.775000\n      65.491667\n      1.365900\n    \n    \n      1987-01-01\n      10791.884860\n      486.136253\n      505.683690\n      1900.350807\n      5480.081397\n      2231.807343\n      1160.097876\n      0.0\n      10.008333\n      65.350000\n      1.389767\n    \n    \n      1988-01-01\n      10774.989850\n      655.047057\n      572.857965\n      1675.835569\n      5818.165763\n      1713.642551\n      1649.535059\n      0.0\n      9.525000\n      65.450000\n      1.326175\n    \n    \n      1989-01-01\n      10745.455671\n      865.692419\n      543.484030\n      1968.964999\n      5716.675586\n      1739.148895\n      1642.874581\n      0.0\n      7.983333\n      66.925000\n      1.230942\n    \n    \n      1990-01-01\n      10614.752643\n      923.523101\n      578.462597\n      2014.173627\n      5689.536231\n      1936.372929\n      1319.730359\n      0.0\n      7.158333\n      67.516667\n      1.184108\n    \n    \n      1991-01-01\n      10060.915107\n      878.291420\n      488.608088\n      2043.330951\n      5298.490171\n      1351.526066\n      1757.251250\n      0.0\n      6.916667\n      67.500000\n      1.167017\n    \n    \n      1992-01-01\n      10490.255898\n      920.232018\n      413.099221\n      1811.929709\n      5614.128666\n      1415.691681\n      2155.638638\n      0.0\n      8.250000\n      66.591667\n      1.145975\n    \n    \n      1993-01-01\n      9567.673764\n      1046.414892\n      540.289188\n      1820.154561\n      5604.076850\n      1782.195133\n      867.372924\n      0.0\n      9.458333\n      65.383333\n      1.208808\n    \n    \n      1994-01-01\n      8304.447335\n      1075.635389\n      661.029079\n      1886.982358\n      5328.892392\n      2081.040289\n      -577.861395\n      0.0\n      9.583333\n      64.875000\n      1.290167\n    \n    \n      1995-01-01\n      7542.256294\n      1000.994980\n      792.231321\n      1889.578759\n      4888.995090\n      1657.024370\n      -684.578266\n      0.0\n      8.783333\n      65.725000\n      1.365892\n    \n    \n      1996-01-01\n      7286.510699\n      838.743299\n      807.190029\n      1976.382124\n      4455.321793\n      2314.288966\n      -1427.928914\n      0.0\n      7.841667\n      66.650000\n      1.372650\n    \n    \n      1997-01-01\n      7598.400009\n      729.331650\n      1020.071271\n      2138.894710\n      4551.426710\n      2084.277589\n      -1466.938621\n      0.0\n      6.883333\n      67.316667\n      1.363700\n    \n    \n      1998-01-01\n      7647.600932\n      735.120709\n      884.383797\n      2452.712386\n      4366.478407\n      1262.339259\n      -583.192208\n      0.0\n      5.875000\n      67.925000\n      1.384867\n    \n    \n      1999-01-01\n      8342.393922\n      487.608742\n      640.113987\n      2601.260027\n      4640.443868\n      2371.737084\n      -1423.552301\n      0.0\n      5.583333\n      68.566667\n      1.483633\n    \n    \n      2000-01-01\n      8710.352822\n      474.863472\n      980.253881\n      1910.598641\n      4348.877202\n      5129.494603\n      -3184.008033\n      0.0\n      5.666667\n      68.500000\n      1.485825\n    \n    \n      2001-01-01\n      9338.287124\n      360.113309\n      1037.070500\n      1946.193764\n      4320.894451\n      2897.190669\n      -502.948950\n      0.0\n      4.966667\n      68.675000\n      1.485517\n    \n    \n      2002-01-01\n      8826.496403\n      209.515399\n      888.679810\n      2127.725707\n      3820.134757\n      3138.329395\n      -938.857868\n      0.0\n      4.725000\n      68.975000\n      1.549017\n    \n    \n      2003-01-01\n      8889.845744\n      112.157737\n      701.917057\n      1909.164731\n      4925.835384\n      3176.836868\n      -1711.750559\n      0.0\n      5.341667\n      68.975000\n      1.570592\n    \n    \n      2004-01-01\n      9563.476337\n      121.092191\n      947.887219\n      1864.098004\n      5040.562703\n      3907.027522\n      -2075.006920\n      0.0\n      5.116667\n      69.741667\n      1.401167\n    \n    \n      2005-01-01\n      10252.960247\n      95.080363\n      1118.344428\n      1793.108293\n      5214.467798\n      4829.545684\n      -2607.425593\n      1.0\n      4.591667\n      70.125000\n      1.301575\n    \n    \n      2006-01-01\n      10491.526962\n      77.006633\n      1291.562414\n      2729.974686\n      5203.857546\n      3943.455955\n      -2600.317006\n      1.0\n      3.941667\n      69.850000\n      1.211483\n    \n    \n      2007-01-01\n      11093.119793\n      71.131049\n      1560.562037\n      2749.181812\n      4712.930591\n      3359.113940\n      -1217.537538\n      1.0\n      3.458333\n      70.791667\n      1.134375\n    \n    \n      2008-01-01\n      11481.223206\n      65.508008\n      1339.134853\n      2742.517945\n      3444.209491\n      3752.538047\n      268.330878\n      0.0\n      3.566667\n      71.383333\n      1.074183\n    \n    \n      2009-01-01\n      11192.177199\n      111.838586\n      1464.684956\n      2426.866512\n      5009.321141\n      2085.188848\n      317.954328\n      0.0\n      3.691667\n      71.800000\n      1.066767\n    \n    \n      2010-01-01\n      11408.370379\n      141.808459\n      1001.672465\n      2292.670240\n      4699.207866\n      2532.122236\n      1024.506031\n      0.0\n      6.575000\n      69.416667\n      1.141442\n    \n    \n      2011-01-01\n      11288.615235\n      144.188676\n      1062.777455\n      2474.323912\n      4526.773140\n      3362.283433\n      6.645971\n      0.0\n      6.516667\n      68.133333\n      1.030125\n    \n    \n      2012-01-01\n      11491.360391\n      143.201492\n      1330.207614\n      2690.901483\n      4642.860889\n      2175.711738\n      794.880159\n      0.0\n      5.433333\n      69.508333\n      0.989025\n    \n    \n      2013-01-01\n      11803.343431\n      158.459375\n      1473.940765\n      2829.977012\n      5288.245930\n      2572.413384\n      -202.774285\n      0.0\n      4.700000\n      69.958333\n      0.999408\n    \n    \n      2014-01-01\n      11006.721496\n      182.199224\n      1479.028999\n      2817.708454\n      4975.008515\n      2283.359468\n      -366.184716\n      0.0\n      4.658333\n      69.591667\n      1.029992\n    \n    \n      2015-01-01\n      10664.784806\n      176.142114\n      1040.727000\n      2817.529567\n      4492.864354\n      691.915996\n      1797.890004\n      0.0\n      4.775000\n      69.058333\n      1.104683\n    \n    \n      2016-01-01\n      11322.810385\n      230.029051\n      913.571647\n      2608.854241\n      4407.637789\n      750.684901\n      2872.090858\n      0.0\n      6.091667\n      68.408333\n      1.278808\n    \n    \n      2017-01-01\n      11433.853017\n      319.856119\n      813.921696\n      2543.505301\n      5094.564399\n      1175.559759\n      2126.157981\n      0.0\n      8.133333\n      66.250000\n      1.325583\n    \n    \n      2018-01-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      7.883333\n      66.316667\n      1.297858\n    \n  \n\n\n\n\n\nmodel_df_first_diff(mdfl_err)\n\n\n\n\n\n  \n    \n      \n      natural_resource_revenue\n      nrri\n      nrrd\n      corporate_income_tax\n      personal_income_tax\n      other_revenue\n      debt_service\n      program_expenditure\n      deficit\n      ur_lag\n      er_lag\n      cad_usd_lag\n      heritage_dummy\n      constant\n    \n    \n      budget_dt\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1970-01-01\n      -162.934650\n      0.000000\n      -162.934650\n      -40.793701\n      171.208950\n      292.603734\n      20.225817\n      348.912745\n      109.054229\n      0.100000\n      0.791232\n      NaN\n      0.0\n      1\n    \n    \n      1971-01-01\n      111.837554\n      111.837554\n      0.000000\n      25.901302\n      -4.135108\n      111.188949\n      29.780410\n      248.061068\n      33.048782\n      1.800000\n      -0.146706\n      NaN\n      0.0\n      1\n    \n    \n      1972-01-01\n      134.264440\n      134.264440\n      0.000000\n      90.999806\n      91.499750\n      -80.389972\n      17.198975\n      -5.345936\n      -224.520986\n      0.500000\n      -1.229583\n      NaN\n      0.0\n      1\n    \n    \n      1973-01-01\n      935.749273\n      935.749273\n      0.000000\n      19.425579\n      118.651006\n      -380.873704\n      -3.963247\n      15.878875\n      -681.036526\n      0.000000\n      1.013650\n      -0.019092\n      0.0\n      1\n    \n    \n      1974-01-01\n      2389.215812\n      2389.215812\n      0.000000\n      447.066481\n      61.487767\n      -34.446183\n      11.641888\n      1120.022088\n      -1731.659902\n      -0.400000\n      1.372855\n      0.009442\n      0.0\n      1\n    \n    \n      1975-01-01\n      -132.012493\n      0.000000\n      -132.012493\n      -142.808507\n      -113.041292\n      368.955115\n      -8.643749\n      946.839909\n      957.103338\n      -1.800000\n      1.997466\n      -0.022100\n      0.0\n      1\n    \n    \n      1976-01-01\n      469.650252\n      469.650252\n      0.000000\n      -360.039903\n      112.387516\n      108.640274\n      -25.036197\n      -225.855503\n      -581.529839\n      0.700000\n      0.599003\n      0.039133\n      1.0\n      1\n    \n    \n      1977-01-01\n      -374.183202\n      0.000000\n      -374.183202\n      228.662485\n      168.603139\n      -110.168974\n      -12.866176\n      215.788591\n      290.008967\n      -0.308333\n      21.360088\n      -0.031192\n      1.0\n      1\n    \n    \n      1978-01-01\n      188.307315\n      188.307315\n      0.000000\n      -9.260164\n      18.423575\n      -77.027853\n      2.012553\n      -236.067116\n      -354.497436\n      0.583333\n      -0.166667\n      0.077450\n      1.0\n      1\n    \n    \n      1979-01-01\n      479.977121\n      479.977121\n      0.000000\n      -203.218170\n      40.025870\n      -117.904605\n      -15.485732\n      2477.905398\n      2263.539451\n      0.275000\n      0.708333\n      0.077242\n      1.0\n      1\n    \n    \n      1980-01-01\n      -383.891935\n      0.000000\n      -383.891935\n      277.904203\n      72.684398\n      151.711140\n      -1.397977\n      634.752660\n      514.946876\n      -0.783333\n      1.783333\n      0.030792\n      1.0\n      1\n    \n    \n      1981-01-01\n      -795.958126\n      0.000000\n      -795.958126\n      81.181763\n      259.795177\n      2417.616038\n      78.509306\n      503.808699\n      -1380.316848\n      -0.100000\n      0.925000\n      -0.002150\n      1.0\n      1\n    \n    \n      1982-01-01\n      -1113.869651\n      0.000000\n      -1113.869651\n      -62.503803\n      133.912455\n      -286.318941\n      -52.169623\n      1840.418278\n      3117.028595\n      0.016667\n      1.450000\n      0.029483\n      1.0\n      1\n    \n    \n      1983-01-01\n      1114.041596\n      1114.041596\n      0.000000\n      131.933427\n      -242.707181\n      5.102961\n      110.880183\n      -792.779158\n      -1690.269779\n      3.866667\n      -3.225000\n      0.034967\n      1.0\n      1\n    \n    \n      1984-01-01\n      304.752020\n      304.752020\n      0.000000\n      22.658992\n      -92.189386\n      716.410620\n      49.949560\n      -168.903301\n      -1070.585987\n      3.258333\n      -2.458333\n      -0.001275\n      1.0\n      1\n    \n    \n      1985-01-01\n      -366.991657\n      0.000000\n      -366.991657\n      -70.104285\n      14.361613\n      92.664162\n      -49.714652\n      1548.910801\n      1829.266316\n      0.400000\n      0.116667\n      0.062567\n      1.0\n      1\n    \n    \n      1986-01-01\n      -2451.586574\n      0.000000\n      -2451.586574\n      -371.744807\n      159.793281\n      -681.904345\n      95.031719\n      -1006.031688\n      2434.442476\n      -1.633333\n      1.558333\n      0.070750\n      1.0\n      1\n    \n    \n      1987-01-01\n      743.692037\n      743.692037\n      0.000000\n      153.866579\n      329.611786\n      539.538581\n      222.273420\n      -1071.281516\n      -2615.717079\n      0.233333\n      -0.141667\n      0.023867\n      0.0\n      1\n    \n    \n      1988-01-01\n      -518.164793\n      0.000000\n      -518.164793\n      67.174276\n      -224.515238\n      338.084366\n      168.910804\n      -16.895010\n      489.437183\n      -0.483333\n      0.100000\n      -0.063592\n      0.0\n      1\n    \n    \n      1989-01-01\n      25.506344\n      25.506344\n      0.000000\n      -29.373936\n      293.129430\n      -101.490177\n      210.645362\n      -29.534180\n      -6.660479\n      -1.541667\n      1.475000\n      -0.095233\n      0.0\n      1\n    \n    \n      1990-01-01\n      197.224034\n      197.224034\n      0.000000\n      34.978568\n      45.208628\n      -27.139354\n      57.830682\n      -130.703028\n      -323.144222\n      -0.825000\n      0.591667\n      -0.046833\n      0.0\n      1\n    \n    \n      1991-01-01\n      -584.846863\n      0.000000\n      -584.846863\n      -89.854509\n      29.157325\n      -391.046061\n      -45.231681\n      -553.837536\n      437.520891\n      -0.241667\n      -0.016667\n      -0.017092\n      0.0\n      1\n    \n    \n      1992-01-01\n      64.165615\n      64.165615\n      0.000000\n      -75.508867\n      -231.401242\n      315.638496\n      41.940598\n      429.340791\n      398.387388\n      1.333333\n      -0.908333\n      -0.021042\n      0.0\n      1\n    \n    \n      1993-01-01\n      366.503452\n      366.503452\n      0.000000\n      127.189967\n      8.224851\n      -10.051816\n      126.182874\n      -922.582134\n      -1288.265714\n      1.208333\n      -1.208333\n      0.062833\n      0.0\n      1\n    \n    \n      1994-01-01\n      298.845156\n      298.845156\n      0.000000\n      120.739891\n      66.827798\n      -275.184458\n      29.220498\n      -1263.226429\n      -1445.234319\n      0.125000\n      -0.508333\n      0.081358\n      0.0\n      1\n    \n    \n      1995-01-01\n      -424.015919\n      0.000000\n      -424.015919\n      131.202242\n      2.596401\n      -439.897302\n      -74.640409\n      -762.191041\n      -106.716872\n      -0.800000\n      0.850000\n      0.075725\n      0.0\n      1\n    \n    \n      1996-01-01\n      657.264596\n      657.264596\n      0.000000\n      14.958707\n      86.803365\n      -433.673297\n      -162.251681\n      -255.745595\n      -743.350647\n      -0.941667\n      0.925000\n      0.006758\n      0.0\n      1\n    \n    \n      1997-01-01\n      -230.011377\n      0.000000\n      -230.011377\n      212.881242\n      162.512585\n      96.104917\n      -109.411649\n      311.889310\n      -39.009707\n      -0.958333\n      0.666667\n      -0.008950\n      0.0\n      1\n    \n    \n      1998-01-01\n      -821.938330\n      0.000000\n      -821.938330\n      -135.687474\n      313.817676\n      -184.948302\n      5.789059\n      49.200923\n      883.746412\n      -1.008333\n      0.608333\n      0.021167\n      0.0\n      1\n    \n    \n      1999-01-01\n      1109.397825\n      1109.397825\n      0.000000\n      -244.269810\n      148.547641\n      273.965461\n      -247.511966\n      694.792990\n      -840.360093\n      -0.291667\n      0.641667\n      0.098767\n      0.0\n      1\n    \n    \n      2000-01-01\n      2757.757519\n      2757.757519\n      0.000000\n      340.139894\n      -690.661386\n      -291.566666\n      -12.745271\n      367.958900\n      -1760.455732\n      0.083333\n      -0.066667\n      0.002192\n      0.0\n      1\n    \n    \n      2001-01-01\n      -2232.303933\n      0.000000\n      -2232.303933\n      56.816619\n      35.595123\n      -27.982751\n      -114.750162\n      627.934302\n      2681.059082\n      -0.700000\n      0.175000\n      -0.000308\n      0.0\n      1\n    \n    \n      2002-01-01\n      241.138725\n      241.138725\n      0.000000\n      -148.390689\n      181.531943\n      -500.759694\n      -150.597911\n      -511.790721\n      -435.908918\n      -0.241667\n      0.300000\n      0.063500\n      0.0\n      1\n    \n    \n      2003-01-01\n      38.507474\n      38.507474\n      0.000000\n      -186.762753\n      -218.560977\n      1105.700627\n      -97.357661\n      63.349341\n      -772.892691\n      0.616667\n      0.000000\n      0.021575\n      0.0\n      1\n    \n    \n      2004-01-01\n      730.190654\n      730.190654\n      0.000000\n      245.970162\n      -45.066727\n      114.727320\n      8.934454\n      673.630593\n      -363.256361\n      -0.225000\n      0.766667\n      -0.169425\n      0.0\n      1\n    \n    \n      2005-01-01\n      922.518162\n      922.518162\n      0.000000\n      170.457209\n      -70.989711\n      173.905095\n      -26.011829\n      689.483910\n      -532.418673\n      -0.525000\n      0.383333\n      -0.099592\n      1.0\n      1\n    \n    \n      2006-01-01\n      -886.089729\n      0.000000\n      -886.089729\n      173.217986\n      936.866393\n      -10.610252\n      -18.073730\n      238.566715\n      7.108587\n      -0.650000\n      -0.275000\n      -0.090092\n      1.0\n      1\n    \n    \n      2007-01-01\n      -584.342016\n      0.000000\n      -584.342016\n      268.999623\n      19.207126\n      -490.926955\n      -5.875584\n      601.592831\n      1382.779468\n      -0.483333\n      0.941667\n      -0.077108\n      1.0\n      1\n    \n    \n      2008-01-01\n      393.424107\n      393.424107\n      0.000000\n      -221.427184\n      -6.663868\n      -1268.721100\n      -5.623041\n      388.103413\n      1485.868417\n      0.108333\n      0.591667\n      -0.060192\n      0.0\n      1\n    \n    \n      2009-01-01\n      -1667.349199\n      0.000000\n      -1667.349199\n      125.550103\n      -315.651432\n      1565.111650\n      46.330578\n      -289.046007\n      49.623450\n      0.125000\n      0.416667\n      -0.007417\n      0.0\n      1\n    \n    \n      2010-01-01\n      446.933388\n      446.933388\n      0.000000\n      -463.012491\n      -134.196272\n      -310.113274\n      29.969873\n      216.193180\n      706.551703\n      2.883333\n      -2.383333\n      0.074675\n      0.0\n      1\n    \n    \n      2011-01-01\n      830.161197\n      830.161197\n      0.000000\n      61.104990\n      181.653671\n      -172.434726\n      2.380217\n      -119.755144\n      -1017.860060\n      -0.058333\n      -1.283333\n      -0.111317\n      0.0\n      1\n    \n    \n      2012-01-01\n      -1186.571695\n      0.000000\n      -1186.571695\n      267.430159\n      216.577571\n      116.087749\n      -0.987183\n      202.745156\n      788.234188\n      -1.083333\n      1.375000\n      -0.041100\n      0.0\n      1\n    \n    \n      2013-01-01\n      396.701645\n      396.701645\n      0.000000\n      143.733151\n      139.075529\n      645.385041\n      15.257883\n      311.983040\n      -997.654445\n      -0.733333\n      0.450000\n      0.010383\n      0.0\n      1\n    \n    \n      2014-01-01\n      -289.053915\n      0.000000\n      -289.053915\n      5.088233\n      -12.268558\n      -313.237415\n      23.739849\n      -796.621934\n      -163.410431\n      -0.041667\n      -0.366667\n      0.030583\n      0.0\n      1\n    \n    \n      2015-01-01\n      -1591.443473\n      0.000000\n      -1591.443473\n      -438.301999\n      -0.178887\n      -482.144162\n      -6.057110\n      -341.936691\n      2164.074720\n      0.116667\n      -0.533333\n      0.074692\n      0.0\n      1\n    \n    \n      2016-01-01\n      58.768905\n      58.768905\n      0.000000\n      -127.155353\n      -208.675326\n      -85.226565\n      53.886937\n      658.025579\n      1074.200854\n      1.316667\n      -0.650000\n      0.174125\n      0.0\n      1"
  },
  {
    "objectID": "posts/2020-11-21-ansible.html",
    "href": "posts/2020-11-21-ansible.html",
    "title": "Automating provisioning Arch continued - Ansible",
    "section": "",
    "text": "This is part 2 of a 4 part series describing how I provision my systems. Links to each part are below:\n\npart 1 - The base OS install\npart 2 - Software install and system configuration with Ansible\npart 3 - User level and python environment config with dotfiles and mkrc\npart 4 - The tldr that wraps up how to do the whole thing from start to finish]\n\n\nIntroduction\nMy previous post described how to automate a base installation of Arch. This follow up post will give an overview of the next step of configuration.\nAfter getting a base system setup there is still a ton of administrative tasks to do, like creating a user account and installing software. I accomplished this using Ansible. As with the previous post, I borrowed heavily from Brennan Fee for the configuration. My copy is here. This post won’t be as in depth as the previous one, as the ansible syntax is a lot more directly readable, so in most cases it should be enough to look at the code and maybe consult the ansible docs to figure out what’s going on. The sections below will outline a few of the parts that were a little tricky.\n\n\nHashed passwords\nAnsible lets you create a user and include the hash of their password, which means you can have the data available publicly without a security concern. In order to generate a hash of a password refer to this section of the ansible FAQ\n\n\ngit clone\nI had a tricky time with this task. I wanted to clone some repositories I controlled using ssh and save them in my home directory. After a lot of googling I determined that trying to become my user and do the clone directly wouldn’t work because ansible wouldn’t know which key to use (I have a separate key for GitHub than for my local network). This task splits it up by cloning into the ansible user directory and then using the copy task to move them over to my home directory and set the correct permissions. A little hacky, but it worked.\n\n\ndconf\nYou can use ansible to configure your GNOME desktop with the dconf module. The trickiest part of that is figuring out what key you have to change. This blog has the solution I used.\n\ndconf dump / > before.txt\nmake changes in settings or tweak tool\ndconf dump / > after.txt\ndiff before.txt after.txt\nFigure out what changed and create a dconf task for it.\n\n\n\nOther resources\nBeyond the links previously mentioned I want to highlight a tutorial series from Jeff Geerling which was excellent and informative. He also wrote a book on ansible that I haven’t read yet but imagine is quite good, given the quality of his video guide, and the fact that I found posts from him a few times when I was googling how to do something.\n\n\nConclusion\nAnsible is a pretty rad way to reproducibly get your desktop environment set up just the way you like it. It’s a bit overkill given what it’s actually designed for, but it’s a handy skill to learn and it saves rebuilding your environment from scratch."
  },
  {
    "objectID": "posts/2020-05-03-ssh.html",
    "href": "posts/2020-05-03-ssh.html",
    "title": "Managing SSH keys for a home network",
    "section": "",
    "text": "This guide covers how I set up and manage ssh keys on my home network. It’s not meant as an explainer on what ssh is or why you should use it, more a recipe for how I do things.\n\nEdit October 16 2020 - Added scripts to automate this setup"
  },
  {
    "objectID": "posts/2020-05-03-ssh.html#get-the-drive-ready",
    "href": "posts/2020-05-03-ssh.html#get-the-drive-ready",
    "title": "Managing SSH keys for a home network",
    "section": "Get the drive ready",
    "text": "Get the drive ready\nI have a 64GB USB key that I’m going to store the CAs on. That’s way too big for what I need, and I wouldn’t mind having it handy to shuttle other files around (I’m not going to be taking it anywhere, on account of it has CAs for my network on it, but still). So I created a big exfat partition that took up most of the disk for files and such, along with a 200MB ntfs partition right at the end for storing keys. I went with exfat for the storage partition because it’s readable in both Windows and Linux (once you install exfat-utils and exfat-fuse), is designed for flash media, and can handle large files. I went with NTFS for the secrets partition because I have to have file permissions on the private keys or it won’t work, and windows machines won’t read EXT4. It does mean I’ll have to install NTFS support on any Linux boxes I want to run it from, which is a hassle, but I can live with that."
  },
  {
    "objectID": "posts/2020-05-03-ssh.html#generate-ca-keys",
    "href": "posts/2020-05-03-ssh.html#generate-ca-keys",
    "title": "Managing SSH keys for a home network",
    "section": "Generate CA keys",
    "text": "Generate CA keys\nFrom the usb drive:\nssh-keygen -t ed25519 -f user_ca -C user_ca\nssh-keygen -t ed25519 -f host_ca -C host_ca\nGenerate a host and user signing key using ed25519 encryption. Generates public private key pairs named host/user_ca and host/user_ca.pub for the public keys. RSA is the default, but all of my systems support ed25519 and I understand it’s better in terms of security and performance so I might as well take this opportunity to update.\n\nGenerate known_hosts file\nThis will go in the ~/.ssh folder of clients in order to validate access.\necho -n \"@cert-authority * \" > known_hosts\ncat host_ca.pub >> known_hosts"
  },
  {
    "objectID": "posts/2020-05-03-ssh.html#generate-the-host-key-and-sign-it",
    "href": "posts/2020-05-03-ssh.html#generate-the-host-key-and-sign-it",
    "title": "Managing SSH keys for a home network",
    "section": "Generate the host key and sign it",
    "text": "Generate the host key and sign it\nFor any machines that I want to be able to ssh into I need to generate and sign a host key. From the same folder as the CA keys were generated:\nssh-keygen -t ed25519 -f mars_host_ed25519_key\nssh-keygen -s host_ca -I mars -h mars_host_ed25519_key.pub\n\nWARNING: DO NOT PUT A PASSPHRASE ON HOST KEYS\nThe first line operates the same as before. The second line signs the public key. -I mars is the certificate’s identity, apparently it can be used to revoke a certificate in the future although I’m not totally clear on how at this point. You can use the -n flag to specify which hostname in particular this key is valid for but I don’t really see the need in as small a setup as I’m doing. -h identifies this as a host key.\nAt the end of this, three new files are generated: mars_host_ed25519_key, mars_host_ed25519_key-cert.pub, and mars_host_ed25519_key.pub."
  },
  {
    "objectID": "posts/2020-05-03-ssh.html#configure-ssh-to-use-host-certificates",
    "href": "posts/2020-05-03-ssh.html#configure-ssh-to-use-host-certificates",
    "title": "Managing SSH keys for a home network",
    "section": "Configure SSH to use host certificates",
    "text": "Configure SSH to use host certificates\nMove the three generated files from the last section and copy user_ca.pub to /etc/ssh on your host and set the permissions to match the other files there. In this example I’ve physically moved the key over to the host machine and mounted it. If your host currently has ssh enabled with password based authentication you could scp it over instead:\nsudo mv mars_host_ed25519_key* /etc/ssh/\nsudo cp user_ca.pub /etc/ssh/\ncd /etc/ssh/\nsudo chown root:root mars_host*\nsudo chown root:root user_ca.pub\nsudo chmod 644 mars_host*\nsudo chmod 600 mars_host_ed25519_key # stricter private key permissions\nsudo chmod 644 user_ca.pub\nNext, edit /etc/ssh/sshd_config to have the lines\nHostKey /etc/ssh/mars_host_ed25519_key\nHostCertificate /etc/ssh/mars_host_ed25519_key-cert.pub\nTrustedUserCAKeys /etc/ssh/user_ca.pub\nThere was a commented out line in the file for HostKey so I put it below there. Placement shouldn’t really matter but it will hopefully be easier to find if I have to track this file down later.\nsudo systemctl restart sshd\nOnce you’re sure everything is working you’ll want to disable password authentication. Be aware that if you screw up keys and have the second line set you’ll have to physically connect to the machine to resolve it. For my home network this is no big deal, but just be aware. Go into /etc/ssh/sshd_config and set the following lines and restart ssh again:\nPubkeyAuthentication yes\nPasswordAuthentication no"
  },
  {
    "objectID": "posts/2020-05-03-ssh.html#create-user-key",
    "href": "posts/2020-05-03-ssh.html#create-user-key",
    "title": "Managing SSH keys for a home network",
    "section": "Create user key",
    "text": "Create user key\nStill in the folder with the ca key:\nssh-keygen -f luna_ed25519 -t ed25519\nssh-keygen -s user_ca -I ian@luna -n admin,ansible,ipreston,pi luna_ed25519.pub\nmv luna* ~/.ssh/\ncp known_hosts ~/.ssh/\nThe only new flag in this is -n ipreston,admin,pi which is the comma separated list of users I want this to be valid for. In addition to the username I set up on my server I want this key to be able to connect to my pfsense admin user and my raspberry pi, which I generally just leave with the default pi user."
  },
  {
    "objectID": "posts/2020-05-03-ssh.html#give-it-a-test-drive",
    "href": "posts/2020-05-03-ssh.html#give-it-a-test-drive",
    "title": "Managing SSH keys for a home network",
    "section": "Give it a test drive",
    "text": "Give it a test drive\nAt this point if everything is configured correctly you should be able to ssh from your client to your host and only be prompted for the passkey you set (or without any prompting if you didn’t set a passkey)\nssh -i ~/.ssh/luna_ed25519 ipreston@mars"
  },
  {
    "objectID": "posts/2020-05-03-ssh.html#set-up-an-ssh-config-file",
    "href": "posts/2020-05-03-ssh.html#set-up-an-ssh-config-file",
    "title": "Managing SSH keys for a home network",
    "section": "Set up an ssh config file",
    "text": "Set up an ssh config file\nIf that all worked the next step will be to set a nice alias to save some typing in the future.\ntouch ~/.ssh/config\nchmod 600 ~/.ssh/config\nIn config:\nHost mars\n    User ipreston\n    IdentityFile ~/.ssh/luna_ed25519\nThere are lots of other options you can set in that config but that’s all I need for my setup. After that all you should need to type is ssh mars to be connected to your host with the right user and using the correct authentication."
  },
  {
    "objectID": "posts/2020-05-03-ssh.html#set-up-host-authentication",
    "href": "posts/2020-05-03-ssh.html#set-up-host-authentication",
    "title": "Managing SSH keys for a home network",
    "section": "Set up host authentication",
    "text": "Set up host authentication\nThis script when run as root will generate a signed host key, move it to the correct directory, modify SSH configuration to authenticate using it and then restart ssh.\n#!/usr/bin/env bash\n\n# Bash \"strict\" mode, -o pipefail removed\nSOURCED=false && [ \"${0}\" = \"${BASH_SOURCE[0]}\" ] || SOURCED=true\nif ! $SOURCED; then\n  set -eEu\n  shopt -s extdebug\n  trap 's=$?; echo \"$0: Error on line \"$LINENO\": $BASH_COMMAND\"; exit $s' ERR\n  IFS=$'\\n\\t'\nfi\n\n# Text modifiers\nBold=\"\\033[1m\"\nReset=\"\\033[0m\"\n\n# Colors\nRed=\"\\033[31m\"\nGreen=\"\\033[32m\"\nYellow=\"\\033[33m\"\n\nerror_msg() {\n  T_COLS=$(tput cols)\n  echo -e \"${Red}$1${Reset}\\n\" | fold -sw $((T_COLS - 1))\n  exit 1\n}\n\ncheck_root() {\n  echo \"Checking root permissions...\"\n\n  if [[ \"$(id -u)\" != \"0\" ]]; then\n    error_msg \"ERROR! You must execute the script as the 'root' user.\"\n  fi\n}\n\n\ngenerate_and_sign_host_key() {\n  private_suffix=\"_host_ed25519_key\"\n  private_key=\"$HOSTNAME$private_suffix\"\n  public_key=\"$private_key.pub\"\n  ssh-keygen -t ed25519 -f $private_key -N \"\"\n  chown root:root $private_key*\n  # Have to lock it down before signing\n  chmod 600 $private_key\n  ssh-keygen -s CA/host_ca -I $HOSTNAME -h $public_key\n  cp CA/user_ca.pub /etc/ssh/\n  chown root:root /etc/ssh/user_ca.pub\n  chmod 644 /etc/ssh/user_ca.pub\n  chmod 644 $private_key*\n  # Set back to stricter private key access\n  chmod 600 $private_key\n  mv $private_key* /etc/ssh/\n}\n\nupdate_sshd_config() {\n  private_suffix=\"_host_ed25519_key\"\n  private_key=\"$HOSTNAME$private_suffix\"\n  hostkey=\"HostKey /etc/ssh/$private_key\"\n  cert=\"HostCertificate /etc/ssh/$private_key-cert.pub\"\n  ca=\"TrustedUserCAKeys /etc/ssh/user_ca.pub\"\n\n  sshd=\"/etc/ssh/sshd_config\"\n  sed -i \"/^#HostKey\\s\\/etc\\/ssh\\/ssh_host_ed25519_key/a $ca\" $sshd\n  sed -i \"/^#HostKey\\s\\/etc\\/ssh\\/ssh_host_ed25519_key/a $cert\" $sshd\n  sed -i \"/^#HostKey\\s\\/etc\\/ssh\\/ssh_host_ed25519_key/a $hostkey\" $sshd\n  sed -i 's/^#PasswordAuthentication\\syes/PasswordAuthentication no/g' $sshd\n}\n\ncheck_root\ngenerate_and_sign_host_key\nupdate_sshd_config\nsystemctl restart sshd"
  },
  {
    "objectID": "posts/2020-05-03-ssh.html#setup-user-authentication",
    "href": "posts/2020-05-03-ssh.html#setup-user-authentication",
    "title": "Managing SSH keys for a home network",
    "section": "Setup user authentication",
    "text": "Setup user authentication\nWhatever user your run this as should end up with a configured SSH setup that will allow access into any similarly configured hosts.\n#!/usr/bin/env bash\n\n# Bash \"strict\" mode, -o pipefail removed\nSOURCED=false && [ \"${0}\" = \"${BASH_SOURCE[0]}\" ] || SOURCED=true\nif ! $SOURCED; then\n  set -eEu\n  shopt -s extdebug\n  trap 's=$?; echo \"$0: Error on line \"$LINENO\": $BASH_COMMAND\"; exit $s' ERR\n  IFS=$'\\n\\t'\nfi\n\n# Text modifiers\nBold=\"\\033[1m\"\nReset=\"\\033[0m\"\n\n# Colors\nRed=\"\\033[31m\"\nGreen=\"\\033[32m\"\nYellow=\"\\033[33m\"\n\nerror_msg() {\n  T_COLS=$(tput cols)\n  echo -e \"${Red}$1${Reset}\\n\" | fold -sw $((T_COLS - 1))\n  exit 1\n}\n\ncheck_root() {\n  echo \"Checking root permissions...\"\n\n  if [[ \"$(id -u)\" == \"0\" ]]; then\n    error_msg \"ERROR! Don't run this as root.\"\n  fi\n}\n\nupdate_known_hosts() {\n  cp CA/known_hosts $HOME/.ssh/known_hosts\n}\n\ngen_keys() {\n  cp CA/user_ca .\n  chown $USER user_ca\n  chmod 600 user_ca\n  private_suffix=\"_ed25519\"\n  private_key=$HOME/.ssh/$HOSTNAME$private_suffix\n  public_key=\"$private_key.pub\"\n  ssh-keygen -f $private_key -t ed25519 -N \"\"\n  ssh-keygen -s user_ca -I $USER@$HOSTNAME -n admin,ipreston,cornucrapia,pi,ansible $public_key\n  rm user_ca\n}\n\nupdate_config() {\n  user_config=\"$HOME/.ssh/config\"\n  cp CA/config $user_config\n  chown $USER $user_config\n  chmod 600 $user_config\n  sed -i -e \"s/HOST/$HOSTNAME/g\" $user_config\n}\n\n\ncheck_root\nmkdir -p $HOME/.ssh\nupdate_known_hosts\ngen_keys\nupdate_config"
  },
  {
    "objectID": "posts/2022-12-30-rootless-docker.html",
    "href": "posts/2022-12-30-rootless-docker.html",
    "title": "Setting up rootless docker",
    "section": "",
    "text": "This post covers something that I did for work that I thought might be of more general interest. We have a number of developers that want to build and work in docker containers. Due to security constraints we can’t install WSL and docker desktop on our laptops, so some remote development solution is required. We don’t have kubernetes or anything fancy in our environment, so the initial solution was to just provision an Ubuntu VM and give everyone a login and access to the docker group. Unfortunately there are some major usability and security issues with this approach. Basically, since giving access to docker is essentially giving root access to the machine, developers can see and access each other’s containers and files. From a security perspective this is obviously no good. Even from a usability perspective it means there’s constant risk of inadvertently tripping over another developer’s work, and cleaning up your old images and containers has to be done with a lot more care. To address this issue I decided to look into rootless docker.\nI’m going to try a different approach with this blog. The next section will be a lightly edited transcription of all the things I tried (including failures, dead ends, and dumb mistakes). The following section will be a TLDR summary of what you actually need to do to get this working."
  },
  {
    "objectID": "posts/2022-12-30-rootless-docker.html#host-configuration",
    "href": "posts/2022-12-30-rootless-docker.html#host-configuration",
    "title": "Setting up rootless docker",
    "section": "Host configuration",
    "text": "Host configuration\nOfficial documentation on rootless docker config can be found here\nFor further discussion of the user namespace remapping (which explains why users should be root within the devcontainer and what /etc/subuid and /etc/subgid are doing) see the official docs here\n\nAll testing was done on an Ubuntu VM (specifically 22.04.1 LTS). As most development activity occurs within docker, most of these instructions will hopefully survive a newer Ubuntu release, and could probably even be applied to an entirely different distro if for some reason we wanted to do that. CPU, RAM and disk requirements will largely depend on the size of the team and their activity, but note that docker images tend to take up a fair bit of space, and due to (intentional) isolation of docker runtimes between users there will be no sharing of image layers. Thus 5 users each using a 2GB docker image will take up a total of 10GB of space.\nJoin the machine to the domain\n#Setting up AD Authentication\nsudo apt install -y realmd libnss-sss libpam-sss sssd sssd-tools adcli samba-common-bin oddjob oddjob-mkhomedir packagekit\n#Discovery\nsudo realm discover <domain>\n#Adding to the domain (enter password when prompted)\nsudo realm join -U <privileged domain user>\n#Adding domain user to allow ssh\nrealm permit -g groupname@domainname\nInstall docker enginer as per the official docker docs. Note, do not use the included docker package in the Ubuntu base repository.\nMake sure the system docker daemon is not running and disable it if it is: sudo systemctl disable --now docker.service docker.socket\nInstall autofs and cifs-utils to allow users to mount network shares with their credentials.\nFor each user that will be running docker on this machine, create an entry in /etc/subuid and /etc/subgid (the entry in each file should look the same)\n\neach entry in each file has the format <username>:<baseuid>:65536. <baseuid> starts at 100000 for the first entry and increases by 65536 for each subsequent entry. For example, here’s what /etc/subuid looks like on the machine where this guide was tested:\n\nadmin:100000:65536\ndockertest:165536:65536\ndockertesta:231072:65536\ndockertestb:296608:65536\n<EID>:362144:65536\n\nLocal user accounts seem to get their entries auto-generated correctly, but at least in testing, domain joined user accounts had to be manually created.\n\nFor each user that will be running docker in this machine, create an entry in /etc/passwd that specifies their default shell as bash. Otherwise VS code will not be able to figure out the user level docker socket it should attach to.\n\ngetent passwd <EID> | sudo tee -a /etc/passwd\n\nCreate a base user folder to mount network shares for each user in /mnt/<EID>, make that user the owner of that folder and lock down access to that user (chown <EID> and chmod 700 /mnt/<EID>).\nFor each user that will be running docker from this machine, create a line in /etc/auto.master in the following format:\n/mnt/<EID> /etc/auto.sambashares-<EID> --timeout=30 –ghost\nPopulate /etc/auto.sambashares-<EID> with a line for each network share that user has to access as follows:\n<localsharename> -fstype=cifs,rw,sec=krb5,uid=${UID},cruid=${UID} :<full share path>\nWhere <localsharename> is the name of the folder under /mnt/<EID> that the share will be mounted to, and <full share path> is the path to the SMB file share."
  },
  {
    "objectID": "posts/2022-12-30-rootless-docker.html#user-configuration",
    "href": "posts/2022-12-30-rootless-docker.html#user-configuration",
    "title": "Setting up rootless docker",
    "section": "User configuration",
    "text": "User configuration\nMost configuration of the VM should have been completed by its system administrator, but there are a couple user level tasks you will have to run before you can work with docker.\n\nInstall the rootless docker daemon\ndockerd-rootless-setuptool.sh install\n\n\nSet an environment variable for the Docker socket\nAdd the following line to ~/.bashrc: DOCKER_HOST=unix:///run/user/$(id -u)/docker.sock\n\n\nLog out and back in and test\ndocker run hello-world\n\n\nSet up network shares\nAttaching network shares cannot be done directly by the user. System administrators provision network drives for each user under /mnt/<EID>. If the network share you want is not there, contact your system administrator with its information and they will add it."
  },
  {
    "objectID": "posts/2022-11-21-proxmox.html",
    "href": "posts/2022-11-21-proxmox.html",
    "title": "Home cluster part 1 - Intro and Proxmox install",
    "section": "",
    "text": "I’ve been running services on my home network for years. It started with running things bare metal on the same machine I used as my desktop for day to day work. That was a nice easy way to get started, but I was constantly running into conflicting updates, or accidentally breaking something when I tried to get some desktop service working. The next step was getting a dedicated machine solely for hosting services. This worked a lot better since my service requirements changed a lot less frequently than my desktop requirements, but I still ran into conflicting services, or breaking one service when I was testing something out on another. The next step was a dedicated machine, but running all my services in docker containers. That really helped with isolation and was also where I got serious about automating my environment with ansible, which generally meant that even a complete system wipeout only took me down for as long as it took to reinstall the base OS and re-run my ansible playbook.\nNow it’s time for the next step in making my home server environment fancier - Proxmox."
  },
  {
    "objectID": "posts/2022-11-21-proxmox.html#what-i-actually-bought",
    "href": "posts/2022-11-21-proxmox.html#what-i-actually-bought",
    "title": "Home cluster part 1 - Intro and Proxmox install",
    "section": "What I actually bought",
    "text": "What I actually bought\n\nHP ProDesk 400 G3\n\ni5-7500T\n32GB RAM\n512 GB nvme SSD\n1TB WD Blue SATA SSD (purchased new and installed after)\n348.75 CAD total price - 252.75 CAD for the system 96 CAD for the SSD upgrade.\n\nHP EliteDesk 800 G3\n\ni5-7500T\n32GB RAM (purchased new and installed after, shipped with 4GB)\n512 GB nvme SSD (purchased new, came with 240 GB that I repurposed for a portable drive)\n1TB WD Blue SATA SSD (purchased new and installed after)\n456.63 CAD total price - 141.34 for the system, 125.15 for the RAM, 96 for the SSD, 94.14 for the nvme SSD\n\nDell 3060 Micro\n\ni5-8500T\n32GB RAM (purchased new and installed after, shipped with 8GB)\n512 GB nvme SSD (purchased new, came with 240 GB that I repurposed for a portable drive)\n1TB WD Blue SATA SSD (purchased new and installed after)\n503.53 CAD total price - 180 for the system, 133.39 for the RAM, 96 for the SSD, 94.14 for the nvme SSD\n\n\nAs you can see the price varied between the nodes. I got lucky with the HP ProDesk because it was in Canada and came equipped with the RAM and nvme I wanted. Making those upgrades after on the other systems and ordering from the US increased the price. With further patience and luck maybe I could have saved a couple hundred bucks, but honestly I’d already been waiting a long time to get this project kicked off and I kind of think that ProDesk was a bit of a unicorn."
  },
  {
    "objectID": "posts/2021-04-10-portfolio.html",
    "href": "posts/2021-04-10-portfolio.html",
    "title": "My Portfolio",
    "section": "",
    "text": "Some of the posts on this blog are pretty big guides or documents that I like to reference and refer others to regularly. In addition, I have a few other projects that I haven’t written about, but would still be nice to share. This post will compile all the bigger posts and projects I’ve worked on as one easy to reference article."
  },
  {
    "objectID": "posts/2021-04-10-portfolio.html#where-to-live-app",
    "href": "posts/2021-04-10-portfolio.html#where-to-live-app",
    "title": "My Portfolio",
    "section": "Where to live app",
    "text": "Where to live app\nThis post documents the process I went through to build a where to live app. It scrapes listings from rental and sales sites daily, and then combines them with other data sets like commute times, grocery store locations, and flood risk to produce personalized lists of candidate listings. The blog describes the implementation in more detail, along with some important lessons I learned and links to the code. The blog post is here."
  },
  {
    "objectID": "posts/2021-04-10-portfolio.html#paper-reproduction---albertas-fiscal-responses-to-fluctuations-in-non-renewable-resource-revenue-in-python",
    "href": "posts/2021-04-10-portfolio.html#paper-reproduction---albertas-fiscal-responses-to-fluctuations-in-non-renewable-resource-revenue-in-python",
    "title": "My Portfolio",
    "section": "Paper Reproduction - “Alberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue” in python",
    "text": "Paper Reproduction - “Alberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue” in python\nThis is a paper reproduction I did of a paper that was published by the University of Calgary’s school of public policy. In the course of reproducing the paper I actually found a data error in the original paper. After correcting the error the conclusions of the paper do not appear to be supported. An interesting exercise in coding, and reproducibility in science. The blog post is here."
  },
  {
    "objectID": "posts/2021-04-10-portfolio.html#automating-provisioning-arch",
    "href": "posts/2021-04-10-portfolio.html#automating-provisioning-arch",
    "title": "My Portfolio",
    "section": "Automating provisioning Arch",
    "text": "Automating provisioning Arch\nI use Arch Linux btw. This post links to a 3 part post series I did about automating the setup of my workstations and servers. It goes from a detailed breakdown of the bash script that’s used to get a bare bones arch install, through to using ansible to do all the system configuration, software install, and server setup (mostly a bunch of docker containers), followed by setting up a user profile using rcm."
  },
  {
    "objectID": "posts/2021-04-10-portfolio.html#python-packaging-guide",
    "href": "posts/2021-04-10-portfolio.html#python-packaging-guide",
    "title": "My Portfolio",
    "section": "Python packaging guide",
    "text": "Python packaging guide\nThere are lots of great guides on how to build a python package. I’m personally a big fan of hypermodern python and use its accompanying cookiecutter whenever I’m setting up a new project. But there’s not much out there that walks you through all the phases between just having a script that you can run up to building a full blown package. Also, most guides don’t cover conda, and I find the conda docs are really tuned towards people who are bundling C code or something else with their package, not just making a pure python package. To bridge this gap, and to help cement my own understanding of packaging, I wrote this guide."
  },
  {
    "objectID": "posts/2021-04-10-portfolio.html#setting-up-a-data-science-environment-in-windows",
    "href": "posts/2021-04-10-portfolio.html#setting-up-a-data-science-environment-in-windows",
    "title": "My Portfolio",
    "section": "Setting up a data science environment in Windows",
    "text": "Setting up a data science environment in Windows\nThis guide is for everyone out there that only has a locked down Windows machine, but still wants to work with data in python. Almost all the guides I could find online assumed you had a Mac or Linux machine, and even the Windows ones often assumed you had administrative rights. This guide gets you set up with conda, git, and vs code, all without elevated privilege. The only thing I haven’t managed to do in a Windows environment is build a python package. I’m sure it’s doable, but this guide is for those writing scripts/notebooks."
  },
  {
    "objectID": "posts/2020-11-26-arch-tldr.html",
    "href": "posts/2020-11-26-arch-tldr.html",
    "title": "Automating provisioning Arch continued - TLDR",
    "section": "",
    "text": "This is part 4 of a 4 part series describing how I provision my systems. Links to each part are below:"
  },
  {
    "objectID": "posts/2020-11-26-arch-tldr.html#setup-wifi",
    "href": "posts/2020-11-26-arch-tldr.html#setup-wifi",
    "title": "Automating provisioning Arch continued - TLDR",
    "section": "Setup WiFi",
    "text": "Setup WiFi\nIn the case where I’m doing this on a laptop I’ll likely have to get on WiFi before I can continue.\niwctl\nstation wlan0 connect <your SSID>  # You can enclose it in quotes if it has spaces\n<enter passphrase>\nexit\ndhcpcd wlan0"
  },
  {
    "objectID": "posts/2020-11-26-arch-tldr.html#make-sure-partitions-are-set-up",
    "href": "posts/2020-11-26-arch-tldr.html#make-sure-partitions-are-set-up",
    "title": "Automating provisioning Arch continued - TLDR",
    "section": "Make sure partitions are set up",
    "text": "Make sure partitions are set up\nIf you’re not just going to wipe the whole disk you can run lsblk to determine what partitions you have. cfdisk has a nice interface for creating and modifying partitions if necessary. To format the boot partition run:\nmkfs.vfat -F32 /dev/<partition>\nmkfs.ext4 /dev/<partition> will work for the root partition."
  },
  {
    "objectID": "posts/2020-11-26-arch-tldr.html#run-the-script",
    "href": "posts/2020-11-26-arch-tldr.html#run-the-script",
    "title": "Automating provisioning Arch continued - TLDR",
    "section": "Run the script",
    "text": "Run the script\nbash <(curl -fsSL http://bootstrap.ianpreston.ca)\nAfter that power off, remove the USB and power back on."
  },
  {
    "objectID": "posts/2020-05-17-conda.html",
    "href": "posts/2020-05-17-conda.html",
    "title": "Trouble with reproducible conda environments",
    "section": "",
    "text": "Introduction\nI’m having trouble making reproducible conda environments. I’ve posted the question below on Stack Overflow and Reddit but I’ve got nothing. I’m leaving the question here for easy future reference. If I come up with a solution I’ll update this post.\n\n\nThe question\nHi everyone.\nI’m really struggling to create a reproducible conda environment. I’ll outline the approach I’ve taken so far and the issue I’ve encountered. I’d appreciate any tips for what I can do to troubleshoot next or resources I could check.\nAs background, I work on a small team, and want to be able to share a copy of an environment I’ve been using with other members of my team so I can be sure we have identical versions of all the libraries required for our work.\nMy current workflow is as follows:\n\nWrite out an environment file with unpinned dependencies and let conda build the environment\n\nname: example_env_build\nchannels:\n    - conda-forge\n    - defaults\ndependencies:\n    - pandas\n    - requests\nThe actual environment has a lot more stuff in it, but that’s the idea\n\nI then create the environment with conda env create -f example_env_build.yml\nI export the environment so that all versions and their dependencies will be pinned with conda env export -n example_env_build --no-builds --file test_export.yml. I added --no-builds because I was finding the certain builds were getting marked as broken and causing issues and getting the version right seemed close enough for my purposes.\nI edit the test_export.yml file and change the name to example_env and remove the prefix line from the bottom.\nI build a new environment with this pinned file just to make sure it goes ok, and then share the file with the rest of my team.\n\nThis has generally worked well if everyone tries to build the environment relatively quickly after the file is created. However, the whole point of being able to specify a reproducible environment is that I should be able to recreate that environment at any time. Someone on my team recently got a new computer so I was trying to help her set up her environment and ran into a series of conflicts. To troubleshoot I tried to rebuild the environment on my machine and ran into the same situation.\nFor troubleshooting I did the following: * Clone my environment so I have a backup while I mess around conda create --name example_env_clone --clone example_env * Export the environment conda env export -n example_env --no-builds --file example_env_rebuild.yml * Delete the example environment so I can rebuild it conda env remove --name example_env * Try and recreate the environment I just exported conda env create -f example_env_rebuild.yml\nFrom there I ran into all sorts of version conflicts. I don’t understand this because a) These are all versions being used in a working environment and b) a lot of the “conflicts” don’t seem to be conflicts to me. As an example, here’s one from my current attempt:\nPackage phik conflicts for:\nphik=0.9.10\npandas-profiling=2.4.0 -> phik[version='>=0.9.8']\nI picked that one basically at random but there are tons like that. As I read it I’m trying to install phik 0.9.10, and pandas-profiling requires >=0.9.8, which 0.9.10 satisfies.\nI’m at my wits end here. I’ve read a million “how to manage conda environments” guides (For example this, this, and this) along with the conda environment management docs. All of them seem to indicate what I’m doing should work perfectly fine, but my team and I constantly run into issues.\nHas anyone had a similar experience? Is there something I’m missing, or a resource I could consult? I’d greatly appreciate any pointers.\nThanks"
  },
  {
    "objectID": "posts/2023-02-05-proxmox-ceph.html",
    "href": "posts/2023-02-05-proxmox-ceph.html",
    "title": "Home Cluster Part 4 - Setup CEPH",
    "section": "",
    "text": "This is the fourth post ( part 1, part 2, part 3 ) in my home cluster with proxmox series. In this post we’re going to add distributed storage to the cluster using ceph. As with the other posts in this series, this is not a how-to guide from an established practitioner, but a journal I’m writing as I try and do something new.\nCeph in many ways is overkill for what I’m doing here. It’s designed to support absolutely massive distributed storage at huge scale and throughput while maintaining data integrity. To accomplish that it’s very complicated and their hardware recommendations reflect that. On the other hand, it’s integrated with proxmox and I’ve seen it run on even lower spec gear than I’m using. In this post my goal is to get a ceph cluster working that uses the 3 1TB SSDs I have in my nodes for file sharing. I’m not going to do any performance testing or tuning, and other than deploying an image to one just to confirm it works I probably won’t even use it in this section. The thing I actually want this storage for is to be my persistent storage in kubernetes, backed by rook, but that will come later once I actually have kubernetes set up.\nAs with most things with computers I won’t be starting from scratch. I’ve found a repository of ansible roles for setting up a proxmox cluster that includes ceph configuration and is very similar to my overall setup. I’ll work through vendoring this code into my recipes repository through this post.\n\n\nI ran into lots of problems getting this working. This post is definitely less of a guide and more a diary of the struggles I had getting ceph working. There may be some value to another reader if they find themselves having a similar challenge to me, but mostly this was just my scratchpad as I worked through getting things set up."
  },
  {
    "objectID": "posts/2023-02-05-proxmox-ceph.html#setting-up-the-inventory",
    "href": "posts/2023-02-05-proxmox-ceph.html#setting-up-the-inventory",
    "title": "Home Cluster Part 4 - Setup CEPH",
    "section": "Setting up the inventory",
    "text": "Setting up the inventory\nThe first section of the repo that I’ll incorporate is the inventory folder. This contains descriptions of the hosts, as well as what groups they belong to for roles. The inventory folder in this repo also contains group_vars and host_vars, which I keep in their own folders in my repo.\nLooking at the actual inventory there are a bunch of groups created for various ceph roles like mds, mgr, and osd. However, in the example case and in my case all nodes will fulfill all roles, so this is only necessary for expansion or comprehensibility of what tasks are doing what when a role is run. There is one differentiator for ceph_master, which only targets the first node to handle tasks that are managed at the proxmox cluster level. In my previous setup I’ve just had a pve group for the cluster and manually set pve1 as the host for things that take place at the cluster level. If I end up growing my cluster a lot and want to split things out I’ll have to refactor, but for now for simplicity I’m going to stick with just using the pve group. Based on this I don’t need any actual changes to my inventory. Looking at host_vars there are host specific variables identifying the separate NIC and IP address the nodes are using for the ceph network. Having a separate network for ceph is a recommendation that I am not following at this point so I don’t need to worry about that. They also have a host var specifying which storage drive should be attached to the ceph cluster. For me that’s /dev/sda on all of my nodes. I’ll have to refactor that out if I add another node that deviates from that, but for now I’m going to minimize the complexity in terms of number of files I have to reference and leave that alone. Looking at the group vars under ceph there’s an entry for the pool name, and for the list of nodes. Again, both of those I can just set as defaults for now and refactor later if I have to expand. So based on initial reading I’m going to leave this folder alone."
  },
  {
    "objectID": "posts/2023-02-05-proxmox-ceph.html#checking-the-library-folder",
    "href": "posts/2023-02-05-proxmox-ceph.html#checking-the-library-folder",
    "title": "Home Cluster Part 4 - Setup CEPH",
    "section": "Checking the library folder",
    "text": "Checking the library folder\nThe library folder contains a script for managing proxmox VMs with the qm command. That’s interesting, but not relevant to what I’m trying to do with ceph so I won’t worry about it here."
  },
  {
    "objectID": "posts/2023-02-05-proxmox-ceph.html#roles",
    "href": "posts/2023-02-05-proxmox-ceph.html#roles",
    "title": "Home Cluster Part 4 - Setup CEPH",
    "section": "Roles",
    "text": "Roles\nHere is going to be the bread and butter of this process. There are a number of roles in this folder helpfully prepended with ceph_ that I’ll want to take a look at.\nIn terms of order of reviewing these files I’m going to look at the site.yml file that’s at the base of the repository to understand what order they’re called in. That should make the most sense.\n\nceph_node\nThe first role is ceph_node which runs on all the nodes. There are two steps here, the first with the name “Install ceph packages”, and the second “Configure ceph network”, which I’ll ignore. There’s also a handler in this role, but it’s only to restart the network after configuring the second network, so I don’t need that. The first task looks like this:\n- name: Install ceph packages\n  shell: yes | pveceph install\n  args:\n    creates: /etc/apt/sources.list.d/ceph.list\nThere are a few things I have not seen before here that I’d like to understand before I blindly copy paste. The first is the yes command. This post explains what it is and why I’d use it. It’s basically for entering y into the user input of everything the command it’s piped to installs. The other thing I haven’t seen before is args. While args appears to be a generic keyword its use in this case is pretty well documented in the docs for shell. In this case it’s being used to say that running this command will create that file, so if it exists the file doesn’t need to be run, ensuring idempotency. Pretty handy!\nWhile I’m sure this would just work, I do want to know a bit about what I’m hitting y to by running this playbook, so let’s ssh into one of my nodes and manually run the command and save the output.\nroot@pve1:~# ls /etc/apt/sources.list.d | grep ceph\ndownload_proxmox_com_debian_ceph_quincy.list\nPrior to running the command I can confirm I do not have that file present.\nRunning pveceph install prompts an apt install command, the y is to confirm that I want to install a ton of ceph related packages. There are no other prompts so this seems safe to run.\n\n\nceph_master\nThe next role is described as creating the ceph cluster and only needs to be run on one node. This is also a small task and it looks like this:\n- name: Check ceph status\n  shell: pveceph status 2>&1 | grep -v \"not initialized\"\n  register: pveceph_status\n  ignore_errors: true\n  changed_when: false\n\n- name: Create ceph network\n  command: pveceph init --network 10.10.10.0/24\n  when: pveceph_status.rc == 1\nI’ll have to modify the network part to match my own setup, but otherwise this looks straightforward. Just for curiosity, let’s see what the first command looks like. As a reminder to myself, the 2>&1 redirects stderr to stdout.\nroot@pve1:~# pveceph status 2>&1\npveceph configuration not initialized\nLooking at the pveceph docs it looks like I can just drop the --network argument if I’m not specifying a separate one, so this will be a very small task. Note from me in the future: you need the network flag.\n\n\nceph_mon\nNext up we create monitors. This is also a simple looking role:\n- name: Check for ceph-mon\n  command: pgrep ceph-mon\n  register: ceph_mon_status\n  ignore_errors: true\n  changed_when: false\n\n- name: Create ceph-mon\n  shell: pveceph createmon\n  when: ceph_mon_status.rc == 1\npgrep looks for running processes, so that’s how we check if the monitor is already up and running. If it’s not, we create a monitor. The only arguements for this command are to assign an address or ID, neither of which I want to explicitly do, so I can leave this as is.\n\n\nceph_mgr\nAfter the monitor we create a manager. The setup is basically the same as the monitor and the command it runs has even fewer arguments than the monitor so I won’t spell it out here.\n\n\nceph_osd\nNow we have to create an osd which is the first place we’ll have to touch an actual disk. Having this step not be idempotent would be really bad as it could lead to wiping disks. The task looks like this:\n- name: Check for existing ceph_osd\n  command: pgrep ceph-osd\n  register: ceph_osd_pid\n  changed_when: false\n  ignore_errors: true\n\n- name: Read first 5KB of ceph device to determine state\n  shell: dd if={{ ceph_device }} bs=5K count=1 | sha256sum\n  when: \"ceph_osd_pid.rc != 0\"\n  register: ceph_device_first_5KB_sha256\n  changed_when: false\n\n- name: Determine if should initialize ceph_osd\n  when: \"ceph_osd_pid.rc != 0 and ceph_device_first_5KB_sha256.stdout == 'a11937f356a9b0ba592c82f5290bac8016cb33a3f9bc68d3490147c158ebb10d  -'\"\n  set_fact:\n    ceph_device_initialize: true\n  \n- name: Initialize ceph_osd device\n  when: ceph_device_initialize == True\n  command: pveceph createosd {{ ceph_device }}\nThere’s also a default variable for ceph_device_initialize that’s set to False. It only gets updated to true if that third step’s condition is met. I’m a little confused and worried about this role to be honest. The first step is fine, we’re just checking if the osd process is running. The next one is apparently making some assumption about what the hash of the first 5KB of my disk should look like if it doesn’t already have an osd installed. I don’t know how this would work and searching didn’t turn anything up. Let’s test though and check what it returns on my drives:\nroot@pve1:~# dd if=/dev/sda bs=5K count=1 | sha256sum\n1+0 records in\n1+0 records out\n5120 bytes (5.1 kB, 5.0 KiB) copied, 0.00509153 s, 1.0 MB/s\na11937f356a9b0ba592c82f5290bac8016cb33a3f9bc68d3490147c158ebb10d  -\n\nroot@pve2:~# dd if=/dev/sda bs=5K count=1 | sha256sum\n1+0 records in\n1+0 records out\n5120 bytes (5.1 kB, 5.0 KiB) copied, 0.00511535 s, 1.0 MB/s\na11937f356a9b0ba592c82f5290bac8016cb33a3f9bc68d3490147c158ebb10d  -\n\nroot@pve3:~# dd if=/dev/sda bs=5K count=1 | sha256sum\n1+0 records in\n1+0 records out\n5120 bytes (5.1 kB, 5.0 KiB) copied, 0.00503435 s, 1.0 MB/s\na11937f356a9b0ba592c82f5290bac8016cb33a3f9bc68d3490147c158ebb10d  -\nJust to make sure I wasn’t losing it, I tried it on another device that wasn’t blank and got a different hash. This is why I love the internet, there is absolutely no way I would have figured that out on my own. I don’t know how it works and that makes me a little nervous, but at this point I’m convinced that it will work. I’ll add in a default variable for my ceph device of /dev/sda and should be good to go.\n\n\nceph_pool\nNow that I’ve got my OSDs, it’s time to create a pool. This role also has a defaults file, with currently just one variable to specify the minimum number of nodes that must be up for pool creation (set to 3 which works for me). I’ll have to add in another default to mine for the pool name, as the original repo sets that in group vars. Beyond that let’s focus on the task:\n- name: Check ceph status\n  command: pveceph status\n  register: pveceph_status\n  ignore_errors: true\n  changed_when: false\n\n- name: Check ceph pools\n  shell: pveceph pool ls | grep -e \"^{{ ceph_pool }} \"\n  register: ceph_pool_status\n  changed_when: false\n  ignore_errors: true\n\n- name: Create ceph pool\n  when: ceph_pool_status.rc > 0 and (pveceph_status.stdout | from_json).osdmap.osdmap.num_up_osds >= minimum_num_osds_for_pool\n  command: pveceph pool create {{ ceph_pool }}\n\n- name: Check ceph-vm storage\n  command: pvesm list ceph-vm\n  changed_when: false\n  ignore_errors: true\n  register: ceph_vm_status\n\n- name: Create ceph VM storage (ceph-vm)\n  when: ceph_vm_status.rc > 0\n  command: pvesm add rbd ceph-vm -nodes {{ ceph_nodes }} -pool {{ ceph_pool }} -content images\n\n- name: Check ceph-ct storage\n  command: pvesm list ceph-ct\n  changed_when: false\n  ignore_errors: true\n  register: ceph_ct_status\n\n- name: Create ceph container storage (ceph-ct)\n  when: ceph_ct_status.rc > 0\n  command: pvesm add rbd ceph-ct -nodes {{ ceph_nodes }} -pool {{ ceph_pool }} -content rootdir\nThe first step pulls up a detailed description of the ceph pool status. In the third step we’ll parse it to check that we have the minimum number of OSDs up. The next one is pretty straightforward, make sure the pool we want to create doesn’t already exist. Next, assuming we have at least the minimum number of OSDs and our pool hasn’t been created, create it. This one is using all the defaults of the command since we don’t pass any arguments. Briefly, they are:\n\nnot to configure VM and CT storage for the pool (that appears to happen later)\nset the application as rbd (we will configure ceph fs later on).\nSome other stuff about scaling and erasure coding that I don’t understand and hopefully won’t need for now. Full docs here, search for pveceph pool create <name> [OPTIONS]\n\nThe next four parts configure proxmox to use ceph as a storage location for VMs and containers. I actually don’t want to do that, my VMs will live on my nvme drives, but it won’t hurt to have as an option I guess, and at least I can test if I can do stuff on the pool with this enabled so I’ll leave it but not spend much time working out how it works. I will have to add a variable for ceph_nodes to my defaults that maps to a comma separated list of my nodes.\n\n\nceph_mds\nAfter this we’re doing some necessary pre-configuration for enabling ceph-fs. Specifically the ceph metadata server. This is another very short task that checks if the service is running and starts it if not with a oneliner, so I won’t reproduce it here.\n\n\nceph_fs\nLast one. Ceph fs, from what little I’ve read of it would be nice to have as it will enable sharing storage across pods (docs). This task has very similar structure to the earlier ones as well so I won’t write it up in detail here."
  },
  {
    "objectID": "posts/2023-02-05-proxmox-ceph.html#adding-them-to-the-playbook",
    "href": "posts/2023-02-05-proxmox-ceph.html#adding-them-to-the-playbook",
    "title": "Home Cluster Part 4 - Setup CEPH",
    "section": "Adding them to the playbook",
    "text": "Adding them to the playbook\nHaving created the roles, I now need to make sure they’re done in the correct order in my playbook. As mentioned above I can base that on the order they’re listed in site.yml in the base repository I’ve been working off."
  },
  {
    "objectID": "posts/2023-02-05-proxmox-ceph.html#clean-up-the-install",
    "href": "posts/2023-02-05-proxmox-ceph.html#clean-up-the-install",
    "title": "Home Cluster Part 4 - Setup CEPH",
    "section": "Clean up the install",
    "text": "Clean up the install\nAs discussed in the last section I’ll run pveceph purge --crash --logs on all three nodes (that might be overkill but let’s be safe).\nroot@pve1:~# pveceph purge --crash --logs\nUnable to purge Ceph!\n\nTo continue:\n- remove pools, this will !!DESTROY DATA!!\n- remove active OSD on pve1\n- remove active MDS on pve1\n- remove other MONs, pve1 is not the last MON\nOk, I can’t purge to start, I’ll have to back my way out.\n\nremove cephfs\nThe list above only talks about pools, but I’ve got a cephfs on top of that to remove first. The pveceph docs have a section on destroying a cephfs. Let’s follow that.\numount /mnt/pve/cephfs\npveceph stop --service mds.cephfs # Run this on all nodes\nThat didn’t seem to actually stop the MDSs, so I went into the UI and destroyed them all. Based on the guide, after that I should be able to remove it with pveceph fs destroy cephfs --remove-storages --remove-pools but I get storage 'cephfs' is not disabled, make sure to disable and unmount the storage first. A little more searching gets me ceph fs rm cephfs --yes-i-really-mean-it which runs ok and upon completion I don’t see any entries for cephfs anymore, so I think that’s good.\n\n\nremove my other pool\nI think I’m going to do the rest of this through the UI. It’s not the sort of thing I need to automate, and the UI seem to be cleaner and easier. Ok, my pools are gone, including some related to cephfs that didn’t seem to clear out with the old command. My nodes are still showing the pools as storage locations, but with a ? by them. I think that will go away once I purge the config for ceph, so let’s not worry about it for now.\n\n\nremove OSDs\nFrom the UI, for each OSD in my cluster I first take it out, stop it, then destroy it.\n\n\nremove MDS\nLooks like that was taken care of when I removed cephfs. No action\n\n\nremove managers and monitors\nAgain from the UI I destroy each manager, and then destroy all but one monitor.\n\n\ntry purging again\nHmmm, I’m still getting told to remove pools and mons. Not sure what’s up with that. Ahh, pveceph pool ls tells me I still have a .mgr pool. I didn’t realize that counted. Ok, that’s cleared out. I’ve still got this monitor listed under one of my nodes but with status unknown and I can’t seem to destroy it from the UI. Going into the ceph docs I can see there are some docs on removing mons from an unhealthy cluster. The ghost monitor is running on my third node so I ssh into it and I can see the monitor service is indeed running there. I’m able to stop the service on that node with systemctl stop ceph-mon.target. This still doesn’t let me run purge though. If I run it I get told that my monitor isn’t the last one running, but also if I try and remove that monitor I get told it’s the last one. That’s… confusing. Ok, let’s go back to that third node, disable the monitor service and reboot it the node. Still nothing. Running ceph mon dump on any node only shows the monitor I know is running on my first node. Looking at /etc/pve/ceph.conf I only see the one monitor. Ok, bit of googling and I’m back to this thread which reminds me to check /var/lib/ceph/mon on the node with the unknown status monitor. Sure enough, there’s still a folder there and after I delete it I don’t see that entry anymore. Let’s try purging again.\nThat seems to have worked. If I go to the ceph page in the UI I’m told that it’s not configured. I can still see the storage pools on my nodes though. I wonder if that’s just in /etc/pve/storage.cfg like my NFS share configs are. Yup! Ok, after deleting that I no longer see them as storage in the UI. I think I’m good. One last thing to do is to go into each node through the UI and wipe the SSDs."
  },
  {
    "objectID": "posts/2022-12-30-devcontainers.html",
    "href": "posts/2022-12-30-devcontainers.html",
    "title": "Building my own devcontainers",
    "section": "",
    "text": "Introduction\nMicrosoft has created the devcontainer standard for packaging up your development environment into a docker image and I love it.\nEven though I have a fairly automated environment setup at home, it’s still a hassle whenever I want to start a new project or pick up an old one to make sure I have all the dependencies in place. It’s even trickier if I’m trying to help another person contribute to a project of mine. Devcontainers solve both these issues. Microsoft publishes a number of out of the box images and templates in their GitHub devcontainers project.\nThese work quite well, but I’m picky and want things set up in a certain way. For instance, I want rcm installed for dotfile management, and starship prompt in any environment I work in. On top of that, for python development I like the hypermodern suite of tools to be installed. It would be relatively easy to make a dockerfile that has these features installed and put it in every project, but I want to overengineer things. This isn’t entirely just me liking to hack on things. The build time for my python environment is actually quite long thanks to compiling several different versions of python, so while a Dockerfile would work, it would be annoying to maintain and take quite a while to build.\nIn light of this, I decided to make my own copy of the images repository that Microsoft uses to build their devcontainers and make my own. This post chronicles some of the challenges I had doing that.\n\n\nFiguring out the code\nTo start I just copied the entire images repository and poked around. It’s a beast of a repo (at least compared to the personal or small organization projects I’m used to working on) so it took quite a while just to get a sense of what was there. At a high level there’s a .github folder which contains the CI/CD workflows, a build folder that contains node scripts that build the images, and a src folder that contains the devcontainer specs. I started by deleting all but the base-ubuntu image from src so I could focus on getting one container built without extensive build times. After that I tried to get the build script working locally. Fortunately there are pretty good README files included in each section of the codebase, so I could get a general sense of what was going on.\nThe next two difficult parts that went together were figuring out how to navigate and understand the node codebase, since I’ve never written node or any javascript before, and figuring out what I’d need to modify to get things working in my repository. Some things were relatively straightforward, like the GitHub actions were calling for secrets like REPOSITORY_NAME and REPOSITORY_SECRET that I’d have to swap out for my image registry name and credentials. Once I got past that surface level understanding though, it got trickier. One fairly easy example was that the original GitHub action wanted to be run on some custom Microsoft devcontainer-image-builder-ubuntu VM that I didn’t have access to. It seemed to work fine if I changed that to ubuntu-latest, I just had to notice the issue and change it. Other things were more embedded. Microsoft is publishing their images to Azure container registry whereas I want to use Docker hub. Again, some of this was as simple as switching out az login with docker login in the scripts, but some of it was a little more complicated. Part of the node code queries the registry to see what images are there and what tags they have to make sure published image tags aren’t overwritten accidentally. This is a great feature, but it relied heavily on calling the acr command prompt to retrieve that info. I had to find those sections in the code, figure out what sort of data they’d be returning, figure out what request to send to the docker hub API to get similar data, and then modify the node code to parse it the same way. Since I’d never worked with the docker hub API, or node, or seen the actual output of the acr commands I was trying to reproduce, this took some trial and error.\nAn additional challenge was separating out the nice features of the Microsoft code base from the stuff that I didn’t want and that just made things more complicated. The two main things in the latter category were the secondary registry logic and the stubs repository logic. In both cases, Microsoft is publishing lots of extra stuff besides the built devcontainer image, either because they have two repositories to publish to (I think this relates to them moving the devcontainer spec outside of VS Code into its own project) or they want to publish stub files that other developers can extend for their own purposes. Neither applies to me, but since that logic is embedded in the GitHub actions and the node code that publishes regular images I had to find and strip out all that logic before I could publish my own images.\n\n\nBuilding my own devcontainers\nPrior to going to all the trouble of setting up this build infrastructure, I’d already spent quite a bit of time building devcontainer images, primarily for python. In light of that, once I got the build infrastructure going it wasn’t a huge leap to get my own devcontainers building. There were some growing pains though. The Microsoft image builder builds multiple variants of images, namely basing them off different Ubuntu releases or architectures (x86/64 vs arm). I definitely ran into situations where things seemed to be building fine but then I realized some combo of release and architecture was failing and stopping the whole pipeline from completing. There are ways to test those things locally in the repository, but I didn’t have any comprehensive workflow set up so it was easy to miss things. Some stuff I just didn’t bother fixing and removed the troublesome build. For instance, there were a fair number of issues building images based on Ubuntu 18.04LTS (one of the default variants from Microsoft) and I just decided there was no point spending time fixing issues with a release that was about to be EOL from Ubuntu and just dropped it. Similarly, my Infrastructure as Code image didn’t want to install Terraform on the arm build. I’m not currently planning to run that on an arm system so I just dropped it, maybe I’ll put it back later if I want to run it off a raspberry pi but for now it’s not worth the effort.\n\n\nConclusion\nThis was quite likely more effort than it was objectively worth compared to just building an image and pushing it manually with some tags using the devcontainer cli at least for my personal projects. I did learn a fair bit going through the exercise though, and since I also intend to adopt devcontainers at work (for myself and other people writing at least python code) knowing how to build images in a more automated and versioned manner will be useful.\nMy repository is here, the original Microsoft one is here. My repo is definitely a bit of a mess with ugly commits just testing out CI/CD outcomes and a lot of failed releases since I’d never used GitHub directly to release software before, but that’s all part of the learning experience."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "My blog about things I find interesting, mostly data, python, statistics, and Linux."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ian's blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nHome Cluster Part 4 - Setup CEPH\n\n\n\n\n\nWhat’s HA services without HA storage?\n\n\n\n\n\n\nFeb 5, 2023\n\n\n28 min\n\n\n\n\n\n\n\n\nMigrating to quarto\n\n\n\n\n\nFastpages is dead, long live quarto\n\n\n\n\n\n\nFeb 3, 2023\n\n\n0 min\n\n\n\n\n\n\n\n\nHome cluster part 3 - Setup VM templates on proxmox\n\n\n\n\n\nHomelab cluster adventures continue\n\n\n\n\n\n\nJan 21, 2023\n\n\n28 min\n\n\n\n\n\n\n\n\nHome cluster part 2 - Configuring Proxmox\n\n\n\n\n\nHomelab cluster adventures continue\n\n\n\n\n\n\nDec 31, 2022\n\n\n12 min\n\n\n\n\n\n\n\n\nSetting up rootless docker\n\n\n\n\n\nHow can multiple users share a host for docker without a security nightmare?\n\n\n\n\n\n\nDec 30, 2022\n\n\n11 min\n\n\n\n\n\n\n\n\nBuilding my own devcontainers\n\n\n\n\n\nSure I could just use the Microsoft built ones, but where’s the fun in that?\n\n\n\n\n\n\nDec 30, 2022\n\n\n5 min\n\n\n\n\n\n\n\n\nHome cluster part 1 - Intro and Proxmox install\n\n\n\n\n\nFiguring out how to make my own little homelab cluster\n\n\n\n\n\n\nNov 21, 2022\n\n\n9 min\n\n\n\n\n\n\n\n\nDunning-Kruger is autocorrelation\n\n\n\n\n\nRead a cool post, wanted to play around with the idea some more\n\n\n\n\n\n\nAug 1, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\nModeling out fixed closed vs capped variable mortgages\n\n\n\n\n\nI’m buying a house, let’s use python to help.\n\n\n\n\n\n\nMar 17, 2022\n\n\n8 min\n\n\n\n\n\n\n\n\nBuilding a where to live app\n\n\n\n\n\nAn excuse to teach myself some cool tools and figure out the best place to live\n\n\n\n\n\n\nDec 30, 2021\n\n\n13 min\n\n\n\n\n\n\n\n\nMy Portfolio\n\n\n\n\n\nThe bigger posts and projects I’ve worked on that I want to share.\n\n\n\n\n\n\nApr 10, 2021\n\n\n6 min\n\n\n\n\n\n\n\n\nAlberta’s Fiscal Responses To Fluctuations In Non-Renewable-Resource Revenue\n\n\n\n\n\nI wanted to practice my python and econometrics, found a data error that invalidated the paper’s findings as well.\n\n\n\n\n\n\nFeb 26, 2021\n\n\n9 min\n\n\n\n\n\n\n\n\nUsing traefik for internal services\n\n\n\n\n\nHow to make a reverse proxy work within your LAN\n\n\n\n\n\n\nJan 9, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\nAutomating provisioning Arch continued - TLDR\n\n\n\n\n\nSummarizing how I provision my machines with just the steps to reproduce the process.\n\n\n\n\n\n\nNov 26, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\nAutomating provisioning Arch continued - dotfiles\n\n\n\n\n\nHow I do all the user level configuration for my system, including setting up my python environment.\n\n\n\n\n\n\nNov 25, 2020\n\n\n9 min\n\n\n\n\n\n\n\n\nAutomating provisioning Arch continued - Ansible\n\n\n\n\n\nUsing ansible to install programs and do other system level configuration\n\n\n\n\n\n\nNov 21, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\nAutomating provisioning Arch - OS installation\n\n\n\n\n\nMy bash script for getting a minimal working Arch install.\n\n\n\n\n\n\nOct 14, 2020\n\n\n27 min\n\n\n\n\n\n\n\n\nNotes on docker-compose\n\n\n\n\n\nTiny snippets of things I forget or often have to look up about docker compose.\n\n\n\n\n\n\nOct 6, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\nConnecting a Harmony remote to a raspberry pi\n\n\n\n\n\nHandy tip if you’re using raspberry pi OS as a media front end and have a harmony remote, or just want to remember how to pair bluetooth from the command line.\n\n\n\n\n\n\nJul 20, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\nPython packaging\n\n\n\n\n\nHelping myself understand python packaging by working up from a single file to an actual library.\n\n\n\n\n\n\nJul 9, 2020\n\n\n27 min\n\n\n\n\n\n\n\n\nTrouble with reproducible conda environments\n\n\n\n\n\nSometimes my conda environments aren’t reproducible and I can’t figure out why. This documents what I’ve tried so far.\n\n\n\n\n\n\nMay 17, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\nHow to work with conda environments in shell scripts and Makefiles\n\n\n\n\n\nconda environments are a little tricky to incorporate into automation. This post outlines a bit of how they work, and how to integrate them in scripts and makefiles.\n\n\n\n\n\n\nMay 13, 2020\n\n\n7 min\n\n\n\n\n\n\n\n\nA basic SAMBA share for home networks\n\n\n\n\n\nSimple (but not super secure) SAMBA shares for the home user\n\n\n\n\n\n\nMay 9, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\nBuilding pfsense\n\n\n\n\n\nDocumenting everything I did to configure my pfsense router.\n\n\n\n\n\n\nMay 6, 2020\n\n\n18 min\n\n\n\n\n\n\n\n\nManaging SSH keys for a home network\n\n\n\n\n\nHow to generate certificate authorities so your clients and hosts can all talk to each other easily.\n\n\n\n\n\n\nMay 3, 2020\n\n\n9 min\n\n\n\n\n\n\n\n\nSetting up for data science in python on Windows\n\n\n\n\n\nMy opinionated setup to do data work in python on Windows, without admin rights.\n\n\n\n\n\n\nFeb 15, 2020\n\n\n17 min\n\n\n\n\n\n\nNo matching items"
  }
]